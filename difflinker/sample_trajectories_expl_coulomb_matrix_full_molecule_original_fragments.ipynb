{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"http_proxy\"] = \"http://web-proxy.informatik.uni-bonn.de:3128\"\n",
    "os.environ[\"https_proxy\"] = \"http://web-proxy.informatik.uni-bonn.de:3128\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed is:  42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mastropietro/anaconda3/envs/diff_explainer/lib/python3.10/site-packages/lightning_fabric/utilities/cloud_io.py:57: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.6.3 to v2.3.3. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint models/zinc_difflinker.ckpt`\n",
      "/home/mastropietro/Repositories/DiffSHAPer/difflinker/src/datasets.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.data = torch.load(dataset_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running device: cuda:5\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import torch\n",
    "\n",
    "from src.datasets import get_dataloader\n",
    "from src.lightning import DDPM\n",
    "from src.molecule_builder import get_bond_order\n",
    "from src.visualizer import save_xyz_file\n",
    "from tqdm.auto import tqdm\n",
    "import sys #@mastro\n",
    "from src import const #@mastro\n",
    "import numpy as np #@mastro\n",
    "from numpy.random import default_rng\n",
    "from sklearn.metrics import jaccard_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.spatial.distance import directed_hausdorff\n",
    "import random\n",
    "from sklearn.decomposition import PCA\n",
    "from src.visualizer import load_molecule_xyz, load_xyz_files\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "from src import const\n",
    "import networkx as nx\n",
    "import time \n",
    "import yaml\n",
    "from pysmiles import read_smiles\n",
    "#get running device from const file\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "# Simulate command-line arguments\n",
    "\n",
    "# density = sys.argv[sys.argv.index(\"--P\") + 1]\n",
    "with open('config.yml', 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "checkpoint = config['CHECKPOINT']\n",
    "chains = config['CHAINS']\n",
    "DATA = config['DATA']\n",
    "prefix = config['PREFIX']\n",
    "keep_frames = int(config['KEEP_FRAMES'])\n",
    "P = config['P']\n",
    "device = config['DEVICE'] if torch.cuda.is_available() else 'cpu'\n",
    "SEED = int(config['SEED'])\n",
    "SAVE_VISUALIZATION = config['SAVE_VISUALIZATION']\n",
    "M = int(config['M'])\n",
    "NUM_SAMPLES = int(config['NUM_SAMPLES'])\n",
    "PARALLEL_STEPS = int(config['PARALLEL_STEPS'])\n",
    "DIAGONALIZE = config['DIAGONALIZE']\n",
    "\n",
    "print(\"seed is: \", SEED)\n",
    "\n",
    "experiment_name = checkpoint.split('/')[-1].replace('.ckpt', '')\n",
    "chains_output_dir = os.path.join(chains, experiment_name, prefix, 'chains_coulomb_' + P + '_seed_' + str(SEED))\n",
    "final_states_output_dir = os.path.join(chains, experiment_name, prefix, 'final_states_coulomb_' + P + '_seed_' + str(SEED))\n",
    "\n",
    "if DIAGONALIZE:\n",
    "    chains_output_dir = chains_output_dir + '_diagonalized'\n",
    "    final_states_output_dir = final_states_output_dir + '_diagonalized'\n",
    "    \n",
    "os.makedirs(chains_output_dir, exist_ok=True)\n",
    "os.makedirs(final_states_output_dir, exist_ok=True)\n",
    "\n",
    "# Loading model form checkpoint (all hparams will be automatically set)\n",
    "model = DDPM.load_from_checkpoint(checkpoint, map_location=device)\n",
    "\n",
    "# Possibility to evaluate on different datasets (e.g., on CASF instead of ZINC)\n",
    "model.val_data_prefix = prefix\n",
    "\n",
    "print(f\"Running device: {device}\")\n",
    "# In case <Anonymous> will run my model or vice versa\n",
    "if DATA is not None:\n",
    "    model.data_path = DATA\n",
    "\n",
    "model = model.eval().to(device)\n",
    "model.setup(stage='val')\n",
    "dataloader = get_dataloader(\n",
    "    model.val_dataset,\n",
    "    batch_size=1, #@mastro, it was 32\n",
    "    # batch_size=len(model.val_dataset)\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similarity functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_molecular_similarity(mol1, mol2, mask1 = None, mask2 = None):\n",
    "    \"\"\"\n",
    "    Compute the similarity between two molecules based on distances and atom type.\n",
    "    \n",
    "    Args:\n",
    "        mol1 (torch.Tensor): The first molecule.\n",
    "        mol2 (torch.Tensor): The second molecule.\n",
    "        mask (torch.Tensor, optional): A mask indicating which atoms to consider. If not provided, all atoms will be considered.\n",
    "        \n",
    "    Returns:\n",
    "        float: The similarity between the two molecules.\n",
    "    \"\"\"\n",
    "    # If fragmen_mask is provided, only consider the atoms in the mask\n",
    "    if mask1 is not None:\n",
    "        mask1 = mask1.bool()\n",
    "        mol1 = mol1[mask1,:]\n",
    "\n",
    "    if mask2 is not None:\n",
    "        mask2 = mask2.bool()\n",
    "        mol2 = mol2[mask2,:]\n",
    "\n",
    "    return 1 - torch.norm(mol1 - mol2)\n",
    "\n",
    "def compute_molecular_distance(mol1, mol2, mask1 = None, mask2 = None):\n",
    "    \"\"\"\n",
    "    Compute the similarity between two molecules based on distances and atom type.\n",
    "    \n",
    "    Args:\n",
    "        mol1 (torch.Tensor): The first molecule.\n",
    "        mol2 (torch.Tensor): The second molecule.\n",
    "        mask (torch.Tensor, optional): A mask indicating which atoms to consider. If not provided, all atoms will be considered.\n",
    "        \n",
    "    Returns:\n",
    "        float: The similarity between the two molecules.\n",
    "    \"\"\"\n",
    "    # If fragmen_mask is provided, only consider the atoms in the mask\n",
    "    if mask1 is not None:\n",
    "        mask1 = mask1.bool()\n",
    "        mol1 = mol1[mask1,:]\n",
    "\n",
    "    if mask2 is not None:\n",
    "        mask2 = mask2.bool()\n",
    "        mol2 = mol2[mask2,:]\n",
    "\n",
    "    return torch.norm(mol1 - mol2).item()\n",
    "\n",
    "def compute_molecular_distance_batch(mol1, mol2, mask1 = None, mask2 = None):\n",
    "    \"\"\"\n",
    "    Compute the similarity between two molecules based on distances and atom type.\n",
    "    \n",
    "    Args:\n",
    "        mol1 (torch.Tensor): The first molecule.\n",
    "        mol2 (torch.Tensor): The second molecule.\n",
    "        mask (torch.Tensor, optional): A mask indicating which atoms to consider. If not provided, all atoms will be considered.\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: The similarity between the two molecules for each element in the batch.\n",
    "    \"\"\"\n",
    "    # If fragment_mask is provided, only consider the atoms in the mask\n",
    "    if mask1 is not None:\n",
    "        mask1 = mask1.bool()\n",
    "        batch_size = mol1.shape[0]\n",
    "        masked_mol1 = []\n",
    "        for i in range(batch_size):\n",
    "            masked_mol1.append(mol1[i, mask1[i], :])\n",
    "\n",
    "        if batch_size == 1:\n",
    "            mol1 = masked_mol1[0].unsqueeze(0)\n",
    "        else:    \n",
    "            mol1 = torch.stack(masked_mol1)\n",
    "           \n",
    "    if mask2 is not None:\n",
    "        mask2 = mask2.bool()\n",
    "        batch_size = mol2.shape[0]\n",
    "        masked_mol2 = []\n",
    "        for i in range(batch_size):\n",
    "            masked_mol2.append(mol2[i, mask2[i], :])\n",
    "        \n",
    "        if batch_size == 1:\n",
    "            mol2 = masked_mol2[0].unsqueeze(0)\n",
    "        else:    \n",
    "            mol2 = torch.stack(masked_mol2)\n",
    "\n",
    "    return torch.norm(mol1 - mol2, dim=(1,2))\n",
    "\n",
    "def compute_cosine_similarity(mol1, mol2, mask1 = None, mask2 = None):\n",
    "    \"\"\"\n",
    "    Compute the similarity between two molecules based on distances and atom type.\n",
    "    \n",
    "    Args:\n",
    "        mol1 (torch.Tensor): The first molecule.\n",
    "        mol2 (torch.Tensor): The second molecule.\n",
    "        mask (torch.Tensor, optional): A mask indicating which atoms to consider. If not provided, all atoms will be considered.\n",
    "        \n",
    "    Returns:\n",
    "        float: The similarity between the two molecules.\n",
    "    \"\"\"\n",
    "    # If fragmen_mask is provided, only consider the atoms in the mask\n",
    "    if mask1 is not None:\n",
    "        mask1 = mask1.bool()\n",
    "        mol1 = mol1[mask1,:]\n",
    "\n",
    "    if mask2 is not None:\n",
    "        mask2 = mask2.bool()\n",
    "        mol2 = mol2[mask2,:]\n",
    "\n",
    "    return cosine_similarity(mol1.flatten().reshape(1, -1), mol2.flatten().reshape(1, -1)).item()\n",
    "\n",
    "\n",
    "def compute_cosine_similarity_batch(mol1, mol2, mask1 = None, mask2 = None):\n",
    "    \"\"\"\n",
    "    Compute the similarity between two molecules based on distances and atom type.\n",
    "    \n",
    "    Args:\n",
    "        mol1 (torch.Tensor): The first molecule.\n",
    "        mol2 (torch.Tensor): The second molecule.\n",
    "        mask (torch.Tensor, optional): A mask indicating which atoms to consider. If not provided, all atoms will be considered.\n",
    "        \n",
    "    Returns:\n",
    "        float: The similarity between the two molecules.\n",
    "    \"\"\"\n",
    "    # If fragmen_mask is provided, only consider the atoms in the mask\n",
    "    if mask1 is not None:\n",
    "        mask1 = mask1.bool()\n",
    "        batch_size = mol1.shape[0]\n",
    "        masked_mol1 = []\n",
    "        for i in range(batch_size):\n",
    "            masked_mol1.append(mol1[i, mask1[i], :])\n",
    "        \n",
    "        if batch_size == 1:\n",
    "            mol1 = masked_mol1[0].unsqueeze(0)\n",
    "        else:    \n",
    "            mol1 = torch.stack(masked_mol1)\n",
    "        \n",
    "\n",
    "    if mask2 is not None:\n",
    "        mask2 = mask2.bool()\n",
    "        mask2 = mask2.bool()\n",
    "        batch_size = mol2.shape[0]\n",
    "        masked_mol2 = []\n",
    "        for i in range(batch_size):\n",
    "            masked_mol2.append(mol2[i, mask2[i], :])\n",
    "        \n",
    "        if batch_size == 1:\n",
    "            mol2 = masked_mol2[0].unsqueeze(0)\n",
    "        else:    \n",
    "            mol2 = torch.stack(masked_mol2)\n",
    "\n",
    "    cos_sims = []\n",
    "    for i in range(mol1.shape[0]):\n",
    "        cos_sims.append(cosine_similarity(mol1[i].flatten().reshape(1, -1), mol2[i].flatten().reshape(1, -1)).item())\n",
    "\n",
    "    return cos_sims\n",
    "\n",
    "def compute_molecular_similarity_positions(mol1, mol2, mask1 = None, mask2 = None):\n",
    "    \"\"\"\n",
    "    Compute the similarity between two molecules based on positions.\n",
    "    \n",
    "    Args:\n",
    "        mol1 (torch.Tensor): The first molecule.\n",
    "        mol2 (torch.Tensor): The second molecule.\n",
    "        mask (torch.Tensor, optional): A mask indicating which atoms to consider. If not provided, all atoms will be considered.\n",
    "        \n",
    "    Returns:\n",
    "        float: The similarity between the two molecules.\n",
    "    \"\"\"\n",
    "    # If fragmen_mask is provided, only consider the atoms in the mask\n",
    "    positions1 = mol1[:, :3].squeeze()\n",
    "    positions2 = mol2[:, :3].squeeze()\n",
    "\n",
    "    if mask1 is not None:\n",
    "        mask1 = mask1.bool()\n",
    "        positions1 = positions1[mask1,:]\n",
    "\n",
    "    if mask2 is not None:\n",
    "        mask2 = mask2.bool()\n",
    "        positions2 = positions2[mask2,:]\n",
    "\n",
    "\n",
    "    return 1 - torch.norm(positions1 - positions2) #choose if distance or similarity, need to check what it the better choice\n",
    "\n",
    "def compute_one_hot_similarity(mol1, mol2, mask1 = None, mask2 = None):\n",
    "    \"\"\"\n",
    "    Computes the similarity between two one-hot encoded molecules. The one-hot encoding indicates the atom type\n",
    "    \n",
    "    Args:\n",
    "        mol1 (torch.Tensor): The first one-hot encoded molecule.\n",
    "        mol2 (torch.Tensor): The second one-hot encoded molecule.\n",
    "        mask (torch.Tensor, optional): A mask to apply on the atoms. Defaults to None.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: The similarity between the two molecules.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Apply mask if provided\n",
    "    if mask1 is not None:\n",
    "        mask1 = mask1.bool()\n",
    "        mol1 = mol1[mask1,:]\n",
    "\n",
    "    if mask2 is not None:\n",
    "        mask2 = mask2.bool()\n",
    "        mol2 = mol2[mask2,:]\n",
    "    \n",
    "    # Compute similarity by comparing the one-hot encoded features\n",
    "    similarity = torch.sum(mol1[:,3:-1] == mol2[:,3:-1]) / mol1[:, 3:-1].numel()\n",
    "    \n",
    "    return similarity\n",
    "\n",
    "def compute_hausdorff_distance_batch(mol1, mol2, mask1 = None, mask2 = None):\n",
    "    \"\"\"\n",
    "    Compute the similarity between two molecules based on distances and atom type.\n",
    "    \n",
    "    Args:\n",
    "        mol1 (torch.Tensor): The first molecule.\n",
    "        mol2 (torch.Tensor): The second molecule.\n",
    "        mask1 (torch.Tensor, optional): A mask indicating which atoms to consider for mo1. If not provided, all atoms will be considered.\n",
    "        mask2 (torch.Tensor, optional): A mask indicating which atoms to consider for mol2. If not provided, all atoms will be considered.\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: The similarity between the two molecules for each element in the batch.\n",
    "    \"\"\"\n",
    "    # If fragment_mask is provided, only consider the atoms in the mask\n",
    "\n",
    "    #take only the positions\n",
    "    mol1 = mol1[:, :, :3]\n",
    "    mol2 = mol2[:, :, :3]\n",
    "    \n",
    "    \n",
    "    if mask1 is not None:\n",
    "        mask1 = mask1.bool()\n",
    "        batch_size = mol1.shape[0]\n",
    "        masked_mol1 = []\n",
    "        for i in range(batch_size):\n",
    "            masked_mol1.append(mol1[i, mask1[i], :])\n",
    "        \n",
    "        if batch_size == 1:\n",
    "            mol1 = masked_mol1[0].unsqueeze(0)\n",
    "        else:    \n",
    "            mol1 = torch.stack(masked_mol1)\n",
    "        \n",
    "\n",
    "    if mask2 is not None:\n",
    "        mask2 = mask2.bool()\n",
    "        mask2 = mask2.bool()\n",
    "        batch_size = mol2.shape[0]\n",
    "        masked_mol2 = []\n",
    "        for i in range(batch_size):\n",
    "            masked_mol2.append(mol2[i, mask2[i], :])\n",
    "        \n",
    "        if batch_size == 1:\n",
    "            mol2 = masked_mol2[0].unsqueeze(0)\n",
    "        else:    \n",
    "            mol2 = torch.stack(masked_mol2)\n",
    "\n",
    "    hausdorff_distances = []\n",
    "    for i in range(mol1.shape[0]):\n",
    "        hausdorff_distances.append(max(directed_hausdorff(mol1[i], mol2[i])[0], directed_hausdorff(mol2[i], mol1[i])[0]))\n",
    "\n",
    "    return hausdorff_distances\n",
    "\n",
    "\n",
    "def create_edge_index(mol, weighted=False):\n",
    "    \"\"\"\n",
    "    Create edge index for a molecule.\n",
    "    \"\"\"\n",
    "    adj = nx.to_scipy_sparse_array(mol).todense()\n",
    "    row = torch.from_numpy(adj.row.astype(np.int64)).to(torch.long)\n",
    "    col = torch.from_numpy(adj.col.astype(np.int64)).to(torch.long)\n",
    "    edge_index = torch.stack([row, col], dim=0)\n",
    "\n",
    "    if weighted:\n",
    "        weights = torch.from_numpy(adj.data.astype(np.float32))\n",
    "        edge_weight = torch.FloatTensor(weights)\n",
    "        return edge_index, edge_weight\n",
    "\n",
    "    return edge_index\n",
    "\n",
    "\n",
    "def compute_coulomb_matrix(mol, mask=None, diagonalize=False):\n",
    "    \"\"\"\n",
    "    Compute the Coulomb matrix for a molecule.\n",
    "    \n",
    "    Args:\n",
    "        mol (torch.Tensor): The molecule tensor with shape (N, 4), where N is the number of atoms.\n",
    "                            The last dimension should contain [x, y, z, atomic_number].\n",
    "        mask (torch.Tensor, optional): A mask indicating which atoms to consider. If not provided, all atoms will be considered.\n",
    "        diagonalize (bool, optional): Whether to return the diagonalized Coulomb matrix. Defaults to False.\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: The Coulomb matrix of the molecule.\n",
    "    \"\"\"\n",
    "    if mask is not None:\n",
    "        mask = mask.bool()\n",
    "        mol = mol[mask, :]\n",
    "\n",
    "    positions = mol[:, :3]\n",
    "    one_hot = mol[:, 3:]\n",
    "    atomic_numbers = []\n",
    "    \n",
    "    for i, vec in enumerate(one_hot):\n",
    "        if torch.sum(vec) == 1:\n",
    "            atom_index = torch.argmax(vec).item()\n",
    "            atomic_number = const.CHARGES[const.IDX2ATOM[atom_index]]\n",
    "            atomic_numbers.append(atomic_number)\n",
    "        else:\n",
    "            atomic_numbers.append(0)  \n",
    "    \n",
    "    num_atoms = positions.shape[0]\n",
    "    coulomb_matrix = torch.zeros((num_atoms, num_atoms))\n",
    "\n",
    "    for i in range(num_atoms):\n",
    "        for j in range(num_atoms):\n",
    "            if i == j:\n",
    "                coulomb_matrix[i, j] = 0.5 * atomic_numbers[i] ** 2.4\n",
    "            else:\n",
    "                distance = torch.norm(positions[i] - positions[j])\n",
    "                if distance == 0: #avoid division by zero\n",
    "                    coulomb_matrix[i, j] = 0.0\n",
    "                else:\n",
    "                    coulomb_matrix[i, j] = atomic_numbers[i] * atomic_numbers[j] / distance\n",
    "\n",
    "    if diagonalize:\n",
    "        eigenvalues, eigenvectors = torch.linalg.eigh(coulomb_matrix)\n",
    "        coulomb_matrix = torch.diag(eigenvalues)\n",
    "\n",
    "    return coulomb_matrix\n",
    "\n",
    "def compute_coulomb_matrices_batch(molecules, masks=None, diagonalize=False):\n",
    "    \"\"\"\n",
    "    Compute the Coulomb matrices for a batch of molecules.\n",
    "    \n",
    "    Args:\n",
    "        molecules (torch.Tensor): The batch of molecule tensors with shape (B, N, 4), where B is the batch size,\n",
    "                                    N is the number of atoms, and the last dimension should contain [x, y, z, atomic_number].\n",
    "        masks (torch.Tensor, optional): A batch of masks indicating which atoms to consider for each molecule. \n",
    "                                        If not provided, all atoms will be considered.\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: The Coulomb matrices for the batch of molecules with shape (B, N, N).\n",
    "    \"\"\"\n",
    "    batch_size = molecules.shape[0]\n",
    "    # num_atoms = molecules.shape[1] #this is ok when the mask is not provided\n",
    "    num_atoms = int(torch.sum(masks, dim=1).max().item()) if masks is not None else molecules.shape[1]\n",
    "    coulomb_matrices = torch.zeros((batch_size, num_atoms, num_atoms), device=molecules.device)\n",
    "\n",
    "    for b in range(batch_size):\n",
    "        mol = molecules[b]\n",
    "        mask = masks[b] if masks is not None else None\n",
    "        coulomb_matrices[b] = compute_coulomb_matrix(mol, mask, diagonalize=diagonalize)\n",
    "\n",
    "    return coulomb_matrices\n",
    "\n",
    "def compute_frobenius_norm_batch(matrices):\n",
    "    \"\"\"\n",
    "    Compute the Frobenius norm for a batch of matrices.\n",
    "    \n",
    "    Args:\n",
    "        matrices (torch.Tensor): A batch of matrices with shape (B, N, N), where B is the batch size,\n",
    "                                    and N is the number of rows/columns in each matrix.\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: A tensor containing the Frobenius norm for each matrix in the batch.\n",
    "    \"\"\"\n",
    "    # return torch.norm(matrices, dim=(1, 2), p='fro') #deprecated\n",
    "    return torch.linalg.norm(matrices, ord='fro', dim=(1, 2))\n",
    "\n",
    "def arrestomomentum():\n",
    "    raise KeyboardInterrupt(\"Debug interrupt.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explainability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility function for visualization purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_sphere_xai(ax, x, y, z, size, color, alpha):\n",
    "    u = np.linspace(0, 2 * np.pi, 100)\n",
    "    v = np.linspace(0, np.pi, 100)\n",
    "\n",
    "    xs = size * np.outer(np.cos(u), np.sin(v))\n",
    "    ys = size * np.outer(np.sin(u), np.sin(v)) #* 0.8\n",
    "    zs = size * np.outer(np.ones(np.size(u)), np.cos(v))\n",
    "    ax.plot_surface(x + xs, y + ys, z + zs, rstride=2, cstride=2, color=color, alpha=alpha)\n",
    "\n",
    "def plot_molecule_xai(ax, positions, atom_type, alpha, spheres_3d, hex_bg_color, is_geom, fragment_mask=None, phi_values=None, invert_colormap = False):\n",
    "    x = positions[:, 0]\n",
    "    y = positions[:, 1]\n",
    "    z = positions[:, 2]\n",
    "    # Hydrogen, Carbon, Nitrogen, Oxygen, Flourine\n",
    "\n",
    "    idx2atom = const.GEOM_IDX2ATOM if is_geom else const.IDX2ATOM\n",
    "\n",
    "    colors_dic = np.array(const.COLORS)\n",
    "    radius_dic = np.array(const.RADII)\n",
    "    area_dic = 1500 * radius_dic ** 2\n",
    "\n",
    "    areas = area_dic[atom_type]\n",
    "    radii = radius_dic[atom_type]\n",
    "    colors = colors_dic[atom_type]\n",
    "\n",
    "    if fragment_mask is None:\n",
    "        fragment_mask = torch.ones(len(x))\n",
    "\n",
    "    for i in range(len(x)):\n",
    "        for j in range(i + 1, len(x)):\n",
    "            p1 = np.array([x[i], y[i], z[i]])\n",
    "            p2 = np.array([x[j], y[j], z[j]])\n",
    "            dist = np.sqrt(np.sum((p1 - p2) ** 2))\n",
    "            atom1, atom2 = idx2atom[atom_type[i]], idx2atom[atom_type[j]]\n",
    "            draw_edge_int = get_bond_order(atom1, atom2, dist)\n",
    "            line_width = (3 - 2) * 2 * 2\n",
    "            draw_edge = draw_edge_int > 0\n",
    "            if draw_edge:\n",
    "                if draw_edge_int == 4:\n",
    "                    linewidth_factor = 1.5\n",
    "                else:\n",
    "                    linewidth_factor = 1\n",
    "                linewidth_factor *= 0.5\n",
    "                ax.plot(\n",
    "                    [x[i], x[j]], [y[i], y[j]], [z[i], z[j]],\n",
    "                    linewidth=line_width * linewidth_factor * 2,\n",
    "                    c=hex_bg_color,\n",
    "                    alpha=alpha\n",
    "                )\n",
    "\n",
    "    \n",
    "\n",
    "    if spheres_3d:\n",
    "        \n",
    "        for i, j, k, s, c, f, phi in zip(x, y, z, radii, colors, fragment_mask, phi_values):\n",
    "            if f == 1:\n",
    "                alpha = 1.0\n",
    "                if phi > 0:\n",
    "                    c = 'red'\n",
    "\n",
    "            draw_sphere_xai(ax, i.item(), j.item(), k.item(), 0.5 * s, c, alpha)\n",
    "\n",
    "    else:\n",
    "        phi_values_array = np.array(list(phi_values.values()))\n",
    "\n",
    "        #draw fragments\n",
    "        fragment_mask_on_cpu = fragment_mask.cpu().numpy()\n",
    "        colors_fragment = colors[fragment_mask_on_cpu == 1]\n",
    "        x_fragment = x[fragment_mask_on_cpu == 1]\n",
    "        y_fragment = y[fragment_mask_on_cpu == 1]\n",
    "        z_fragment = z[fragment_mask_on_cpu == 1]\n",
    "        areas_fragment = areas[fragment_mask_on_cpu == 1]\n",
    "        \n",
    "        # Calculate the gradient colors based on phi values\n",
    "        # cmap = plt.cm.get_cmap('coolwarm_r') #reversed heatmap for distance-based importance\n",
    "        cmap = plt.cm.get_cmap('coolwarm') #heatmap for distance-based importance trying non reversed -> high shapley value mean more imporant, that drive the generation.\n",
    "        #@mastro added invert_colormap to invert the colormap if average/expected value in higher than original prediction\n",
    "        if invert_colormap:\n",
    "            cmap = plt.cm.get_cmap('coolwarm_r')\n",
    "\n",
    "        norm = plt.Normalize(vmin=min(phi_values_array), vmax=max(phi_values_array))\n",
    "        colors_fragment_shadow = cmap(norm(phi_values_array))\n",
    "        \n",
    "        # ax.scatter(x_fragment, y_fragment, z_fragment, s=areas_fragment, alpha=0.9 * alpha, c=colors_fragment)\n",
    "\n",
    "        ax.scatter(x_fragment, y_fragment, z_fragment, s=areas_fragment, alpha=0.9 * alpha, c=colors_fragment, edgecolors=colors_fragment_shadow, linewidths=5, rasterized=False)\n",
    "\n",
    "        #draw non-fragment atoms\n",
    "        colors = colors[fragment_mask_on_cpu == 0]\n",
    "        x = x[fragment_mask_on_cpu == 0]\n",
    "        y = y[fragment_mask_on_cpu == 0]\n",
    "        z = z[fragment_mask_on_cpu == 0]\n",
    "        areas = areas[fragment_mask_on_cpu == 0]\n",
    "        ax.scatter(x, y, z, s=areas, alpha=0.9 * alpha, c=colors, rasterized=False)\n",
    "\n",
    "\n",
    "def plot_data3d_xai(positions, atom_type, is_geom, camera_elev=0, camera_azim=0, save_path=None, spheres_3d=False,\n",
    "                bg='black', alpha=1., fragment_mask=None, phi_values=None, invert_colormap = False):\n",
    "    black = (0, 0, 0)\n",
    "    white = (1, 1, 1)\n",
    "    hex_bg_color = '#FFFFFF' if bg == 'black' else '#000000' #'#666666'\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = fig.add_subplot(projection='3d')\n",
    "    ax.set_aspect('auto')\n",
    "    ax.view_init(elev=camera_elev, azim=camera_azim)\n",
    "    if bg == 'black':\n",
    "        ax.set_facecolor(black)\n",
    "    else:\n",
    "        ax.set_facecolor(white)\n",
    "    ax.xaxis.pane.set_alpha(0)\n",
    "    ax.yaxis.pane.set_alpha(0)\n",
    "    ax.zaxis.pane.set_alpha(0)\n",
    "    ax._axis3don = False\n",
    "\n",
    "    if bg == 'black':\n",
    "        ax.w_xaxis.line.set_color(\"black\")\n",
    "    else:\n",
    "        ax.w_xaxis.line.set_color(\"white\")\n",
    "\n",
    "    plot_molecule_xai(\n",
    "        ax, positions, atom_type, alpha, spheres_3d, hex_bg_color, is_geom=is_geom, fragment_mask=fragment_mask, phi_values=phi_values, invert_colormap=invert_colormap\n",
    "    )\n",
    "\n",
    "    max_value = positions.abs().max().item()\n",
    "    axis_lim = min(40, max(max_value / 1.5 + 0.3, 3.2))\n",
    "    ax.set_xlim(-axis_lim, axis_lim)\n",
    "    ax.set_ylim(-axis_lim, axis_lim)\n",
    "    ax.set_zlim(-axis_lim, axis_lim)\n",
    "    dpi = 300 if spheres_3d else 300 #it was 120 and 50\n",
    "\n",
    "    if save_path is not None:\n",
    "        plt.savefig(save_path, bbox_inches='tight', pad_inches=0.0, dpi=dpi)\n",
    "        # plt.savefig(save_path, bbox_inches='tight', pad_inches=0.0, dpi=dpi, transparent=True)\n",
    "\n",
    "        if spheres_3d:\n",
    "            img = imageio.imread(save_path)\n",
    "            img_brighter = np.clip(img * 1.4, 0, 255).astype('uint8')\n",
    "            imageio.imsave(save_path, img_brighter)\n",
    "    else:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def visualize_chain_xai(\n",
    "        path, spheres_3d=False, bg=\"black\", alpha=1.0, wandb=None, mode=\"chain\", is_geom=False, fragment_mask=None, phi_values=None, invert_colormap = False\n",
    "):\n",
    "    files = load_xyz_files(path)\n",
    "    save_paths = []\n",
    "\n",
    "    # Fit PCA to the final molecule â€“ to obtain the best orientation for visualization\n",
    "    positions, one_hot, charges = load_molecule_xyz(files[-1], is_geom=is_geom)\n",
    "    pca = PCA(n_components=3)\n",
    "    pca.fit(positions)\n",
    "\n",
    "    for i in range(len(files)):\n",
    "        file = files[i]\n",
    "\n",
    "        positions, one_hot, charges = load_molecule_xyz(file, is_geom=is_geom)\n",
    "        atom_type = torch.argmax(one_hot, dim=1).numpy()\n",
    "\n",
    "        # Transform positions of each frame according to the best orientation of the last frame\n",
    "        positions = pca.transform(positions)\n",
    "        positions = torch.tensor(positions)\n",
    "\n",
    "        fn = file[:-4] + '.png'\n",
    "        plot_data3d_xai(\n",
    "            positions, atom_type,\n",
    "            save_path=fn,\n",
    "            spheres_3d=spheres_3d,\n",
    "            alpha=alpha,\n",
    "            bg=bg,\n",
    "            camera_elev=90,\n",
    "            camera_azim=90,\n",
    "            is_geom=is_geom,\n",
    "            fragment_mask=fragment_mask,\n",
    "            phi_values=phi_values,\n",
    "            invert_colormap=invert_colormap\n",
    "        )\n",
    "        save_paths.append(fn)\n",
    "\n",
    "    imgs = [imageio.imread(fn) for fn in save_paths]\n",
    "    dirname = os.path.dirname(save_paths[0])\n",
    "    gif_path = dirname + '/output.gif'\n",
    "    imageio.mimsave(gif_path, imgs, subrectangles=True)\n",
    "\n",
    "    if wandb is not None:\n",
    "        wandb.log({mode: [wandb.Video(gif_path, caption=gif_path)]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explainabiliy phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Multiple sampling steps at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading initial distribution of noisy features and positions.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99311aec54cd462ea670e59d4905b9e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using P: 0.5 0.5\n",
      "Coulomb matrix (diag):  tensor([[  1.3887,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "        [  0.0000,   2.8734,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   3.2186,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000,   5.6355,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000,   0.0000,   5.9394,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   7.7797,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   8.2528,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           9.3483,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,  10.8840,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,  12.5304,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,  17.6412,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,  20.4573,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,  21.4131,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,  25.0803,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          26.2172,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,  28.2539,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,  35.9196,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,  37.3077,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,  43.0097,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,  44.4242,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,  56.3599,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          72.9451,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,  97.6918,   0.0000,   0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000, 109.8292,   0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000, 221.5723,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000, 486.1635]])\n",
      "Coulomb matrix (non diag)  tensor([[ 36.8581,  23.7211,  15.1551,  17.8637,  23.5823,   5.7190,   6.4382,\n",
      "           8.5249,   8.9639,   6.8147,   6.3703,   5.5933,   7.1523,   5.4731,\n",
      "           6.2366,   6.8937,   5.1574,   5.4223,   5.5904,   4.6011,   5.4999,\n",
      "           9.8287,  19.2464,   5.9475,   9.8111,   6.4739],\n",
      "        [ 23.7211,  36.8581,  23.5833,  17.9081,  15.0934,   6.7551,   7.4445,\n",
      "          10.0158,   9.4906,   7.0001,   6.1694,   5.2622,   6.5226,   5.0906,\n",
      "           5.9462,   6.2867,   4.8342,   5.0427,   5.9179,   4.7760,   6.0620,\n",
      "           9.6609,  20.3849,   5.9245,   8.9414,   6.1266],\n",
      "        [ 15.1551,  23.5833,  36.8581,  28.8410,  14.9438,   8.2996,   8.8033,\n",
      "          12.0474,   9.9718,   7.1814,   6.0333,   5.2934,   6.7514,   5.4822,\n",
      "           6.4047,   6.9716,   5.5072,   5.8996,   6.2659,   4.9670,   6.7342,\n",
      "          14.2254,  29.7155,   7.6567,  11.2840,   7.6310],\n",
      "        [ 17.8637,  17.9081,  28.8410,  53.3587,  28.8300,   9.4275,  10.7868,\n",
      "          15.6767,  14.1789,   9.7675,   8.1981,   7.3564,   9.9048,   8.0313,\n",
      "           9.0062,  10.6321,   8.0970,   8.7213,   8.1497,   6.3610,   8.4667,\n",
      "          29.3923,  40.2325,  10.8645,  20.5774,  11.8579],\n",
      "        [ 23.5823,  15.0934,  14.9438,  28.8300,  36.8581,   6.3791,   7.4863,\n",
      "          10.4205,  11.5057,   8.2868,   7.5888,   6.7996,   9.2440,   7.0334,\n",
      "           7.6867,   9.1833,   6.5899,   6.9117,   6.5542,   5.2580,   6.3671,\n",
      "          14.2675,  23.4317,   7.3602,  14.5760,   8.5394],\n",
      "        [  5.7190,   6.7551,   8.2996,   9.4275,   6.3791,  36.8581,  24.0075,\n",
      "          14.3190,   9.4772,   8.3151,   6.1605,   5.4471,   6.5011,   5.6677,\n",
      "           7.2593,   6.6344,   5.6040,   5.4594,   9.4198,   7.1932,  14.2987,\n",
      "           8.2691,  23.8000,   7.1756,   8.1207,   6.5895],\n",
      "        [  6.4382,   7.4445,   8.8033,  10.7868,   7.4863,  24.0075,  36.8581,\n",
      "          25.8246,  14.8648,  12.7193,   8.2855,   6.9718,   8.1931,   6.9297,\n",
      "           8.6916,   7.8394,   6.2795,   5.8722,  14.6916,   9.4750,  25.7530,\n",
      "           9.0579,  22.6689,   7.0971,   9.1428,   7.0231],\n",
      "        [  8.5249,  10.0158,  12.0474,  15.6767,  10.4205,  14.3190,  25.8246,\n",
      "          36.8581,  25.7796,  14.7395,   9.4468,   7.6260,   9.2679,   7.4520,\n",
      "           8.8067,   8.6642,   6.5999,   6.2976,  12.8157,   8.3466,  14.9891,\n",
      "          11.6147,  25.1006,   7.5057,  11.0967,   7.7757],\n",
      "        [  8.9639,   9.4906,   9.9718,  14.1789,  11.5057,   9.4772,  14.8648,\n",
      "          25.7796,  36.8581,  25.5681,  14.3302,  10.2503,  12.1443,   8.8289,\n",
      "           9.9909,   9.7402,   6.8613,   6.2891,  14.8422,   9.3871,  12.9846,\n",
      "          10.1734,  20.4692,   6.6605,  10.9024,   7.3807],\n",
      "        [  6.8147,   7.0001,   7.1814,   9.7675,   8.2868,   8.3151,  12.7193,\n",
      "          14.7395,  25.5681,  36.8581,  23.7470,  14.0491,  14.0159,   9.8076,\n",
      "          11.5890,   9.6706,   6.7475,   5.8494,  25.5263,  14.0285,  14.8252,\n",
      "           7.7096,  16.7192,   5.8077,   8.9035,   6.4491],\n",
      "        [  6.3703,   6.1694,   6.0333,   8.1981,   7.5888,   6.1605,   8.2855,\n",
      "           9.4468,  14.3302,  23.7470,  36.8581,  23.4722,  17.0488,  10.1470,\n",
      "          11.8132,   9.4512,   6.3403,   5.4298,  14.0081,  11.7758,   9.3693,\n",
      "           6.4677,  14.0052,   5.0332,   7.8252,   5.7132],\n",
      "        [  5.5933,   5.2622,   5.2934,   7.3564,   6.7996,   5.4471,   6.9718,\n",
      "           7.6260,  10.2503,  14.0491,  23.4722,  36.8581,  29.0927,  14.5568,\n",
      "          16.6640,  11.6206,   7.3980,   5.9780,  10.9025,  10.4969,   7.9041,\n",
      "           6.1598,  13.4777,   5.1007,   7.9231,   5.9571],\n",
      "        [  7.1523,   6.5226,   6.7514,   9.9048,   9.2440,   6.5011,   8.1931,\n",
      "           9.2679,  12.1443,  14.0159,  17.0488,  29.0927,  53.3587,  30.6324,\n",
      "          24.3251,  21.7584,  11.6256,   8.9733,  10.9451,   9.7673,   8.7430,\n",
      "           8.6259,  18.1172,   7.0743,  11.9192,   8.7464],\n",
      "        [  5.4731,   5.0906,   5.4822,   8.0313,   7.0334,   5.6677,   6.9297,\n",
      "           7.4520,   8.8289,   9.8076,  10.1470,  14.5568,  30.6324,  36.8581,\n",
      "          39.2781,  30.5505,  14.7149,   9.5974,   8.6994,   7.9625,   7.4178,\n",
      "           7.5515,  16.4967,   6.8577,  10.9582,   8.6958],\n",
      "        [  6.2366,   5.9462,   6.4047,   9.0062,   7.6867,   7.2593,   8.6916,\n",
      "           8.8067,   9.9909,  11.5890,  11.8132,  16.6640,  24.3251,  39.2781,\n",
      "          73.5167,  24.3329,  17.0973,  11.1507,  11.3419,  11.2834,   9.7402,\n",
      "           8.4810,  19.7177,   8.2766,  11.7373,   9.9186],\n",
      "        [  6.8937,   6.2867,   6.9716,  10.6321,   9.1833,   6.6344,   7.8394,\n",
      "           8.6642,   9.7402,   9.6706,   9.4512,  11.6206,  21.7584,  30.5505,\n",
      "          24.3329,  53.3587,  28.8318,  16.9878,   8.5530,   7.5399,   7.8489,\n",
      "          10.7248,  22.6888,  10.0173,  17.6666,  14.2232],\n",
      "        [  5.1574,   4.8342,   5.5072,   8.0970,   6.5899,   5.6040,   6.2795,\n",
      "           6.5999,   6.8613,   6.7475,   6.3403,   7.3980,  11.6256,  14.7149,\n",
      "          17.0973,  28.8318,  36.8581,  23.4965,   6.4013,   5.7679,   6.2294,\n",
      "           8.6935,  20.4638,  10.2087,  14.0340,  14.5349],\n",
      "        [  5.4223,   5.0427,   5.8996,   8.7213,   6.9117,   5.4594,   5.8722,\n",
      "           6.2976,   6.2891,   5.8494,   5.4298,   5.9780,   8.9733,   9.5974,\n",
      "          11.1507,  16.9878,  23.4965,  36.8581,   5.4977,   4.8713,   5.5392,\n",
      "          10.0478,  24.4970,  14.1518,  17.1985,  24.0898],\n",
      "        [  5.5904,   5.9179,   6.2659,   8.1497,   6.5542,   9.4198,  14.6916,\n",
      "          12.8157,  14.8422,  25.5263,  14.0081,  10.9025,  10.9451,   8.6994,\n",
      "          11.3419,   8.5530,   6.4013,   5.4977,  36.8581,  23.8995,  25.5989,\n",
      "           6.7936,  15.9213,   5.6062,   7.7826,   5.9921],\n",
      "        [  4.6011,   4.7760,   4.9670,   6.3610,   5.2580,   7.1932,   9.4750,\n",
      "           8.3466,   9.3871,  14.0285,  11.7758,  10.4969,   9.7673,   7.9625,\n",
      "          11.2834,   7.5399,   5.7679,   4.8713,  23.8995,  36.8581,  14.4471,\n",
      "           5.4016,  13.0499,   4.7669,   6.3649,   5.0805],\n",
      "        [  5.4999,   6.0620,   6.7342,   8.4667,   6.3671,  14.2987,  25.7530,\n",
      "          14.9891,  12.9846,  14.8252,   9.3693,   7.9041,   8.7430,   7.4178,\n",
      "           9.7402,   7.8489,   6.2294,   5.5392,  25.5989,  14.4471,  36.8581,\n",
      "           7.2371,  17.9755,   6.1296,   7.9153,   6.2456],\n",
      "        [  9.8287,   9.6609,  14.2254,  29.3923,  14.2675,   8.2691,   9.0579,\n",
      "          11.6147,  10.1734,   7.7096,   6.4677,   6.1598,   8.6259,   7.5515,\n",
      "           8.4810,  10.7248,   8.6935,  10.0478,   6.7936,   5.4016,   7.2371,\n",
      "          36.8581,  57.2189,  14.6986,  31.7955,  16.4192],\n",
      "        [ 19.2464,  20.3849,  29.7155,  40.2325,  23.4317,  23.8000,  22.6689,\n",
      "          25.1006,  20.4692,  16.7192,  14.0052,  13.4777,  18.1172,  16.4967,\n",
      "          19.7177,  22.6888,  20.4638,  24.4970,  15.9213,  13.0499,  17.9755,\n",
      "          57.2189, 388.0234,  56.9243,  44.6624,  39.0895],\n",
      "        [  5.9475,   5.9245,   7.6567,  10.8645,   7.3602,   7.1756,   7.0971,\n",
      "           7.5057,   6.6605,   5.8077,   5.0332,   5.1007,   7.0743,   6.8577,\n",
      "           8.2766,  10.0173,  10.2087,  14.1518,   5.6062,   4.7669,   6.1296,\n",
      "          14.6986,  56.9243,  36.8581,  17.9391,  26.0354],\n",
      "        [  9.8111,   8.9414,  11.2840,  20.5774,  14.5760,   8.1207,   9.1428,\n",
      "          11.0967,  10.9024,   8.9035,   7.8252,   7.9231,  11.9192,  10.9582,\n",
      "          11.7373,  17.6666,  14.0340,  17.1985,   7.7826,   6.3649,   7.9153,\n",
      "          31.7955,  44.6624,  17.9391,  53.3587,  31.0209],\n",
      "        [  6.4739,   6.1266,   7.6310,  11.8579,   8.5394,   6.5895,   7.0231,\n",
      "           7.7757,   7.3807,   6.4491,   5.7132,   5.9571,   8.7464,   8.6958,\n",
      "           9.9186,  14.2232,  14.5349,  24.0898,   5.9921,   5.0805,   6.2456,\n",
      "          16.4192,  39.0895,  26.0354,  31.0209,  36.8581]])\n",
      "Frobenius norm of the original molecule (diag): tensor(570.9117)\n",
      "Frobenius norm of the original molecule (non diag): tensor(570.9119)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba701e2e88c94c35b2ff80a2812e3c04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V_j_plus_frobenius_norm tensor(46796.1562)\n",
      "V_j_plus_frobenius_norm_non_diag tensor(46796.1562)\n",
      "V_j_minus_frobenius_norm tensor(45406.8516)\n",
      "V_j_minus_frobenius_norm_non_diag tensor(45406.8516)\n",
      "V_random_frobenius_norm tensor(43084.5000)\n",
      "V_random_frobenius_norm_non_diag tensor(43084.5039)\n",
      "V_j_plus_frobenius_norm tensor(44216.2852)\n",
      "V_j_plus_frobenius_norm_non_diag tensor(44216.2852)\n",
      "V_j_minus_frobenius_norm tensor(44510.2773)\n",
      "V_j_minus_frobenius_norm_non_diag tensor(44510.2773)\n",
      "V_random_frobenius_norm tensor(43737.0859)\n",
      "V_random_frobenius_norm_non_diag tensor(43737.0898)\n",
      "V_j_plus_frobenius_norm tensor(46310.8906)\n",
      "V_j_plus_frobenius_norm_non_diag tensor(46310.8945)\n",
      "V_j_minus_frobenius_norm tensor(43166.4531)\n",
      "V_j_minus_frobenius_norm_non_diag tensor(43166.4531)\n",
      "V_random_frobenius_norm tensor(43978.5742)\n",
      "V_random_frobenius_norm_non_diag tensor(43978.5742)\n",
      "V_j_plus_frobenius_norm tensor(49634.3516)\n",
      "V_j_plus_frobenius_norm_non_diag tensor(49634.3555)\n",
      "V_j_minus_frobenius_norm tensor(43312.0898)\n",
      "V_j_minus_frobenius_norm_non_diag tensor(43312.0898)\n",
      "V_random_frobenius_norm tensor(45485.9570)\n",
      "V_random_frobenius_norm_non_diag tensor(45485.9609)\n",
      "V_j_plus_frobenius_norm tensor(46586.0820)\n",
      "V_j_plus_frobenius_norm_non_diag tensor(46586.0781)\n",
      "V_j_minus_frobenius_norm tensor(44725.6289)\n",
      "V_j_minus_frobenius_norm_non_diag tensor(44725.6250)\n",
      "V_random_frobenius_norm tensor(42922.1602)\n",
      "V_random_frobenius_norm_non_diag tensor(42922.1602)\n",
      "V_j_plus_frobenius_norm tensor(47126.5195)\n",
      "V_j_plus_frobenius_norm_non_diag tensor(47126.5195)\n",
      "V_j_minus_frobenius_norm tensor(46709.9219)\n",
      "V_j_minus_frobenius_norm_non_diag tensor(46709.9219)\n",
      "V_random_frobenius_norm tensor(44279.7383)\n",
      "V_random_frobenius_norm_non_diag tensor(44279.7344)\n",
      "V_j_plus_frobenius_norm tensor(45142.1445)\n",
      "V_j_plus_frobenius_norm_non_diag tensor(45142.1484)\n",
      "V_j_minus_frobenius_norm tensor(47174.7305)\n",
      "V_j_minus_frobenius_norm_non_diag tensor(47174.7305)\n",
      "V_random_frobenius_norm tensor(43947.1523)\n",
      "V_random_frobenius_norm_non_diag tensor(43947.1523)\n",
      "V_j_plus_frobenius_norm tensor(46662.9609)\n",
      "V_j_plus_frobenius_norm_non_diag tensor(46662.9609)\n",
      "V_j_minus_frobenius_norm tensor(48630.4531)\n",
      "V_j_minus_frobenius_norm_non_diag tensor(48630.4531)\n",
      "V_random_frobenius_norm tensor(44465.6758)\n",
      "V_random_frobenius_norm_non_diag tensor(44465.6758)\n",
      "V_j_plus_frobenius_norm tensor(46865.3477)\n",
      "V_j_plus_frobenius_norm_non_diag tensor(46865.3477)\n",
      "V_j_minus_frobenius_norm tensor(48435.7461)\n",
      "V_j_minus_frobenius_norm_non_diag tensor(48435.7461)\n",
      "V_random_frobenius_norm tensor(45110.5742)\n",
      "V_random_frobenius_norm_non_diag tensor(45110.5742)\n",
      "V_j_plus_frobenius_norm tensor(45689.3164)\n",
      "V_j_plus_frobenius_norm_non_diag tensor(45689.3164)\n",
      "V_j_minus_frobenius_norm tensor(44405.4062)\n",
      "V_j_minus_frobenius_norm_non_diag tensor(44405.4102)\n",
      "V_random_frobenius_norm tensor(44785.6680)\n",
      "V_random_frobenius_norm_non_diag tensor(44785.6680)\n",
      "V_j_plus_frobenius_norm tensor(45792.3984)\n",
      "V_j_plus_frobenius_norm_non_diag tensor(45792.3945)\n",
      "V_j_minus_frobenius_norm tensor(46432.5938)\n",
      "V_j_minus_frobenius_norm_non_diag tensor(46432.5898)\n",
      "V_random_frobenius_norm tensor(43665.9336)\n",
      "V_random_frobenius_norm_non_diag tensor(43665.9336)\n",
      "V_j_plus_frobenius_norm tensor(45011.6953)\n",
      "V_j_plus_frobenius_norm_non_diag tensor(45011.6953)\n",
      "V_j_minus_frobenius_norm tensor(46706.5508)\n",
      "V_j_minus_frobenius_norm_non_diag tensor(46706.5508)\n",
      "V_random_frobenius_norm tensor(43462.4375)\n",
      "V_random_frobenius_norm_non_diag tensor(43462.4414)\n",
      "V_j_plus_frobenius_norm tensor(46622.8555)\n",
      "V_j_plus_frobenius_norm_non_diag tensor(46622.8594)\n",
      "V_j_minus_frobenius_norm tensor(45850.0977)\n",
      "V_j_minus_frobenius_norm_non_diag tensor(45850.0977)\n",
      "V_random_frobenius_norm tensor(44697.6133)\n",
      "V_random_frobenius_norm_non_diag tensor(44697.6133)\n",
      "V_j_plus_frobenius_norm tensor(46026.8320)\n",
      "V_j_plus_frobenius_norm_non_diag tensor(46026.8320)\n",
      "V_j_minus_frobenius_norm tensor(46121.8633)\n",
      "V_j_minus_frobenius_norm_non_diag tensor(46121.8672)\n",
      "V_random_frobenius_norm tensor(43610.2930)\n",
      "V_random_frobenius_norm_non_diag tensor(43610.2969)\n",
      "V_j_plus_frobenius_norm tensor(46674.3438)\n",
      "V_j_plus_frobenius_norm_non_diag tensor(46674.3438)\n",
      "V_j_minus_frobenius_norm tensor(46781.2383)\n",
      "V_j_minus_frobenius_norm_non_diag tensor(46781.2383)\n",
      "V_random_frobenius_norm tensor(43747.6172)\n",
      "V_random_frobenius_norm_non_diag tensor(43747.6172)\n",
      "V_j_plus_frobenius_norm tensor(44147.6133)\n",
      "V_j_plus_frobenius_norm_non_diag tensor(44147.6133)\n",
      "V_j_minus_frobenius_norm tensor(45434.0664)\n",
      "V_j_minus_frobenius_norm_non_diag tensor(45434.0703)\n",
      "V_random_frobenius_norm tensor(44150.4492)\n",
      "V_random_frobenius_norm_non_diag tensor(44150.4492)\n",
      "V_j_plus_frobenius_norm tensor(46005.4219)\n",
      "V_j_plus_frobenius_norm_non_diag tensor(46005.4219)\n",
      "V_j_minus_frobenius_norm tensor(43492.4844)\n",
      "V_j_minus_frobenius_norm_non_diag tensor(43492.4844)\n",
      "V_random_frobenius_norm tensor(43699.9648)\n",
      "V_random_frobenius_norm_non_diag tensor(43699.9648)\n",
      "V_j_plus_frobenius_norm tensor(47601.6719)\n",
      "V_j_plus_frobenius_norm_non_diag tensor(47601.6758)\n",
      "V_j_minus_frobenius_norm tensor(43693.1055)\n",
      "V_j_minus_frobenius_norm_non_diag tensor(43693.1055)\n",
      "V_random_frobenius_norm tensor(42778.8633)\n",
      "V_random_frobenius_norm_non_diag tensor(42778.8633)\n",
      "V_j_plus_frobenius_norm tensor(46321.2930)\n",
      "V_j_plus_frobenius_norm_non_diag tensor(46321.2930)\n",
      "V_j_minus_frobenius_norm tensor(45905.4297)\n",
      "V_j_minus_frobenius_norm_non_diag tensor(45905.4336)\n",
      "V_random_frobenius_norm tensor(43600.0977)\n",
      "V_random_frobenius_norm_non_diag tensor(43600.0938)\n",
      "V_j_plus_frobenius_norm tensor(47376.0938)\n",
      "V_j_plus_frobenius_norm_non_diag tensor(47376.0938)\n",
      "V_j_minus_frobenius_norm tensor(47530.8711)\n",
      "V_j_minus_frobenius_norm_non_diag tensor(47530.8711)\n",
      "V_random_frobenius_norm tensor(44066.2812)\n",
      "V_random_frobenius_norm_non_diag tensor(44066.2812)\n",
      "V_j_plus_frobenius_norm tensor(45978.9805)\n",
      "V_j_plus_frobenius_norm_non_diag tensor(45978.9805)\n",
      "V_j_minus_frobenius_norm tensor(48086.2227)\n",
      "V_j_minus_frobenius_norm_non_diag tensor(48086.2227)\n",
      "V_random_frobenius_norm tensor(44722.6992)\n",
      "V_random_frobenius_norm_non_diag tensor(44722.7031)\n",
      "['Cc1ccc(CCNC(=O)NCCc2csc(N3CCCC3)n2)c(C)c1']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152778/1366132971.py:181: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning dissapear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  imgs = [imageio.imread(fn) for fn in save_paths]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using P: 0.5 0.5\n",
      "Coulomb matrix (diag):  tensor([[  1.2210,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "        [  0.0000,   1.4088,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   3.1401,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000,   5.1357,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000,   0.0000,   5.7776,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   6.9945,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   7.8054,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           8.3094,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,  11.4803,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,  12.4039,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,  15.9451,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,  19.6285,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,  20.4884,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,  23.8601,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          26.3284,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,  27.1413,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,  29.9483,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,  37.1510,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,  38.8382,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,  44.8204,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,  52.3457,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          71.3782,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,  97.6209,   0.0000,   0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,  98.0171,   0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000, 211.7882,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000, 480.0015]])\n",
      "Coulomb matrix (non diag)  tensor([[ 36.8581,  24.0075,  14.3190,   9.4772,   8.3151,   6.1605,   5.4471,\n",
      "           9.4198,   7.1932,  14.2987,   7.0976,  23.2106,   8.1339,   9.4275,\n",
      "           6.3791,   5.7190,   6.7551,   8.2996,   8.0632,   6.5564,   4.7778,\n",
      "           5.6680,   5.3565,   5.4676,   6.7996,   5.4438],\n",
      "        [ 24.0075,  36.8581,  25.8246,  14.8648,  12.7193,   8.2855,   6.9718,\n",
      "          14.6916,   9.4750,  25.7530,   7.0254,  22.1165,   8.9199,  10.7868,\n",
      "           7.4863,   6.4382,   7.4445,   8.8033,   9.0741,   6.9834,   5.7807,\n",
      "           6.6400,   5.9621,   6.8564,   8.3718,   5.8271],\n",
      "        [ 14.3190,  25.8246,  36.8581,  25.7796,  14.7395,   9.4468,   7.6260,\n",
      "          12.8157,   8.3466,  14.9891,   7.4821,  24.6663,  11.4912,  15.6767,\n",
      "          10.4205,   8.5249,  10.0158,  12.0474,  11.0448,   7.7403,   6.7514,\n",
      "           7.1772,   6.3192,   7.7776,   9.1928,   6.2337],\n",
      "        [  9.4772,  14.8648,  25.7796,  36.8581,  25.5681,  14.3302,  10.2503,\n",
      "          14.8422,   9.3871,  12.9846,   6.6466,  20.2002,  10.1420,  14.1789,\n",
      "          11.5057,   8.9639,   9.4906,   9.9718,  10.8734,   7.3509,   8.6051,\n",
      "           7.9263,   6.5806,  10.1649,  11.0203,   6.1971],\n",
      "        [  8.3151,  12.7193,  14.7395,  25.5681,  36.8581,  23.7470,  14.0491,\n",
      "          25.5263,  14.0285,  14.8252,   5.7716,  16.4591,   7.6694,   9.7675,\n",
      "           8.2868,   6.8147,   7.0001,   7.1814,   8.8622,   6.4190,   8.8032,\n",
      "           7.9878,   6.4199,  11.5876,  11.9322,   5.7583],\n",
      "        [  6.1605,   8.2855,   9.4468,  14.3302,  23.7470,  36.8581,  23.4722,\n",
      "          14.0081,  11.7758,   9.3693,   5.0118,  13.8436,   6.4597,   8.1981,\n",
      "           7.5888,   6.3703,   6.1694,   6.0333,   7.8064,   5.6930,  10.8304,\n",
      "           7.7397,   6.0999,  14.2371,  12.1979,   5.3376],\n",
      "        [  5.4471,   6.9718,   7.6260,  10.2503,  14.0491,  23.4722,  36.8581,\n",
      "          10.9025,  10.4969,   7.9041,   5.0698,  13.3004,   6.1455,   7.3564,\n",
      "           6.7997,   5.5933,   5.2622,   5.2934,   7.8941,   5.9341,  13.9118,\n",
      "           9.5495,   7.0989,  23.9927,  16.8744,   5.8575],\n",
      "        [  9.4198,  14.6916,  12.8157,  14.8422,  25.5263,  14.0081,  10.9025,\n",
      "          36.8581,  23.8995,  25.5989,   5.5480,  15.6144,   6.7319,   8.1497,\n",
      "           6.5542,   5.5904,   5.9179,   6.2659,   7.7303,   5.9595,   6.9111,\n",
      "           7.2605,   6.0411,   9.0555,  10.2789,   5.4272],\n",
      "        [  7.1932,   9.4750,   8.3466,   9.3871,  14.0285,  11.7758,  10.4969,\n",
      "          23.8995,  36.8581,  14.4471,   4.7145,  12.8206,   5.3594,   6.3610,\n",
      "           5.2580,   4.6011,   4.7760,   4.9670,   6.3254,   5.0550,   6.2124,\n",
      "           6.4874,   5.4582,   8.1113,   9.1384,   4.8126],\n",
      "        [ 14.2987,  25.7530,  14.9891,  12.9846,  14.8252,   9.3693,   7.9041,\n",
      "          25.5989,  14.4471,  36.8581,   6.0523,  17.5597,   7.1445,   8.4667,\n",
      "           6.3671,   5.4999,   6.0620,   6.7342,   7.8529,   6.2086,   5.8570,\n",
      "           6.7165,   5.8742,   7.2796,   8.8052,   5.4871],\n",
      "        [  7.0976,   7.0254,   7.4821,   6.6466,   5.7716,   5.0118,   5.0698,\n",
      "           5.5480,   4.7145,   6.0523,  36.8581,  56.0935,  14.6912,  11.0707,\n",
      "           7.4786,   6.0409,   6.0042,   7.7865,  18.0216,  26.1473,   5.6134,\n",
      "           8.5061,   9.9460,   5.9765,   8.2891,  14.3210],\n",
      "        [ 23.2106,  22.1165,  24.6663,  20.2002,  16.4591,  13.8436,  13.3004,\n",
      "          15.6144,  12.8206,  17.5597,  56.0935, 388.0234,  55.4035,  40.8949,\n",
      "          23.7064,  19.5797,  20.7432,  30.5018,  43.7776,  37.9114,  14.3905,\n",
      "          18.7104,  19.6761,  15.1669,  19.8756,  24.1538],\n",
      "        [  8.1339,   8.9199,  11.4912,  10.1420,   7.6694,   6.4597,   6.1455,\n",
      "           6.7319,   5.3594,   7.1445,  14.6912,  55.4035,  36.8581,  30.8464,\n",
      "          14.7648,  10.0782,   9.8370,  14.5050,  31.8896,  16.0720,   7.0902,\n",
      "           8.6515,   8.4890,   7.3215,   9.4437,   9.8653],\n",
      "        [  9.4275,  10.7868,  15.6767,  14.1789,   9.7675,   8.1981,   7.3564,\n",
      "           8.1497,   6.3610,   8.4667,  11.0707,  40.8949,  30.8464,  53.3587,\n",
      "          28.8300,  17.8637,  17.9081,  28.8410,  20.8573,  11.8718,   8.4083,\n",
      "           8.5790,   7.9989,   8.4419,  10.0936,   8.6440],\n",
      "        [  6.3791,   7.4863,  10.4205,  11.5057,   8.2868,   7.5888,   6.7997,\n",
      "           6.5542,   5.2580,   6.3671,   7.4786,  23.7064,  14.7648,  28.8300,\n",
      "          36.8581,  23.5823,  15.0934,  14.9438,  14.8301,   8.5575,   8.6434,\n",
      "           7.2817,   6.5771,   7.9456,   8.8865,   6.8238],\n",
      "        [  5.7190,   6.4382,   8.5249,   8.9639,   6.8147,   6.3703,   5.5933,\n",
      "           5.5904,   4.6011,   5.4999,   6.0409,  19.5797,  10.0782,  17.8637,\n",
      "          23.5823,  36.8581,  23.7211,  15.1551,   9.9334,   6.4885,   6.7072,\n",
      "           5.5666,   5.1549,   6.1496,   6.7844,   5.3760],\n",
      "        [  6.7551,   7.4445,  10.0158,   9.4906,   7.0001,   6.1694,   5.2622,\n",
      "           5.9179,   4.7760,   6.0620,   6.0042,  20.7432,   9.8370,  17.9081,\n",
      "          15.0934,  23.7211,  36.8581,  23.5833,   9.0146,   6.1330,   5.7265,\n",
      "           5.1387,   4.7898,   5.5767,   6.2667,   5.0106],\n",
      "        [  8.2996,   8.8033,  12.0474,   9.9718,   7.1814,   6.0333,   5.2934,\n",
      "           6.2659,   4.9670,   6.7342,   7.7865,  30.5018,  14.5050,  28.8410,\n",
      "          14.9438,  15.1551,  23.5833,  36.8581,  11.3705,   7.6386,   5.7155,\n",
      "           5.7210,   5.4389,   5.7531,   6.7703,   5.8732],\n",
      "        [  8.0632,   9.0741,  11.0448,  10.8734,   8.8622,   7.8064,   7.8941,\n",
      "           7.7303,   6.3254,   7.8529,  18.0216,  43.7776,  31.8896,  20.8573,\n",
      "          14.8301,   9.9334,   9.0146,  11.3705,  53.3587,  30.1642,   9.8489,\n",
      "          13.8564,  13.8448,  10.1207,  13.9108,  16.6035],\n",
      "        [  6.5564,   6.9834,   7.7403,   7.3509,   6.4190,   5.6930,   5.9341,\n",
      "           5.9595,   5.0550,   6.2086,  26.1473,  37.9114,  16.0720,  11.8718,\n",
      "           8.5575,   6.4885,   6.1330,   7.6386,  30.1642,  36.8581,   6.9921,\n",
      "          11.9290,  14.5930,   7.4133,  10.7522,  24.1855],\n",
      "        [  4.7778,   5.7807,   6.7514,   8.6051,   8.8032,  10.8304,  13.9118,\n",
      "           6.9111,   6.2124,   5.8570,   5.6134,  14.3905,   7.0902,   8.4083,\n",
      "           8.6434,   6.7072,   5.7265,   5.7155,   9.8489,   6.9921,  36.8581,\n",
      "          11.2813,   8.4221,  23.8121,  17.4481,   7.0315],\n",
      "        [  5.6680,   6.6400,   7.1772,   7.9263,   7.9878,   7.7397,   9.5495,\n",
      "           7.2605,   6.4874,   6.7165,   8.5061,  18.7104,   8.6515,   8.5790,\n",
      "           7.2817,   5.5666,   5.1387,   5.7210,  13.8564,  11.9290,  11.2813,\n",
      "          36.8581,  27.6348,  14.3727,  32.7875,  14.3253],\n",
      "        [  5.3565,   5.9621,   6.3192,   6.5806,   6.4199,   6.0999,   7.0989,\n",
      "           6.0411,   5.4582,   5.8742,   9.9460,  19.6761,   8.4890,   7.9989,\n",
      "           6.5771,   5.1549,   4.7898,   5.4389,  13.8448,  14.5930,   8.4221,\n",
      "          27.6348,  36.8581,   9.5041,  16.2620,  25.1443],\n",
      "        [  5.4676,   6.8564,   7.7776,  10.1649,  11.5876,  14.2371,  23.9927,\n",
      "           9.0555,   8.1113,   7.2796,   5.9765,  15.1669,   7.3215,   8.4419,\n",
      "           7.9456,   6.1496,   5.5767,   5.7531,  10.1207,   7.4133,  23.8121,\n",
      "          14.3727,   9.5041,  36.8581,  32.0830,   7.4574],\n",
      "        [  6.7996,   8.3718,   9.1928,  11.0203,  11.9322,  12.1979,  16.8744,\n",
      "          10.2789,   9.1384,   8.8052,   8.2891,  19.8756,   9.4437,  10.0936,\n",
      "           8.8865,   6.7844,   6.2667,   6.7703,  13.9108,  10.7522,  17.4481,\n",
      "          32.7875,  16.2620,  32.0830,  53.3587,  11.3122],\n",
      "        [  5.4438,   5.8271,   6.2337,   6.1971,   5.7583,   5.3376,   5.8575,\n",
      "           5.4272,   4.8126,   5.4871,  14.3210,  24.1538,   9.8653,   8.6440,\n",
      "           6.8238,   5.3760,   5.0106,   5.8732,  16.6035,  24.1855,   7.0315,\n",
      "          14.3253,  25.1443,   7.4574,  11.3122,  36.8581]])\n",
      "Frobenius norm of the original molecule (diag): tensor(558.2277)\n",
      "Frobenius norm of the original molecule (non diag): tensor(558.2278)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a22e745b05084be4bbfddcddd043e9bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V_j_plus_frobenius_norm tensor(59779.4062)\n",
      "V_j_plus_frobenius_norm_non_diag tensor(59779.4062)\n",
      "V_j_minus_frobenius_norm tensor(60327.6250)\n",
      "V_j_minus_frobenius_norm_non_diag tensor(60327.6328)\n",
      "V_random_frobenius_norm tensor(60292.9531)\n",
      "V_random_frobenius_norm_non_diag tensor(60292.9531)\n",
      "V_j_plus_frobenius_norm tensor(58957.9648)\n",
      "V_j_plus_frobenius_norm_non_diag tensor(58957.9766)\n",
      "V_j_minus_frobenius_norm tensor(59311.0742)\n",
      "V_j_minus_frobenius_norm_non_diag tensor(59311.0781)\n",
      "V_random_frobenius_norm tensor(60263.6641)\n",
      "V_random_frobenius_norm_non_diag tensor(60263.6602)\n",
      "V_j_plus_frobenius_norm tensor(59659.4727)\n",
      "V_j_plus_frobenius_norm_non_diag tensor(59659.4609)\n",
      "V_j_minus_frobenius_norm tensor(60043.8516)\n",
      "V_j_minus_frobenius_norm_non_diag tensor(60043.8516)\n",
      "V_random_frobenius_norm tensor(60341.4258)\n",
      "V_random_frobenius_norm_non_diag tensor(60341.4219)\n",
      "V_j_plus_frobenius_norm tensor(59236.5625)\n",
      "V_j_plus_frobenius_norm_non_diag tensor(59236.5625)\n",
      "V_j_minus_frobenius_norm tensor(60700.9766)\n",
      "V_j_minus_frobenius_norm_non_diag tensor(60700.9766)\n",
      "V_random_frobenius_norm tensor(60028.7500)\n",
      "V_random_frobenius_norm_non_diag tensor(60028.7500)\n",
      "V_j_plus_frobenius_norm tensor(58655.5547)\n",
      "V_j_plus_frobenius_norm_non_diag tensor(58655.5508)\n",
      "V_j_minus_frobenius_norm tensor(59152.9570)\n",
      "V_j_minus_frobenius_norm_non_diag tensor(59152.9648)\n",
      "V_random_frobenius_norm tensor(60536.9219)\n",
      "V_random_frobenius_norm_non_diag tensor(60536.9219)\n",
      "V_j_plus_frobenius_norm tensor(59924.8945)\n",
      "V_j_plus_frobenius_norm_non_diag tensor(59924.9023)\n",
      "V_j_minus_frobenius_norm tensor(61537.8164)\n",
      "V_j_minus_frobenius_norm_non_diag tensor(61537.8125)\n",
      "V_random_frobenius_norm tensor(64336.4297)\n",
      "V_random_frobenius_norm_non_diag tensor(64336.4336)\n",
      "V_j_plus_frobenius_norm tensor(59338.7656)\n",
      "V_j_plus_frobenius_norm_non_diag tensor(59338.7656)\n",
      "V_j_minus_frobenius_norm tensor(62318.0430)\n",
      "V_j_minus_frobenius_norm_non_diag tensor(62318.0508)\n",
      "V_random_frobenius_norm tensor(59501.6055)\n",
      "V_random_frobenius_norm_non_diag tensor(59501.6133)\n",
      "V_j_plus_frobenius_norm tensor(58701.6641)\n",
      "V_j_plus_frobenius_norm_non_diag tensor(58701.6758)\n",
      "V_j_minus_frobenius_norm tensor(59911.3555)\n",
      "V_j_minus_frobenius_norm_non_diag tensor(59911.3594)\n",
      "V_random_frobenius_norm tensor(59927.1250)\n",
      "V_random_frobenius_norm_non_diag tensor(59927.1250)\n",
      "V_j_plus_frobenius_norm tensor(60114.8672)\n",
      "V_j_plus_frobenius_norm_non_diag tensor(60114.8711)\n",
      "V_j_minus_frobenius_norm tensor(60932.5664)\n",
      "V_j_minus_frobenius_norm_non_diag tensor(60932.5625)\n",
      "V_random_frobenius_norm tensor(58838.9023)\n",
      "V_random_frobenius_norm_non_diag tensor(58838.9062)\n",
      "V_j_plus_frobenius_norm tensor(60561.9844)\n",
      "V_j_plus_frobenius_norm_non_diag tensor(60561.9805)\n",
      "V_j_minus_frobenius_norm tensor(59922.4805)\n",
      "V_j_minus_frobenius_norm_non_diag tensor(59922.4844)\n",
      "V_random_frobenius_norm tensor(62942.6367)\n",
      "V_random_frobenius_norm_non_diag tensor(62942.6367)\n",
      "V_j_plus_frobenius_norm tensor(58737.8789)\n",
      "V_j_plus_frobenius_norm_non_diag tensor(58737.8828)\n",
      "V_j_minus_frobenius_norm tensor(59599.5000)\n",
      "V_j_minus_frobenius_norm_non_diag tensor(59599.5000)\n",
      "V_random_frobenius_norm tensor(59914.0625)\n",
      "V_random_frobenius_norm_non_diag tensor(59914.0703)\n",
      "V_j_plus_frobenius_norm tensor(59476.1953)\n",
      "V_j_plus_frobenius_norm_non_diag tensor(59476.2031)\n",
      "V_j_minus_frobenius_norm tensor(63293.3477)\n",
      "V_j_minus_frobenius_norm_non_diag tensor(63293.3516)\n",
      "V_random_frobenius_norm tensor(61124.4180)\n",
      "V_random_frobenius_norm_non_diag tensor(61124.4180)\n",
      "V_j_plus_frobenius_norm tensor(60858.2422)\n",
      "V_j_plus_frobenius_norm_non_diag tensor(60858.2578)\n",
      "V_j_minus_frobenius_norm tensor(60235.6992)\n",
      "V_j_minus_frobenius_norm_non_diag tensor(60235.6953)\n",
      "V_random_frobenius_norm tensor(61324.3867)\n",
      "V_random_frobenius_norm_non_diag tensor(61324.3867)\n",
      "V_j_plus_frobenius_norm tensor(60136.5156)\n",
      "V_j_plus_frobenius_norm_non_diag tensor(60136.5195)\n",
      "V_j_minus_frobenius_norm tensor(60285.1055)\n",
      "V_j_minus_frobenius_norm_non_diag tensor(60285.1055)\n",
      "V_random_frobenius_norm tensor(59634.6719)\n",
      "V_random_frobenius_norm_non_diag tensor(59634.6797)\n",
      "V_j_plus_frobenius_norm tensor(62759.2305)\n",
      "V_j_plus_frobenius_norm_non_diag tensor(62759.2422)\n",
      "V_j_minus_frobenius_norm tensor(59775.8633)\n",
      "V_j_minus_frobenius_norm_non_diag tensor(59775.8633)\n",
      "V_random_frobenius_norm tensor(60080.5859)\n",
      "V_random_frobenius_norm_non_diag tensor(60080.5938)\n",
      "V_j_plus_frobenius_norm tensor(61101.7461)\n",
      "V_j_plus_frobenius_norm_non_diag tensor(61101.7383)\n",
      "V_j_minus_frobenius_norm tensor(59595.8828)\n",
      "V_j_minus_frobenius_norm_non_diag tensor(59595.8828)\n",
      "V_random_frobenius_norm tensor(63195.7500)\n",
      "V_random_frobenius_norm_non_diag tensor(63195.7461)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 67>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    594\u001b[0m chain_j_plus_batch, node_mask_j_plus_batch \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39msample_chain(data_j_plus_batch, keep_frames\u001b[38;5;241m=\u001b[39mkeep_frames, noisy_positions\u001b[38;5;241m=\u001b[39mnoisy_positions_batch_j_plus, noisy_features\u001b[38;5;241m=\u001b[39mnoisy_features_batch_j_plus)\n\u001b[1;32m    596\u001b[0m chain_j_plus \u001b[38;5;241m=\u001b[39m chain_j_plus_batch[\u001b[38;5;241m0\u001b[39m, :, :, :] \u001b[38;5;66;03m#it should take the first frame and all batch elements -> check it is really the first frame (I need the one at t0, the final generated molecule)\u001b[39;00m\n\u001b[0;32m--> 599\u001b[0m chain_j_minus_batch, node_mask_j_minus_batch \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_j_minus_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_frames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_frames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoisy_positions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnoisy_positions_batch_j_minus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoisy_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnoisy_features_batch_j_minus\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    601\u001b[0m chain_j_minus \u001b[38;5;241m=\u001b[39m chain_j_minus_batch[\u001b[38;5;241m0\u001b[39m, :, :, :]\n\u001b[1;32m    603\u001b[0m chain_random_batch, node_mask_random_batch \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39msample_chain(data_random_batch, keep_frames\u001b[38;5;241m=\u001b[39mkeep_frames, noisy_positions\u001b[38;5;241m=\u001b[39mnoisy_positions_batch_random, noisy_features\u001b[38;5;241m=\u001b[39mnoisy_features_batch_random)\n",
      "File \u001b[0;32m~/Repositories/DiffSHAPer/difflinker/src/lightning.py:458\u001b[0m, in \u001b[0;36mDDPM.sample_chain\u001b[0;34m(self, data, sample_fn, keep_frames, noisy_positions, noisy_features)\u001b[0m\n\u001b[1;32m    455\u001b[0m x \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mremove_partial_mean_with_mask(x, node_mask, center_of_mass_mask)\n\u001b[1;32m    457\u001b[0m \u001b[38;5;66;03m#@mastro edited, added noisy_positions and noisy_features\u001b[39;00m\n\u001b[0;32m--> 458\u001b[0m chain \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_chain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m    \u001b[49m\u001b[43mh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnode_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnode_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[43m    \u001b[49m\u001b[43medge_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfragment_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfragment_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    464\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlinker_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlinker_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_frames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnoisy_positions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnoisy_positions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnoisy_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnoisy_features\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m chain, node_mask\n",
      "File \u001b[0;32m~/anaconda3/envs/diff_explainer/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Repositories/DiffSHAPer/difflinker/src/edm.py:164\u001b[0m, in \u001b[0;36mEDM.sample_chain\u001b[0;34m(self, x, h, node_mask, fragment_mask, linker_mask, edge_mask, context, keep_frames, noisy_positions, noisy_features)\u001b[0m\n\u001b[1;32m    161\u001b[0m s_array \u001b[38;5;241m=\u001b[39m s_array \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m    162\u001b[0m t_array \u001b[38;5;241m=\u001b[39m t_array \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m--> 164\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_p_zs_given_zt_only_linker\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m    \u001b[49m\u001b[43ms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ms_array\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m    \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt_array\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m    \u001b[49m\u001b[43mz_t\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnode_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnode_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfragment_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfragment_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlinker_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlinker_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m    \u001b[49m\u001b[43medge_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m write_index \u001b[38;5;241m=\u001b[39m (s \u001b[38;5;241m*\u001b[39m keep_frames) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m    175\u001b[0m chain[write_index] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnormalize_z(z)\n",
      "File \u001b[0;32m~/Repositories/DiffSHAPer/difflinker/src/edm.py:203\u001b[0m, in \u001b[0;36mEDM.sample_p_zs_given_zt_only_linker\u001b[0;34m(self, s, t, z_t, node_mask, fragment_mask, linker_mask, edge_mask, context)\u001b[0m\n\u001b[1;32m    200\u001b[0m sigma_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigma(gamma_t, target_tensor\u001b[38;5;241m=\u001b[39mz_t)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;66;03m# Neural net prediction.\u001b[39;00m\n\u001b[0;32m--> 203\u001b[0m eps_hat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdynamics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mz_t\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m    \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnode_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnode_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlinker_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlinker_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m    \u001b[49m\u001b[43medge_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    211\u001b[0m eps_hat \u001b[38;5;241m=\u001b[39m eps_hat \u001b[38;5;241m*\u001b[39m linker_mask\n\u001b[1;32m    213\u001b[0m \u001b[38;5;66;03m# Compute mu for p(z_s | z_t)\u001b[39;00m\n",
      "File \u001b[0;32m~/Repositories/DiffSHAPer/difflinker/src/egnn.py:449\u001b[0m, in \u001b[0;36mDynamics.forward\u001b[0;34m(self, t, xh, node_mask, linker_mask, edge_mask, context)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;66;03m# Forward EGNN\u001b[39;00m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;66;03m# Output: h_final (B*N, nf), x_final (B*N, 3), vel (B*N, 3)\u001b[39;00m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124megnn_dynamics\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 449\u001b[0m     h_final, x_final \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdynamics\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[43m        \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43medges\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnode_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnode_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlinker_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlinker_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43medge_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_mask\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    457\u001b[0m     node_mask \u001b[38;5;241m=\u001b[39m node_mask\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mrunning_device) \u001b[38;5;66;03m#@mastro moved node_mask to device\u001b[39;00m\n\u001b[1;32m    458\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mrunning_device) \u001b[38;5;66;03m#@mastro moved x to device \u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/diff_explainer/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/diff_explainer/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Repositories/DiffSHAPer/difflinker/src/egnn.py:252\u001b[0m, in \u001b[0;36mEGNN.forward\u001b[0;34m(self, h, x, edge_index, node_mask, linker_mask, edge_mask)\u001b[0m\n\u001b[1;32m    249\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(h) \u001b[38;5;66;03m#@mastro moved h to device of embedding (hopefully the selected GPU)\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_layers):\n\u001b[0;32m--> 252\u001b[0m     h, x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_modules\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43me_block_\u001b[39;49m\u001b[38;5;132;43;01m%d\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnode_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnode_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlinker_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlinker_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[43medge_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdistances\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;66;03m# Important, the bias of the last linear might be non-zero\u001b[39;00m\n\u001b[1;32m    261\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_out(h) \u001b[38;5;66;03m#@mastro moved h to device of embedding (hopefully the selected GPU)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/diff_explainer/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/diff_explainer/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Repositories/DiffSHAPer/difflinker/src/egnn.py:186\u001b[0m, in \u001b[0;36mEquivariantBlock.forward\u001b[0;34m(self, h, x, edge_index, node_mask, linker_mask, edge_mask, edge_attr)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_layers):\n\u001b[1;32m    185\u001b[0m     h, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgcl_\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m i](h, edge_index, edge_attr\u001b[38;5;241m=\u001b[39medge_attr, node_mask\u001b[38;5;241m=\u001b[39mnode_mask, edge_mask\u001b[38;5;241m=\u001b[39medge_mask)\n\u001b[0;32m--> 186\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_modules\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgcl_equiv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m    \u001b[49m\u001b[43medge_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoord_diff\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoord_diff\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_attr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlinker_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlinker_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnode_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnode_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m    \u001b[49m\u001b[43medge_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# Important, the bias of the last linear might be non-zero\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m node_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/diff_explainer/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/diff_explainer/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Repositories/DiffSHAPer/difflinker/src/egnn.py:141\u001b[0m, in \u001b[0;36mEquivariantUpdate.forward\u001b[0;34m(self, h, coord, edge_index, coord_diff, edge_attr, linker_mask, node_mask, edge_mask)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    139\u001b[0m         \u001b[38;5;28mself\u001b[39m, h, coord, edge_index, coord_diff, edge_attr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, linker_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, node_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, edge_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    140\u001b[0m ):\n\u001b[0;32m--> 141\u001b[0m     coord \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoord_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoord\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoord_diff\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlinker_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m node_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    143\u001b[0m         node_mask \u001b[38;5;241m=\u001b[39m node_mask\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mrunning_device) \u001b[38;5;66;03m#@mastro moved node_mask to device (hopefully the selected GPU)\u001b[39;00m\n",
      "File \u001b[0;32m~/Repositories/DiffSHAPer/difflinker/src/egnn.py:123\u001b[0m, in \u001b[0;36mEquivariantUpdate.coord_model\u001b[0;34m(self, h, coord, edge_index, coord_diff, edge_attr, edge_mask, linker_mask)\u001b[0m\n\u001b[1;32m    121\u001b[0m     trans \u001b[38;5;241m=\u001b[39m coord_diff \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mtanh(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoord_mlp(input_tensor)) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoords_range\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 123\u001b[0m     trans \u001b[38;5;241m=\u001b[39m coord_diff \u001b[38;5;241m*\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoord_mlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m edge_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    125\u001b[0m     edge_mask \u001b[38;5;241m=\u001b[39m edge_mask\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mrunning_device) \u001b[38;5;66;03m#@mastro moved edge_mask to device (hopefully the selected GPU)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/diff_explainer/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/diff_explainer/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/diff_explainer/lib/python3.10/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/diff_explainer/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/diff_explainer/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/diff_explainer/lib/python3.10/site-packages/torch/nn/modules/linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#@mastro\n",
    "torch.set_printoptions(threshold=float('inf'))\n",
    "\n",
    "num_samples = NUM_SAMPLES\n",
    "sampled = 0\n",
    "#end @mastro\n",
    "start = 0\n",
    "\n",
    "chain_with_full_fragments = None\n",
    "\n",
    "# P = None #probability of atom to exist in random graph (also edge in the future)\n",
    "\n",
    "# Create the folder if it does not exist\n",
    "folder_save_path = \"results/explanations_coulomb_\" + P + \"_seed_\" + str(SEED) + \"_full_molecule_original_fragments\"\n",
    "\n",
    "if DIAGONALIZE:\n",
    "    folder_save_path = \"results/explanations_diagonalized_coulomb_\" + P + \"_seed_\" + str(SEED) + \"_full_molecule_original_fragments\"\n",
    "    \n",
    "if not os.path.exists(folder_save_path):\n",
    "    os.makedirs(folder_save_path)\n",
    "\n",
    "data_list = []\n",
    "for data in dataloader:\n",
    "\n",
    "    if sampled < num_samples:\n",
    "        data_list.append(data)\n",
    "        sampled += 1\n",
    "\n",
    "#determine max numebr of atoms of the molecules in the dataset. This is used to determine the size of the random noise, which we want to be equal for all molecules -> atoms not present in the molecule will be discarded using masks \n",
    "max_num_atoms = max(data[\"positions\"].shape[1] for data in data_list)\n",
    "\n",
    "\n",
    "#define initial random noise for positions and features #shape = [1, max_num_atoms, 3] for positions and [1, max_num_atoms, 8] for features. 1 since batch size is 1 for our explaination task\n",
    "pos_size = (data_list[0][\"positions\"].shape[0], max_num_atoms, data_list[0][\"positions\"].shape[2])\n",
    "feature_size = (data_list[0][\"one_hot\"].shape[0], max_num_atoms, data_list[0][\"one_hot\"].shape[2])\n",
    "\n",
    "INTIAL_DISTIBUTION_PATH = \"results/explanations_\" + P + \"_seed_\" + str(SEED)\n",
    "noisy_features = None\n",
    "noisy_positions = None\n",
    "#check if the initial distribution of the noisy features and positions already exists, if not create it\n",
    "if os.path.exists(INTIAL_DISTIBUTION_PATH + \"/noisy_features_seed_\" + str(SEED) + \".pt\"):\n",
    "    # load initial distrubution of noisy features and positions\n",
    "    print(\"Loading initial distribution of noisy features and positions.\")\n",
    "    noisy_features = torch.load(INTIAL_DISTIBUTION_PATH + \"/noisy_features_seed_\" + str(SEED) + \".pt\", map_location=device, weights_only=True)\n",
    "    noisy_positions = torch.load(INTIAL_DISTIBUTION_PATH + \"/noisy_positions_seed_\" + str(SEED) + \".pt\", map_location=device, weights_only=True)\n",
    "\n",
    "else:\n",
    "    print(\"Creating initial distribution of noisy features and positions.\")\n",
    "    noisy_positions = torch.randn(pos_size, device=device)\n",
    "    noisy_features = torch.randn(feature_size, device=device)\n",
    "\n",
    "\n",
    "    #save the noisy positions and features on file .txt\n",
    "    print(\"Saving noisy features and positions to .txt and .pt files.\")\n",
    "    noisy_positions_file = os.path.join(folder_save_path, \"noisy_positions_seed_\" + str(SEED) + \".txt\")\n",
    "    noisy_features_file = os.path.join(folder_save_path, \"noisy_features_seed_\" + str(SEED) + \".txt\")\n",
    "\n",
    "    with open(noisy_positions_file, \"w\") as f:\n",
    "        f.write(str(noisy_positions))\n",
    "\n",
    "    with open(noisy_features_file, \"w\") as f:\n",
    "        f.write(str(noisy_features))\n",
    "\n",
    "    torch.save(noisy_positions, os.path.join(folder_save_path, \"noisy_positions_seed_\" + str(SEED) + \".pt\"))\n",
    "    torch.save(noisy_features, os.path.join(folder_save_path, \"noisy_features_seed_\" + str(SEED) + \".pt\"))\n",
    "\n",
    "for data_index, data in enumerate(tqdm(data_list)): #7:\n",
    "\n",
    "        # start_time = time.time()\n",
    "        \n",
    "        smile = data[\"name\"][0]\n",
    "        \n",
    "        mol = read_smiles(smile)\n",
    "        num_nodes = mol.number_of_nodes()\n",
    "        \n",
    "        num_edges = mol.number_of_edges()\n",
    "        num_edges_directed = num_edges*2\n",
    "        \n",
    "        \n",
    "        graph_density = num_edges_directed/(num_nodes*(num_nodes-1))\n",
    "        max_number_of_nodes = num_edges + 1\n",
    "\n",
    "        node_density = num_nodes/max_number_of_nodes\n",
    "\n",
    "        node_edge_ratio = num_nodes/num_edges\n",
    "        \n",
    "        edge_node_ratio = num_edges/num_nodes\n",
    "        \n",
    "        if P == \"graph_density\":\n",
    "            P = graph_density #probability of atom to exist in random graph\n",
    "        elif P == \"node_density\":\n",
    "            P = node_density\n",
    "        elif P == \"node_edge_ratio\" or P == \"edge_node_ratio\":\n",
    "            if node_edge_ratio < edge_node_ratio:\n",
    "                P = node_edge_ratio\n",
    "                print(\"Using node-edge ratio\", node_edge_ratio)\n",
    "            else:\n",
    "                P = edge_node_ratio\n",
    "                print(\"Using edge-node ratio\", edge_node_ratio)            \n",
    "        else:\n",
    "            try:\n",
    "                P = float(P)\n",
    "            except ValueError:\n",
    "                raise ValueError(\"P must be either 'graph_density', 'node_density', 'node_edge_ratio', 'edge_node_ratio' or a float value.\")\n",
    "        \n",
    "\n",
    "        print(\"Using P:\", P, P)\n",
    "\n",
    "        chain_with_full_fragments = None\n",
    "       \n",
    "        rng = default_rng(seed = SEED)\n",
    "        rng_torch = torch.Generator(device=\"cpu\")\n",
    "        rng_torch.manual_seed(SEED)\n",
    "        # generate chain with original and full fragments\n",
    "        \n",
    "        #filter the noisy positions and features to have the same size as the data, removing the atoms not actually present in the molecule\n",
    "        #we use the same max sized noise for all molecules to guaranteethat the same moleclues are inzialized with the same noise for the linker atoms in common -> noise for the fragme atoms will be discarded\n",
    "        noisy_positions_present_atoms = noisy_positions.clone()\n",
    "        noisy_features_present_atoms = noisy_features.clone()\n",
    "\n",
    "        noisy_positions_present_atoms = noisy_positions_present_atoms[:, :data[\"positions\"].shape[1], :]\n",
    "        noisy_features_present_atoms = noisy_features_present_atoms[:, :data[\"one_hot\"].shape[1], :]\n",
    "\n",
    "        chain_batch, node_mask = model.sample_chain(data, keep_frames=keep_frames, noisy_positions=noisy_positions_present_atoms, noisy_features=noisy_features_present_atoms)\n",
    "        \n",
    "        #get the generated molecule and store it in a variable\n",
    "        chain_with_full_fragments = chain_batch[0, :, :, :] #need to get only the final frame, is 0 ok in the first dimension?\n",
    "        \n",
    "        #compute the Coulob matrix of the generated linker @mastro edited to try with full molecule to capute all the interactions\n",
    "        # coulomb_matrix = compute_coulomb_matrix(chain_with_full_fragments.squeeze(), mask = data[\"linker_mask\"][0].squeeze())\n",
    "        \n",
    "        #compute coulomb matrix for the whole molecule\n",
    "        coulomb_matrix = compute_coulomb_matrix(chain_with_full_fragments.squeeze(), diagonalize=DIAGONALIZE)\n",
    "\n",
    "        coulomb_matrix_non_diag = compute_coulomb_matrix(chain_with_full_fragments.squeeze(), diagonalize=False)\n",
    "\n",
    "        print(\"Coulomb matrix (diag): \", coulomb_matrix)\n",
    "        print(\"Coulomb matrix (non diag) \", coulomb_matrix_non_diag)\n",
    "\n",
    "        frobenius_norm_original_linker = torch.linalg.norm(coulomb_matrix, ord='fro')\n",
    "\n",
    "        fro_non_diag = torch.linalg.norm(coulomb_matrix_non_diag, ord='fro')\n",
    "\n",
    "        print(\"Frobenius norm of the original molecule (diag):\", frobenius_norm_original_linker)\n",
    "        print(\"Frobenius norm of the original molecule (non diag):\", fro_non_diag)\n",
    "        \n",
    "        \n",
    "        \n",
    "        original_linker_mask_batch = data[\"linker_mask\"][0].squeeze().repeat(PARALLEL_STEPS, 1) #check why it works\n",
    "    \n",
    "        \n",
    "        \n",
    "        num_fragment_atoms = torch.sum(data[\"fragment_mask\"] == 1)\n",
    "\n",
    "        phi_atoms = {}\n",
    "        \n",
    "        num_atoms = data[\"positions\"].shape[1]\n",
    "        num_linker_atoms = torch.sum(data[\"linker_mask\"] == 1)\n",
    "        \n",
    "        # distances_random_samples = []\n",
    "        # cosine_similarities_random_samples = []\n",
    "        hausdorff_distances_random_samples = []\n",
    "        frobenius_norm_random_samples = []\n",
    "        # end_time = time.time()\n",
    "        \n",
    "\n",
    "\n",
    "        for j in tqdm(range(num_fragment_atoms)): \n",
    "            \n",
    "            # marginal_contrib_distance = 0\n",
    "            # marginal_contrib_cosine_similarity = 0\n",
    "            # marginal_contrib_hausdorff = 0\n",
    "            marginal_contrib_frobenius_norm = 0\n",
    "\n",
    "            for step in range(int(M/PARALLEL_STEPS)):\n",
    "\n",
    "                # start_time = time.time()\n",
    "\n",
    "                fragment_indices = torch.where(data[\"fragment_mask\"] == 1)[1]\n",
    "                num_fragment_atoms = len(fragment_indices)\n",
    "                fragment_indices = fragment_indices.repeat(PARALLEL_STEPS).to(device)\n",
    "\n",
    "                data_j_plus = data.copy()\n",
    "                data_j_minus = data.copy()\n",
    "                data_random = data.copy()\n",
    "\n",
    "                N_z_mask = torch.tensor(np.array([rng.binomial(1, P, size = num_fragment_atoms) for _ in range(PARALLEL_STEPS)]), dtype=torch.int32)\n",
    "                # Ensure at least one element is 1, otherwise randomly select one since at least one fragment atom must be present\n",
    "                \n",
    "                \n",
    "                for i in range(len(N_z_mask)):\n",
    "\n",
    "                    #set the current explained atom to 0 in N_z_mask\n",
    "                    N_z_mask[i][j] = 0 #so it is always one when taken from the oriignal sample and 0 when taken from the random sample. Check if it is more efficient to directly set it or check if it is already 0\n",
    "\n",
    "                    if not N_z_mask[i].any():\n",
    "                        \n",
    "                        \n",
    "                        random_index = j #j is the current explained atom, it should always be set to 0\n",
    "                        while random_index == j:\n",
    "                            random_index = rng.integers(0, num_fragment_atoms)\n",
    "                        N_z_mask[i][random_index] = 1\n",
    "                        \n",
    "                       \n",
    "                    \n",
    "\n",
    "                N_z_mask=N_z_mask.flatten().to(device)\n",
    "                \n",
    "                \n",
    "                \n",
    "\n",
    "                N_mask = torch.ones(PARALLEL_STEPS * num_fragment_atoms, dtype=torch.int32, device=device)\n",
    "\n",
    "                \n",
    "\n",
    "                pi = torch.cat([torch.randperm(num_fragment_atoms, generator=rng_torch) for _ in range(PARALLEL_STEPS)], dim=0)\n",
    "\n",
    "                N_j_plus_index = torch.ones(PARALLEL_STEPS*num_fragment_atoms, dtype=torch.int, device=device)\n",
    "                N_j_minus_index = torch.ones(PARALLEL_STEPS*num_fragment_atoms, dtype=torch.int, device=device)\n",
    "\n",
    "                selected_node_index = np.where(pi == j)\n",
    "                selected_node_index = torch.tensor(np.array(selected_node_index), device=device).squeeze()\n",
    "                selected_node_index = selected_node_index.repeat_interleave(num_fragment_atoms) #@mastro TO BE CHECKED IF THIS IS CORRECT\n",
    "                \n",
    "                k_values = torch.arange(num_fragment_atoms*PARALLEL_STEPS, device=device)\n",
    "\n",
    "                add_to_pi = torch.arange(start=0, end=PARALLEL_STEPS*num_fragment_atoms, step=num_fragment_atoms).repeat_interleave(num_fragment_atoms) #check if it is correct ot consider num_fragment_atoms and not num_atoms\n",
    "\n",
    "                pi_add = pi + add_to_pi\n",
    "                pi_add = pi_add.to(device=device)\n",
    "                #this must be cafeully checked. this should be adapted for nodes\n",
    "                add_to_node_index = torch.arange(start=0, end=PARALLEL_STEPS*num_atoms, step=num_atoms) #@mastro change step from num_fragment_atoms to num_atoms\n",
    "                \n",
    "                add_to_node_index = add_to_node_index.repeat_interleave(num_fragment_atoms).to(device) #changed from num_atoms to num_fragment_atoms\n",
    "\n",
    "                \n",
    "                N_j_plus_index[pi_add] = torch.where(k_values <= selected_node_index, N_mask[pi_add], N_z_mask[pi_add])\n",
    "                N_j_minus_index[pi_add] = torch.where(k_values < selected_node_index, N_mask[pi_add], N_z_mask[pi_add]) \n",
    "\n",
    "                #fragements to keep in molecule j plus\n",
    "                fragment_indices = fragment_indices + add_to_node_index\n",
    "                \n",
    "                \n",
    "                N_j_plus = fragment_indices[(N_j_plus_index==1)] #fragment to keep in molecule j plus\n",
    "                #fragement indices to keep in molecule j minus\n",
    "               \n",
    "                N_j_minus = fragment_indices[(N_j_minus_index==1)] #it is ok. it contains fragmens indices to keep in molecule j minus (indices that index the atom nodes)\n",
    "\n",
    "                #fragement indices to keep in random molecule\n",
    "                N_random_sample = fragment_indices[(N_z_mask==1)] \n",
    "                \n",
    "               \n",
    "                atom_mask_j_plus = torch.zeros(num_atoms*PARALLEL_STEPS, dtype=torch.bool)\n",
    "                atom_mask_j_minus = torch.zeros(num_atoms*PARALLEL_STEPS, dtype=torch.bool)\n",
    "                atom_mask_random_molecule = torch.zeros(num_atoms*PARALLEL_STEPS, dtype=torch.bool)\n",
    "\n",
    "                atom_mask_j_plus[N_j_plus] = True\n",
    "                \n",
    "                atom_mask_j_minus[N_j_minus] = True\n",
    "\n",
    "                #set to true also linker atoms\n",
    "                parallelized_linker_mask = data[\"linker_mask\"][0].squeeze().to(torch.int).repeat(PARALLEL_STEPS)\n",
    "                atom_mask_j_plus[(parallelized_linker_mask == 1)] = True \n",
    "\n",
    "                #set to true also linker atoms\n",
    "                atom_mask_j_minus[(parallelized_linker_mask == 1)] = True \n",
    "\n",
    "                atom_mask_random_molecule[N_random_sample] = True\n",
    "                #set to true also linker atoms\n",
    "                atom_mask_random_molecule[(parallelized_linker_mask == 1)] = True\n",
    "                \n",
    "               \n",
    "                atom_mask_j_plus = atom_mask_j_plus.view(PARALLEL_STEPS, num_atoms)\n",
    "                atom_mask_j_minus = atom_mask_j_minus.view(PARALLEL_STEPS, num_atoms)\n",
    "                atom_mask_random_molecule = atom_mask_random_molecule.view(PARALLEL_STEPS, num_atoms)\n",
    "                \n",
    "                \n",
    "\n",
    "                data_j_plus_dict = {}\n",
    "                data_j_minus_dict = {}\n",
    "                data_random_dict = {}\n",
    "\n",
    "                noisy_features_j_plus_dict = {}\n",
    "                noisy_positions_j_plus_dict = {}\n",
    "                noisy_features_j_minus_dict = {}\n",
    "                noisy_positions_j_minus_dict = {}\n",
    "                noisy_features_random_dict = {}\n",
    "                noisy_positions_random_dict = {}\n",
    "                \n",
    "                # start_time = time.time()\n",
    "                for i in range(PARALLEL_STEPS):\n",
    "\n",
    "                    # Remove fragment atoms that are not present for j plus\n",
    "                    noisy_features_present_atoms_j_plus = noisy_features_present_atoms.clone()\n",
    "                    noisy_features_j_plus_dict[i] = noisy_features_present_atoms_j_plus[:, atom_mask_j_plus[i], :]\n",
    "                    \n",
    "                    noisy_positions_present_atoms_j_plus = noisy_positions_present_atoms.clone()\n",
    "                    noisy_positions_j_plus_dict[i] = noisy_positions_present_atoms_j_plus[:, atom_mask_j_plus[i], :]\n",
    "\n",
    "                    # Remove fragment atoms that are not present for j minus\n",
    "                    noisy_features_present_atoms_j_minus = noisy_features_present_atoms.clone()\n",
    "                    noisy_features_j_minus_dict[i] = noisy_features_present_atoms_j_minus[:, atom_mask_j_minus[i], :]\n",
    "\n",
    "                    noisy_positions_present_atoms_j_minus = noisy_positions_present_atoms.clone()\n",
    "                    noisy_positions_j_minus_dict[i] = noisy_positions_present_atoms_j_minus[:, atom_mask_j_minus[i], :]\n",
    "\n",
    "                    # Remove fragment atoms that are not present for random molecule\n",
    "                    noisy_features_present_atoms_random = noisy_features_present_atoms.clone()\n",
    "                    noisy_features_random_dict[i] = noisy_features_present_atoms_random[:, atom_mask_random_molecule[i], :]\n",
    "\n",
    "                    noisy_positions_present_atoms_random = noisy_positions_present_atoms.clone()\n",
    "                    noisy_positions_random_dict[i] = noisy_positions_present_atoms_random[:, atom_mask_random_molecule[i], :]\n",
    "\n",
    "\n",
    "                    data_j_plus_dict[i] = data.copy()\n",
    "                    data_j_minus_dict[i] = data.copy()\n",
    "                    data_random_dict[i] = data.copy()\n",
    "\n",
    "                    #data j plus\n",
    "                    data_j_plus_dict[i][\"positions\"] = data_j_plus_dict[i][\"positions\"][:, atom_mask_j_plus[i]]\n",
    "                    data_j_plus_dict[i][\"num_atoms\"] = data_j_plus_dict[i][\"positions\"].shape[1]\n",
    "                    # remove one_hot of atoms in random_indices\n",
    "                    data_j_plus_dict[i][\"one_hot\"] = data_j_plus_dict[i][\"one_hot\"][:, atom_mask_j_plus[i]]\n",
    "                    # remove atom_mask of atoms in random_indices\n",
    "                    data_j_plus_dict[i][\"atom_mask\"] = data_j_plus_dict[i][\"atom_mask\"][:, atom_mask_j_plus[i]]\n",
    "                    # remove fragment_mask of atoms in random_indices\n",
    "                    data_j_plus_dict[i][\"fragment_mask\"] = data_j_plus_dict[i][\"fragment_mask\"][:, atom_mask_j_plus[i]]\n",
    "                    # remove linker_mask of atoms in random_indices\n",
    "                    data_j_plus_dict[i][\"linker_mask\"] = data_j_plus_dict[i][\"linker_mask\"][:, atom_mask_j_plus[i]]\n",
    "                    data_j_plus_dict[i][\"charges\"] = data_j_plus_dict[i][\"charges\"][:, atom_mask_j_plus[i]]\n",
    "                    data_j_plus_dict[i][\"anchors\"] = data_j_plus_dict[i][\"anchors\"][:, atom_mask_j_plus[i]]\n",
    "                    edge_mask_to_keep = (atom_mask_j_plus[i].unsqueeze(1) * atom_mask_j_plus[i]).flatten()\n",
    "                    data_j_plus_dict[i][\"edge_mask\"] = data_j_plus_dict[i][\"edge_mask\"][edge_mask_to_keep]\n",
    "\n",
    "                    #data j minus\n",
    "                    data_j_minus_dict[i][\"positions\"] = data_j_minus_dict[i][\"positions\"][:, atom_mask_j_minus[i]]\n",
    "                    data_j_minus_dict[i][\"num_atoms\"] = data_j_minus_dict[i][\"positions\"].shape[1]\n",
    "                    # remove one_hot of atoms in random_indices\n",
    "                    data_j_minus_dict[i][\"one_hot\"] = data_j_minus_dict[i][\"one_hot\"][:, atom_mask_j_minus[i]]\n",
    "                    # remove atom_mask of atoms in random_indices\n",
    "                    data_j_minus_dict[i][\"atom_mask\"] = data_j_minus_dict[i][\"atom_mask\"][:, atom_mask_j_minus[i]]\n",
    "                    # remove fragment_mask of atoms in random_indices\n",
    "                    data_j_minus_dict[i][\"fragment_mask\"] = data_j_minus_dict[i][\"fragment_mask\"][:, atom_mask_j_minus[i]]\n",
    "                    # remove linker_mask of atoms in random_indices\n",
    "                    data_j_minus_dict[i][\"linker_mask\"] = data_j_minus_dict[i][\"linker_mask\"][:, atom_mask_j_minus[i]]\n",
    "                    data_j_minus_dict[i][\"charges\"] = data_j_minus_dict[i][\"charges\"][:, atom_mask_j_minus[i]]\n",
    "                    data_j_minus_dict[i][\"anchors\"] = data_j_minus_dict[i][\"anchors\"][:, atom_mask_j_minus[i]]\n",
    "                    # remove edge_mask of atoms in random_indices\n",
    "                    edge_mask_to_keep = (atom_mask_j_minus[i].unsqueeze(1) * atom_mask_j_minus[i]).flatten() \n",
    "                    data_j_minus_dict[i][\"edge_mask\"] = data_j_minus_dict[i][\"edge_mask\"][edge_mask_to_keep]\n",
    "\n",
    "                    #data random\n",
    "                    data_random_dict[i][\"positions\"] = data_random_dict[i][\"positions\"][:, atom_mask_random_molecule[i]]\n",
    "                    data_random_dict[i][\"num_atoms\"] = data_random_dict[i][\"positions\"].shape[1]\n",
    "                    # remove one_hot of atoms in random_indices\n",
    "                    data_random_dict[i][\"one_hot\"] = data_random_dict[i][\"one_hot\"][:, atom_mask_random_molecule[i]]\n",
    "                    # remove atom_mask of atoms in random_indices\n",
    "                    data_random_dict[i][\"atom_mask\"] = data_random_dict[i][\"atom_mask\"][:, atom_mask_random_molecule[i]]\n",
    "                    # remove fragment_mask of atoms in random_indices\n",
    "                    data_random_dict[i][\"fragment_mask\"] = data_random_dict[i][\"fragment_mask\"][:, atom_mask_random_molecule[i]]\n",
    "                    # remove linker_mask of atoms in random_indices\n",
    "                    data_random_dict[i][\"linker_mask\"] = data_random_dict[i][\"linker_mask\"][:, atom_mask_random_molecule[i]]\n",
    "                    data_random_dict[i][\"charges\"] = data_random_dict[i][\"charges\"][:, atom_mask_random_molecule[i]]\n",
    "                    data_random_dict[i][\"anchors\"] = data_random_dict[i][\"anchors\"][:, atom_mask_random_molecule[i]]\n",
    "                    # remove edge_mask of atoms in random_indices\n",
    "                    # remove edge_mask of atoms in random_indices\n",
    "                    edge_mask_to_keep = (atom_mask_random_molecule[i].unsqueeze(1) * atom_mask_random_molecule[i]).flatten() \n",
    "\n",
    "                    data_random_dict[i][\"edge_mask\"] = data_random_dict[i][\"edge_mask\"][edge_mask_to_keep]\n",
    "                \n",
    "                \n",
    "\n",
    "                PADDING = True\n",
    "\n",
    "                # start_time = time.time()\n",
    "                if PADDING:\n",
    "\n",
    "                    max_atoms_j_plus = max(data_j_plus_dict[i][\"num_atoms\"] for i in range(PARALLEL_STEPS))\n",
    "\n",
    "                    max_edges_j_plus = max(data_j_plus_dict[i][\"edge_mask\"].shape[0] for i in range(PARALLEL_STEPS))\n",
    "                    \n",
    "                    \n",
    "                    max_atoms_j_minus = max(data_j_minus_dict[i][\"num_atoms\"] for i in range(PARALLEL_STEPS))\n",
    "\n",
    "                    max_edges_j_minus = max(data_j_minus_dict[i][\"edge_mask\"].shape[0] for i in range(PARALLEL_STEPS))\n",
    "\n",
    "                    max_atoms_random = max(data_random_dict[i][\"num_atoms\"] for i in range(PARALLEL_STEPS))\n",
    "\n",
    "                    max_edges_random = max(data_random_dict[i][\"edge_mask\"].shape[0] for i in range(PARALLEL_STEPS))\n",
    "                    \n",
    "                    for i in range(PARALLEL_STEPS):\n",
    "                        #for j plus positions\n",
    "                        num_atoms_to_stack = max_atoms_j_plus - data_j_plus_dict[i][\"positions\"].shape[1]\n",
    "                        padding = torch.zeros(data_j_plus_dict[i][\"positions\"].shape[0], num_atoms_to_stack, data_j_plus_dict[i][\"positions\"].shape[2]).to(device)\n",
    "                        stacked_positions = torch.cat((data_j_plus_dict[i][\"positions\"], padding), dim=1)\n",
    "                        data_j_plus_dict[i][\"positions\"] = stacked_positions\n",
    "                        #for j plus one_hot\n",
    "                        padding = torch.zeros(data_j_plus_dict[i][\"one_hot\"].shape[0], num_atoms_to_stack, data_j_plus_dict[i][\"one_hot\"].shape[2]).to(device)\n",
    "                        stacked_one_hot = torch.cat((data_j_plus_dict[i][\"one_hot\"], padding), dim=1)\n",
    "                        data_j_plus_dict[i][\"one_hot\"] = stacked_one_hot\n",
    "                        padding = torch.zeros(data_j_plus_dict[i][\"fragment_mask\"].shape[0], num_atoms_to_stack, data_j_plus_dict[i][\"fragment_mask\"].shape[2]).to(device)\n",
    "                        stacked_fragment_mask = torch.cat((data_j_plus_dict[i][\"fragment_mask\"], padding), dim=1)\n",
    "                        data_j_plus_dict[i][\"fragment_mask\"] = stacked_fragment_mask\n",
    "                        padding = torch.zeros(data_j_plus_dict[i][\"charges\"].shape[0], num_atoms_to_stack, data_j_plus_dict[i][\"charges\"].shape[2]).to(device)\n",
    "                        stacked_charges = torch.cat((data_j_plus_dict[i][\"charges\"], padding), dim=1)\n",
    "                        data_j_plus_dict[i][\"charges\"] = stacked_charges\n",
    "                        padding = torch.zeros(data_j_plus_dict[i][\"anchors\"].shape[0], num_atoms_to_stack, data_j_plus_dict[i][\"anchors\"].shape[2]).to(device)\n",
    "                        stacked_anchors = torch.cat((data_j_plus_dict[i][\"anchors\"], padding), dim=1)\n",
    "                        data_j_plus_dict[i][\"anchors\"] = stacked_anchors\n",
    "                        padding = torch.zeros(data_j_plus_dict[i][\"linker_mask\"].shape[0], num_atoms_to_stack, data_j_plus_dict[i][\"linker_mask\"].shape[2]).to(device)\n",
    "                        stacked_linker_mask = torch.cat((data_j_plus_dict[i][\"linker_mask\"], padding), dim=1)\n",
    "                        data_j_plus_dict[i][\"linker_mask\"] = stacked_linker_mask\n",
    "                        padding = torch.zeros(data_j_plus_dict[i][\"atom_mask\"].shape[0], num_atoms_to_stack, data_j_plus_dict[i][\"atom_mask\"].shape[2]).to(device)\n",
    "                        stacked_atom_mask = torch.cat((data_j_plus_dict[i][\"atom_mask\"], padding), dim=1)\n",
    "                        data_j_plus_dict[i][\"atom_mask\"] = stacked_atom_mask\n",
    "                        num_edges_to_stack = max_edges_j_plus - data_j_plus_dict[i][\"edge_mask\"].shape[0]\n",
    "                        data_j_plus_dict[i][\"edge_mask\"] = data_j_plus_dict[i][\"edge_mask\"].unsqueeze(0)\n",
    "                        padding = torch.zeros(data_j_plus_dict[i][\"edge_mask\"].shape[0], num_edges_to_stack, data_j_plus_dict[i][\"edge_mask\"].shape[2]).to(device)\n",
    "                        stacked_edge_mask = torch.cat((data_j_plus_dict[i][\"edge_mask\"], padding), dim=1)\n",
    "                        data_j_plus_dict[i][\"edge_mask\"] = stacked_edge_mask\n",
    "                        \n",
    "                        #for noisy positions and features for j plus\n",
    "                        noisy_positions_j_plus_dict[i] = noisy_positions_j_plus_dict[i] #check this\n",
    "                        padding = torch.zeros(noisy_positions_j_plus_dict[i].shape[0], num_atoms_to_stack, noisy_positions_j_plus_dict[i].shape[2]).to(device)\n",
    "                        stacked_positions = torch.cat((noisy_positions_j_plus_dict[i], padding), dim=1)\n",
    "                        noisy_positions_j_plus_dict[i] = stacked_positions\n",
    "\n",
    "                        noisy_features_j_plus_dict[i] = noisy_features_j_plus_dict[i]\n",
    "                        padding = torch.zeros(noisy_features_j_plus_dict[i].shape[0], num_atoms_to_stack, noisy_features_j_plus_dict[i].shape[2]).to(device)\n",
    "                        stacked_features = torch.cat((noisy_features_j_plus_dict[i], padding), dim=1)\n",
    "                        noisy_features_j_plus_dict[i] = stacked_features\n",
    "\n",
    "                        #for j minus\n",
    "                        num_atoms_to_stack = max_atoms_j_minus - data_j_minus_dict[i][\"positions\"].shape[1]\n",
    "                        padding = torch.zeros(data_j_minus_dict[i][\"positions\"].shape[0], num_atoms_to_stack, data_j_minus_dict[i][\"positions\"].shape[2]).to(device) #why does this work?\n",
    "                        stacked_positions = torch.cat((data_j_minus_dict[i][\"positions\"], padding), dim=1)\n",
    "                        data_j_minus_dict[i][\"positions\"] = stacked_positions\n",
    "                        \n",
    "                        padding = torch.zeros(data_j_minus_dict[i][\"one_hot\"].shape[0], num_atoms_to_stack, data_j_minus_dict[i][\"one_hot\"].shape[2]).to(device)\n",
    "                        stacked_one_hot = torch.cat((data_j_minus_dict[i][\"one_hot\"], padding), dim=1)\n",
    "                        data_j_minus_dict[i][\"one_hot\"] = stacked_one_hot\n",
    "                        \n",
    "                        padding = torch.zeros(data_j_minus_dict[i][\"fragment_mask\"].shape[0], num_atoms_to_stack, data_j_minus_dict[i][\"fragment_mask\"].shape[2]).to(device)\n",
    "                        stacked_fragment_mask = torch.cat((data_j_minus_dict[i][\"fragment_mask\"], padding), dim=1)\n",
    "                        data_j_minus_dict[i][\"fragment_mask\"] = stacked_fragment_mask\n",
    "\n",
    "                        \n",
    "                        padding = torch.zeros(data_j_minus_dict[i][\"charges\"].shape[0], num_atoms_to_stack, data_j_minus_dict[i][\"charges\"].shape[2]).to(device)\n",
    "                        stacked_charges = torch.cat((data_j_minus_dict[i][\"charges\"], padding), dim=1)\n",
    "                        data_j_minus_dict[i][\"charges\"] = stacked_charges\n",
    "                        \n",
    "                        padding = torch.zeros(data_j_minus_dict[i][\"anchors\"].shape[0], num_atoms_to_stack, data_j_minus_dict[i][\"anchors\"].shape[2]).to(device)\n",
    "                        stacked_anchors = torch.cat((data_j_minus_dict[i][\"anchors\"], padding), dim=1)\n",
    "                        data_j_minus_dict[i][\"anchors\"] = stacked_anchors\n",
    "                       \n",
    "                        padding = torch.zeros(data_j_minus_dict[i][\"linker_mask\"].shape[0], num_atoms_to_stack, data_j_minus_dict[i][\"linker_mask\"].shape[2]).to(device)\n",
    "                        stacked_linker_mask = torch.cat((data_j_minus_dict[i][\"linker_mask\"], padding), dim=1)\n",
    "                        data_j_minus_dict[i][\"linker_mask\"] = stacked_linker_mask\n",
    "                        \n",
    "                        padding = torch.zeros(data_j_minus_dict[i][\"atom_mask\"].shape[0], num_atoms_to_stack, data_j_minus_dict[i][\"atom_mask\"].shape[2]).to(device)\n",
    "                        stacked_atom_mask = torch.cat((data_j_minus_dict[i][\"atom_mask\"], padding), dim=1)\n",
    "                        data_j_minus_dict[i][\"atom_mask\"] = stacked_atom_mask\n",
    "                        \n",
    "                        num_edges_to_stack = max_edges_j_minus - data_j_minus_dict[i][\"edge_mask\"].shape[0]\n",
    "                        data_j_minus_dict[i][\"edge_mask\"] = data_j_minus_dict[i][\"edge_mask\"].unsqueeze(0)\n",
    "                        padding = torch.zeros(data_j_minus_dict[i][\"edge_mask\"].shape[0], num_edges_to_stack, data_j_minus_dict[i][\"edge_mask\"].shape[2]).to(device)\n",
    "                        stacked_edge_mask = torch.cat((data_j_minus_dict[i][\"edge_mask\"], padding), dim=1)\n",
    "                        data_j_minus_dict[i][\"edge_mask\"] = stacked_edge_mask\n",
    "                    \n",
    "                        #for noisy positions and features for j plus\n",
    "                        noisy_positions_j_minus_dict[i] = noisy_positions_j_minus_dict[i] #check this\n",
    "                        padding = torch.zeros(noisy_positions_j_minus_dict[i].shape[0], num_atoms_to_stack, noisy_positions_j_minus_dict[i].shape[2]).to(device)\n",
    "                        stacked_positions = torch.cat((noisy_positions_j_minus_dict[i], padding), dim=1)\n",
    "                        noisy_positions_j_minus_dict[i] = stacked_positions\n",
    "\n",
    "                        noisy_features_j_minus_dict[i] = noisy_features_j_minus_dict[i]\n",
    "                        padding = torch.zeros(noisy_features_j_minus_dict[i].shape[0], num_atoms_to_stack, noisy_features_j_minus_dict[i].shape[2]).to(device)\n",
    "                        stacked_features = torch.cat((noisy_features_j_minus_dict[i], padding), dim=1)\n",
    "                        noisy_features_j_minus_dict[i] = stacked_features\n",
    "\n",
    "                        #for random\n",
    "                        num_atoms_to_stack = max_atoms_random - data_random_dict[i][\"positions\"].shape[1]\n",
    "                        padding = torch.zeros(data_random_dict[i][\"positions\"].shape[0], num_atoms_to_stack, data_random_dict[i][\"positions\"].shape[2]).to(device)\n",
    "                        stacked_positions = torch.cat((data_random_dict[i][\"positions\"], padding), dim=1)\n",
    "                        data_random_dict[i][\"positions\"] = stacked_positions\n",
    "                        \n",
    "                        padding = torch.zeros(data_random_dict[i][\"one_hot\"].shape[0], num_atoms_to_stack, data_random_dict[i][\"one_hot\"].shape[2]).to(device)\n",
    "                        stacked_one_hot = torch.cat((data_random_dict[i][\"one_hot\"], padding), dim=1)\n",
    "                        data_random_dict[i][\"one_hot\"] = stacked_one_hot\n",
    "                        \n",
    "                        padding = torch.zeros(data_random_dict[i][\"fragment_mask\"].shape[0], num_atoms_to_stack, data_random_dict[i][\"fragment_mask\"].shape[2]).to(device)\n",
    "                        stacked_fragment_mask = torch.cat((data_random_dict[i][\"fragment_mask\"], padding), dim=1)\n",
    "                        data_random_dict[i][\"fragment_mask\"] = stacked_fragment_mask\n",
    "                        \n",
    "                        padding = torch.zeros(data_random_dict[i][\"linker_mask\"].shape[0], num_atoms_to_stack, data_random_dict[i][\"linker_mask\"].shape[2]).to(device)\n",
    "                        stacked_linker_mask = torch.cat((data_random_dict[i][\"linker_mask\"], padding), dim=1)\n",
    "                        data_random_dict[i][\"linker_mask\"] = stacked_linker_mask\n",
    "\n",
    "                       \n",
    "                        padding = torch.zeros(data_random_dict[i][\"charges\"].shape[0], num_atoms_to_stack, data_random_dict[i][\"charges\"].shape[2]).to(device)\n",
    "                        stacked_charges = torch.cat((data_random_dict[i][\"charges\"], padding), dim=1)\n",
    "                        data_random_dict[i][\"charges\"] = stacked_charges\n",
    "\n",
    "                    \n",
    "                        padding = torch.zeros(data_random_dict[i][\"anchors\"].shape[0], num_atoms_to_stack, data_random_dict[i][\"anchors\"].shape[2]).to(device)\n",
    "                        stacked_anchors = torch.cat((data_random_dict[i][\"anchors\"], padding), dim=1)\n",
    "                        data_random_dict[i][\"anchors\"] = stacked_anchors\n",
    "                       \n",
    "                        padding = torch.zeros(data_random_dict[i][\"atom_mask\"].shape[0], num_atoms_to_stack, data_random_dict[i][\"atom_mask\"].shape[2]).to(device)\n",
    "                        stacked_atom_mask = torch.cat((data_random_dict[i][\"atom_mask\"], padding), dim=1)\n",
    "                        data_random_dict[i][\"atom_mask\"] = stacked_atom_mask\n",
    "                        \n",
    "                        num_edges_to_stack = max_edges_random - data_random_dict[i][\"edge_mask\"].shape[0]\n",
    "                        data_random_dict[i][\"edge_mask\"] = data_random_dict[i][\"edge_mask\"].unsqueeze(0)\n",
    "                        padding = torch.zeros(data_random_dict[i][\"edge_mask\"].shape[0], num_edges_to_stack, data_random_dict[i][\"edge_mask\"].shape[2]).to(device)\n",
    "                        stacked_edge_mask = torch.cat((data_random_dict[i][\"edge_mask\"], padding), dim=1)\n",
    "                        data_random_dict[i][\"edge_mask\"] = stacked_edge_mask\n",
    "\n",
    "                        #for noisy positions and features for j plus\n",
    "                        noisy_positions_random_dict[i] = noisy_positions_random_dict[i] #check this\n",
    "                        padding = torch.zeros(noisy_positions_random_dict[i].shape[0], num_atoms_to_stack, noisy_positions_random_dict[i].shape[2]).to(device)\n",
    "                        stacked_positions = torch.cat((noisy_positions_random_dict[i], padding), dim=1)\n",
    "                        noisy_positions_random_dict[i] = stacked_positions\n",
    "\n",
    "                        noisy_features_random_dict[i] = noisy_features_random_dict[i]\n",
    "                        padding = torch.zeros(noisy_features_random_dict[i].shape[0], num_atoms_to_stack, noisy_features_random_dict[i].shape[2]).to(device)\n",
    "                        stacked_features = torch.cat((noisy_features_random_dict[i], padding), dim=1)\n",
    "                        noisy_features_random_dict[i] = stacked_features\n",
    "                        \n",
    "                        \n",
    "\n",
    "                \n",
    "                #create batch for j plus\n",
    "                data_j_plus_batch = {}\n",
    "                data_j_plus_batch[\"positions\"] = torch.stack([data_j_plus_dict[i][\"positions\"] for i in range(PARALLEL_STEPS)], dim=0).squeeze()\n",
    "                data_j_plus_batch[\"one_hot\"] = torch.stack([data_j_plus_dict[i][\"one_hot\"] for i in range(PARALLEL_STEPS)], dim=0).squeeze(1)\n",
    "                data_j_plus_batch[\"atom_mask\"] = torch.stack([data_j_plus_dict[i][\"atom_mask\"] for i in range(PARALLEL_STEPS)], dim=0).squeeze(1)\n",
    "                data_j_plus_batch[\"fragment_mask\"] = torch.stack([data_j_plus_dict[i][\"fragment_mask\"] for i in range(PARALLEL_STEPS)], dim=0).squeeze(1)\n",
    "                data_j_plus_batch[\"linker_mask\"] = torch.stack([data_j_plus_dict[i][\"linker_mask\"] for i in range(PARALLEL_STEPS)], dim=0).squeeze(1)\n",
    "                data_j_plus_batch[\"charges\"] = torch.stack([data_j_plus_dict[i][\"charges\"] for i in range(PARALLEL_STEPS)], dim=0).squeeze(1)\n",
    "                data_j_plus_batch[\"anchors\"] = torch.stack([data_j_plus_dict[i][\"anchors\"] for i in range(PARALLEL_STEPS)], dim=0).squeeze(1)\n",
    "                \n",
    "                \n",
    "                data_j_plus_batch[\"uuid\"] = [i for i in range(PARALLEL_STEPS)]\n",
    "                data_j_plus_batch[\"num_atoms\"] = [data_j_plus_dict[i][\"num_atoms\"] for i in range(PARALLEL_STEPS)]\n",
    "                data_j_plus_batch[\"name\"] = [data[\"name\"] for _ in range(PARALLEL_STEPS)]\n",
    "                data_j_plus_batch[\"edge_mask\"] = torch.cat([data_j_plus_dict[i][\"edge_mask\"] for i in range(PARALLEL_STEPS)], dim=0).squeeze().view(-1).unsqueeze(1)\n",
    "\n",
    "\n",
    "                #create batch for j minus\n",
    "                data_j_minus_batch = {}\n",
    "                data_j_minus_batch[\"positions\"] = torch.stack([data_j_minus_dict[i][\"positions\"] for i in range(PARALLEL_STEPS)], dim=0).squeeze()\n",
    "                data_j_minus_batch[\"one_hot\"] = torch.stack([data_j_minus_dict[i][\"one_hot\"] for i in range(PARALLEL_STEPS)], dim=0).squeeze(1)\n",
    "                data_j_minus_batch[\"atom_mask\"] = torch.stack([data_j_minus_dict[i][\"atom_mask\"] for i in range(PARALLEL_STEPS)], dim=0).squeeze(1)\n",
    "                data_j_minus_batch[\"fragment_mask\"] = torch.stack([data_j_minus_dict[i][\"fragment_mask\"] for i in range(PARALLEL_STEPS)], dim=0).squeeze(1)\n",
    "                data_j_minus_batch[\"linker_mask\"] = torch.stack([data_j_minus_dict[i][\"linker_mask\"] for i in range(PARALLEL_STEPS)], dim=0).squeeze(1)\n",
    "                data_j_minus_batch[\"charges\"] = torch.stack([data_j_minus_dict[i][\"charges\"] for i in range(PARALLEL_STEPS)], dim=0).squeeze(1)\n",
    "                data_j_minus_batch[\"anchors\"] = torch.stack([data_j_minus_dict[i][\"anchors\"] for i in range(PARALLEL_STEPS)], dim=0).squeeze(1)\n",
    "                \n",
    "                data_j_minus_batch[\"uuid\"] = [i for i in range(PARALLEL_STEPS)]\n",
    "                data_j_minus_batch[\"num_atoms\"] = [data_j_minus_dict[i][\"num_atoms\"] for i in range(PARALLEL_STEPS)]\n",
    "                data_j_minus_batch[\"name\"] = [data[\"name\"] for _ in range(PARALLEL_STEPS)]\n",
    "                data_j_minus_batch[\"edge_mask\"] = torch.cat([data_j_minus_dict[i][\"edge_mask\"] for i in range(PARALLEL_STEPS)], dim=0).squeeze().view(-1).unsqueeze(1)\n",
    "\n",
    "                #create batch for random\n",
    "                data_random_batch = {}\n",
    "                data_random_batch[\"positions\"] = torch.stack([data_random_dict[i][\"positions\"] for i in range(PARALLEL_STEPS)], dim=0).squeeze()\n",
    "                data_random_batch[\"one_hot\"] = torch.stack([data_random_dict[i][\"one_hot\"] for i in range(PARALLEL_STEPS)], dim=0).squeeze(1)\n",
    "                data_random_batch[\"atom_mask\"] = torch.stack([data_random_dict[i][\"atom_mask\"] for i in range(PARALLEL_STEPS)], dim=0).squeeze(1)\n",
    "                data_random_batch[\"fragment_mask\"] = torch.stack([data_random_dict[i][\"fragment_mask\"] for i in range(PARALLEL_STEPS)], dim=0).squeeze(1)\n",
    "                data_random_batch[\"linker_mask\"] = torch.stack([data_random_dict[i][\"linker_mask\"] for i in range(PARALLEL_STEPS)], dim=0).squeeze(1)\n",
    "                data_random_batch[\"charges\"] = torch.stack([data_random_dict[i][\"charges\"] for i in range(PARALLEL_STEPS)], dim=0).squeeze(1)\n",
    "                data_random_batch[\"anchors\"] = torch.stack([data_random_dict[i][\"anchors\"] for i in range(PARALLEL_STEPS)], dim=0).squeeze(1)\n",
    "                \n",
    "                data_random_batch[\"uuid\"] = [i for i in range(PARALLEL_STEPS)]\n",
    "                data_random_batch[\"num_atoms\"] = [data_random_dict[i][\"num_atoms\"] for i in range(PARALLEL_STEPS)]\n",
    "                data_random_batch[\"name\"] = [data[\"name\"] for _ in range(PARALLEL_STEPS)]\n",
    "                data_random_batch[\"edge_mask\"] = torch.cat([data_random_dict[i][\"edge_mask\"] for i in range(PARALLEL_STEPS)], dim=0).squeeze().view(-1).unsqueeze(1)\n",
    "\n",
    "                \n",
    "                #create batches for noisy positions and features\n",
    "                noisy_positions_batch_j_plus = torch.stack([noisy_positions_j_plus_dict[i] for i in range(PARALLEL_STEPS)], dim=0).squeeze()\n",
    "                noisy_features_batch_j_plus = torch.stack([noisy_features_j_plus_dict[i] for i in range(PARALLEL_STEPS)], dim=0).squeeze()\n",
    "\n",
    "                noisy_positions_batch_j_minus = torch.stack([noisy_positions_j_minus_dict[i] for i in range(PARALLEL_STEPS)], dim=0).squeeze()\n",
    "                noisy_features_batch_j_minus = torch.stack([noisy_features_j_minus_dict[i] for i in range(PARALLEL_STEPS)], dim=0).squeeze()\n",
    "\n",
    "                noisy_positions_batch_random = torch.stack([noisy_positions_random_dict[i] for i in range(PARALLEL_STEPS)], dim=0).squeeze()\n",
    "                noisy_features_batch_random = torch.stack([noisy_features_random_dict[i] for i in range(PARALLEL_STEPS)], dim=0).squeeze()\n",
    "                \n",
    "                \n",
    "\n",
    "                \n",
    "                chain_j_plus_batch, node_mask_j_plus_batch = model.sample_chain(data_j_plus_batch, keep_frames=keep_frames, noisy_positions=noisy_positions_batch_j_plus, noisy_features=noisy_features_batch_j_plus)\n",
    "\n",
    "                chain_j_plus = chain_j_plus_batch[0, :, :, :] #it should take the first frame and all batch elements -> check it is really the first frame (I need the one at t0, the final generated molecule)\n",
    "                \n",
    "\n",
    "                chain_j_minus_batch, node_mask_j_minus_batch = model.sample_chain(data_j_minus_batch, keep_frames=keep_frames, noisy_positions=noisy_positions_batch_j_minus, noisy_features=noisy_features_batch_j_minus)\n",
    "\n",
    "                chain_j_minus = chain_j_minus_batch[0, :, :, :]\n",
    "\n",
    "                chain_random_batch, node_mask_random_batch = model.sample_chain(data_random_batch, keep_frames=keep_frames, noisy_positions=noisy_positions_batch_random, noisy_features=noisy_features_batch_random)\n",
    "\n",
    "                chain_random = chain_random_batch[0, :, :, :]\n",
    "                \n",
    "                \n",
    "\n",
    "                chain_with_full_fragments_batch = chain_with_full_fragments.repeat(PARALLEL_STEPS, 1, 1)\n",
    "\n",
    "                # Check if all vectors in data_j_plus_batch[\"linker_mask\"] are the same\n",
    "                \n",
    "                \n",
    "                ####NEW CODE#######\n",
    "                #@mastro creating new molecule containing the original fragmsnes and the linker generated using molecule j_plus, molecule j_minus and random molecule\n",
    "                chain_j_plus_batch_original_fragments = chain_with_full_fragments_batch.clone()\n",
    "                \n",
    "                # Ensure the masks have the correct shape\n",
    "                mask1 = data[\"linker_mask\"][0].squeeze() == 1\n",
    "                mask2 = data_j_plus_batch[\"linker_mask\"].squeeze() == 1\n",
    "\n",
    "                # Check if the masks need to be expanded\n",
    "                if mask1.dim() == 1 and chain_j_plus_batch_original_fragments.dim() == 3:\n",
    "                    mask1 = mask1.unsqueeze(0).expand(chain_j_plus_batch_original_fragments.size(0), -1)\n",
    "\n",
    "                \n",
    "                # Apply the masks\n",
    "                # Ensure the shapes match for the assignment\n",
    "                if chain_j_plus_batch_original_fragments[mask1, :].shape == chain_j_plus[mask2, :].shape:\n",
    "                    chain_j_plus_batch_original_fragments[mask1, :] = chain_j_plus[mask2, :]\n",
    "                else:\n",
    "                    print(\"Shape mismatch:\", chain_j_plus_batch_original_fragments[mask1, :].shape, chain_j_plus[mask2, :].shape)\n",
    "                \n",
    "                \n",
    "                # print(\"chain_j_plus_batch_original_fragments shape\", chain_j_plus_batch_original_fragments.shape)\n",
    "\n",
    "                \n",
    "                # chain_j_minus_batch_original_fragments = chain_j_minus.clone()\n",
    "                # chain_j_minus_batch_original_fragments[:, data[\"fragment_mask\"][0].squeeze() == 1, :] = chain_with_full_fragments_batch[:, data[\"fragment_mask\"][0].squeeze() == 1, :]\n",
    "\n",
    "                chain_j_minus_batch_original_fragments = chain_with_full_fragments_batch.clone()\n",
    "                \n",
    "                # Ensure the masks have the correct shape\n",
    "                mask1 = data[\"linker_mask\"][0].squeeze() == 1\n",
    "                mask2 = data_j_minus_batch[\"linker_mask\"].squeeze() == 1\n",
    "\n",
    "                # Check if the masks need to be expanded\n",
    "                if mask1.dim() == 1 and chain_j_minus_batch_original_fragments.dim() == 3:\n",
    "                    mask1 = mask1.unsqueeze(0).expand(chain_j_minus_batch_original_fragments.size(0), -1)\n",
    "\n",
    "                \n",
    "                # Apply the masks\n",
    "                # Ensure the shapes match for the assignment\n",
    "                if chain_j_minus_batch_original_fragments[mask1, :].shape == chain_j_minus[mask2, :].shape:\n",
    "                    chain_j_minus_batch_original_fragments[mask1, :] = chain_j_minus[mask2, :]\n",
    "                else:\n",
    "                    print(\"Shape mismatch:\", chain_j_minus_batch_original_fragments[mask1, :].shape, chain_j_minus[mask2, :].shape)\n",
    "                \n",
    "                \n",
    "                # print(\"chain_j_minus_batch_original_fragments shape\", chain_j_minus_batch_original_fragments.shape)\n",
    "\n",
    "                # chain_random_batch_original_fragments = chain_random.clone()\n",
    "                # chain_random_batch_original_fragments[:, data[\"fragment_mask\"][0].squeeze() == 1, :] = chain_with_full_fragments_batch[:, data[\"fragment_mask\"][0].squeeze() == 1, :]\n",
    "\n",
    "                # chain_random_batch_original_fragments = chain_with_full_fragments_batch.clone()\n",
    "                # chain_random_batch_original_fragments[data[\"linker_mask\"][0].squeeze() == 1, :] = chain_random[:, data_random_batch[\"linker_mask\"].squeeze() == 1, :]\n",
    "\n",
    "                chain_random_batch_original_fragments = chain_with_full_fragments_batch.clone()\n",
    "                \n",
    "                # Ensure the masks have the correct shape\n",
    "                mask1 = data[\"linker_mask\"][0].squeeze() == 1\n",
    "                mask2 = data_random_batch[\"linker_mask\"].squeeze() == 1\n",
    "\n",
    "                # Check if the masks need to be expanded\n",
    "                if mask1.dim() == 1 and chain_random_batch_original_fragments.dim() == 3:\n",
    "                    mask1 = mask1.unsqueeze(0).expand(chain_random_batch_original_fragments.size(0), -1)\n",
    "\n",
    "                \n",
    "                # Apply the masks\n",
    "                # Ensure the shapes match for the assignment\n",
    "                if chain_random_batch_original_fragments[mask1, :].shape == chain_random[mask2, :].shape:\n",
    "                    chain_random_batch_original_fragments[mask1, :] = chain_random[mask2, :]\n",
    "                else:\n",
    "                    print(\"Shape mismatch:\", chain_random_batch_original_fragments[mask1, :].shape, chain_random[mask2, :].shape)\n",
    "                \n",
    "                \n",
    "                # print(\"chain_random_batch_original_fragments shape\", chain_random_batch_original_fragments.shape)\n",
    "                ###################################\n",
    "\n",
    "\n",
    "                # V_j_plus_coulomb_matrices_batch = compute_coulomb_matrices_batch(chain_j_plus.cpu())\n",
    "\n",
    "                V_j_plus_coulomb_matrices_batch = compute_coulomb_matrices_batch(chain_j_plus_batch_original_fragments.cpu(), diagonalize=DIAGONALIZE)\n",
    "                \n",
    "                V_j_plus_frobenius_norm_batch = compute_frobenius_norm_batch(V_j_plus_coulomb_matrices_batch)\n",
    "                \n",
    "                V_j_plus_frobenius_norm = sum(V_j_plus_frobenius_norm_batch)\n",
    "                \n",
    "                print(\"V_j_plus_frobenius_norm\", V_j_plus_frobenius_norm)\n",
    "\n",
    "                #non diagonalized version for testing\n",
    "                V_j_plus_coulomb_matrices_batch_non_diag = compute_coulomb_matrices_batch(chain_j_plus_batch_original_fragments.cpu(), diagonalize=False)\n",
    "\n",
    "                V_j_plus_frobenius_norm_batch_non_diag = compute_frobenius_norm_batch(V_j_plus_coulomb_matrices_batch_non_diag)\n",
    "\n",
    "                V_j_plus_frobenius_norm_non_diag = sum(V_j_plus_frobenius_norm_batch_non_diag)\n",
    "\n",
    "                print(\"V_j_plus_frobenius_norm_non_diag\", V_j_plus_frobenius_norm_non_diag)\n",
    "                # print(\"V_j_plus_frobenius_norm\", V_j_plus_frobenius_norm)\n",
    "                #@mastro computing for the whole molecule\n",
    "                # V_j_minus_coulomb_matrices_batch = compute_coulomb_matrices_batch(chain_j_minus.cpu())\n",
    "                \n",
    "                V_j_minus_coulomb_matrices_batch = compute_coulomb_matrices_batch(chain_j_minus_batch_original_fragments.cpu(), diagonalize=DIAGONALIZE)\n",
    "\n",
    "                V_j_minus_frobenius_norm_batch = compute_frobenius_norm_batch(V_j_minus_coulomb_matrices_batch)\n",
    "\n",
    "                V_j_minus_frobenius_norm = sum(V_j_minus_frobenius_norm_batch)\n",
    "\n",
    "                print(\"V_j_minus_frobenius_norm\", V_j_minus_frobenius_norm)\n",
    "\n",
    "                #non diagonalized version for testing\n",
    "                V_j_minus_coulomb_matrices_batch_non_diag = compute_coulomb_matrices_batch(chain_j_minus_batch_original_fragments.cpu(), diagonalize=False)\n",
    "\n",
    "                V_j_minus_frobenius_norm_batch_non_diag = compute_frobenius_norm_batch(V_j_minus_coulomb_matrices_batch_non_diag)\n",
    "\n",
    "                V_j_minus_frobenius_norm_non_diag = sum(V_j_minus_frobenius_norm_batch_non_diag)\n",
    "\n",
    "                print(\"V_j_minus_frobenius_norm_non_diag\", V_j_minus_frobenius_norm_non_diag)\n",
    "                \n",
    "                # print(\"V_j_minus_frobenius_norm\", V_j_minus_frobenius_norm)\n",
    "\n",
    "                # V_random_coulomb_matrices_batch = compute_coulomb_matrices_batch(chain_random.cpu())\n",
    "\n",
    "                V_random_coulomb_matrices_batch = compute_coulomb_matrices_batch(chain_random_batch_original_fragments.cpu(), diagonalize=DIAGONALIZE)\n",
    "\n",
    "                V_random_frobenius_norm_batch = compute_frobenius_norm_batch(V_random_coulomb_matrices_batch)\n",
    "\n",
    "                V_random_frobenius_norm = sum(V_random_frobenius_norm_batch)\n",
    "\n",
    "                print(\"V_random_frobenius_norm\", V_random_frobenius_norm)\n",
    "\n",
    "                #non diagonalized version for testing\n",
    "                V_random_coulomb_matrices_batch_non_diag = compute_coulomb_matrices_batch(chain_random_batch_original_fragments.cpu(), diagonalize=False)\n",
    "\n",
    "                V_random_frobenius_norm_batch_non_diag = compute_frobenius_norm_batch(V_random_coulomb_matrices_batch_non_diag)\n",
    "\n",
    "                V_random_frobenius_norm_non_diag = sum(V_random_frobenius_norm_batch_non_diag)\n",
    "\n",
    "                print(\"V_random_frobenius_norm_non_diag\", V_random_frobenius_norm_non_diag)\n",
    "                \n",
    "                # print(\"V_random_frobenius_norm\", V_random_frobenius_norm)\n",
    "                \n",
    "\n",
    "                for r_frob in V_random_frobenius_norm_batch:\n",
    "                    frobenius_norm_random_samples.append(r_frob)\n",
    "                \n",
    "                \n",
    "\n",
    "                marginal_contrib_frobenius_norm += (V_j_plus_frobenius_norm - V_j_minus_frobenius_norm)\n",
    "\n",
    "                \n",
    "\n",
    "            phi_atoms[fragment_indices[j].item()] = [0]    \n",
    "            phi_atoms[fragment_indices[j].item()][0] = marginal_contrib_frobenius_norm/M #j is the index of the fragment atom in the fragment indices tensor\n",
    "\n",
    "        print(data[\"name\"])\n",
    "\n",
    "        phi_atoms_fronebius_norm = {}\n",
    "        for atom_index, phi_values in phi_atoms.items():\n",
    "            phi_atoms_fronebius_norm[atom_index] = phi_values[0]\n",
    "            \n",
    "            # phi_atoms_hausdorff[atom_index] = phi_values[2]\n",
    "\n",
    "        \n",
    "        # Save phi_atoms to a text file\n",
    "        with open(f'{folder_save_path}/phi_atoms_{data_index}.txt', 'w') as write_file:\n",
    "            write_file.write(\"sample name: \" + str(data[\"name\"]) + \"\\n\")\n",
    "            write_file.write(\"atom_index,shapley_value\\n\")\n",
    "            for atom_index, phi_values in phi_atoms.items():\n",
    "                write_file.write(f\"{atom_index},{phi_values[0]}\\n\")\n",
    "\n",
    "            write_file.write(\"\\n\")\n",
    "            # save sum of phi values for disance and cosine similarity\n",
    "            sum_shapley_values = sum([p_values[0] for p_values in phi_atoms.values()])\n",
    "            write_file.write(\"Sum of Shapley values:\")\n",
    "            write_file.write(str(sum_shapley_values.item()) + \"\\n\")\n",
    "            \n",
    "            # write_file.write(\"Sum of phi values for hausdorff\\n\")\n",
    "            # write_file.write(str(sum([p_values[2] for p_values in phi_atoms.values()])) + \"\\n\")     \n",
    "            \n",
    "            # write_file.write(\"Average hausdorff distance random samples:\\n\")\n",
    "            # write_file.write(str(sum(hausdorff_distances_random_samples)/len(hausdorff_distances_random_samples)) + \"\\n\")      \n",
    "            \n",
    "            # write_file.write(\"Hausdorff distances random samples\\n\")\n",
    "            # write_file.write(str(hausdorff_distances_random_samples) + \"\\n\")\n",
    "\n",
    "            write_file.write(\"Frobenius norm of original molecule:\")\n",
    "            write_file.write(str(frobenius_norm_original_linker.item()) + \"\\n\")\n",
    "\n",
    "            average_frobenius_norm_random_samples = sum(frobenius_norm_random_samples)/len(frobenius_norm_random_samples)\n",
    "\n",
    "            write_file.write(\"Average Frobenius norm of random samples:\")\n",
    "            write_file.write(str(average_frobenius_norm_random_samples.item()) + \"\\n\")\n",
    "\n",
    "            \n",
    "            ideal_sum_shapley_values = frobenius_norm_original_linker - average_frobenius_norm_random_samples\n",
    "\n",
    "            approx_error = sum_shapley_values - ideal_sum_shapley_values\n",
    "            abs_approx_error = abs(approx_error)\n",
    "            write_file.write(\"Approximation error:\")\n",
    "            write_file.write(str(approx_error.item()) + \"\\n\")\n",
    "            write_file.write(\"Absolute approximation error:\")\n",
    "            write_file.write(str(abs_approx_error.item()) + \"\\n\")\n",
    "            \n",
    "            write_file.write(\"Frobenius norm of random samples:\\n\")\n",
    "            write_file.write(str(frobenius_norm_random_samples) + \"\\n\")\n",
    "\n",
    "        if SAVE_VISUALIZATION:\n",
    "            phi_values_for_viz = phi_atoms_fronebius_norm\n",
    "\n",
    "            # Saving chains and final states\n",
    "            for i in range(len(data['positions'])):\n",
    "                chain = chain_batch[:, i, :, :]\n",
    "                assert chain.shape[0] == keep_frames\n",
    "                assert chain.shape[1] == data['positions'].shape[1]\n",
    "                assert chain.shape[2] == data['positions'].shape[2] + data['one_hot'].shape[2] + model.include_charges\n",
    "\n",
    "                # Saving chains\n",
    "                name = str(i + start)\n",
    "                chain_output = os.path.join(chains_output_dir, name)\n",
    "                os.makedirs(chain_output, exist_ok=True)\n",
    "                \n",
    "                #save initial random distrubution with noise\n",
    "                positions_combined = torch.zeros_like(data['positions'])\n",
    "                one_hot_combined = torch.zeros_like(data['one_hot'])\n",
    "\n",
    "                # Iterate over each atom and decide whether to use original or noisy data\n",
    "                for atom_idx in range(data['positions'].shape[1]):\n",
    "                    if data['fragment_mask'][0, atom_idx] == 1:\n",
    "                        # Use original positions and features for fragment atoms\n",
    "                        positions_combined[:, atom_idx, :] = data['positions'][:, atom_idx, :]\n",
    "                        one_hot_combined[:, atom_idx, :] = data['one_hot'][:, atom_idx, :]\n",
    "                        # atom_mask_combined[:, atom_idx] = data['atom_mask'][:, atom_idx]\n",
    "                    else:\n",
    "                        # Use noisy positions and features for linker atoms\n",
    "                        positions_combined[:, atom_idx, :] = noisy_positions_present_atoms[:, atom_idx, :]\n",
    "                        one_hot_combined[:, atom_idx, :] = noisy_features_present_atoms[:, atom_idx, :]\n",
    "\n",
    "                #save initial distribution TODO: fix positions, they are not centered\n",
    "                save_xyz_file(\n",
    "                    chain_output,\n",
    "                    one_hot_combined,\n",
    "                    positions_combined,\n",
    "                    node_mask[i].unsqueeze(0),\n",
    "                    names=[f'{name}_' + str(keep_frames)],\n",
    "                    is_geom=model.is_geom\n",
    "                )\n",
    "\n",
    "                # one_hot = chain[:, :, 3:-1]\n",
    "                one_hot = chain[:, :, 3:] #@mastro, added last atom type (not sure whyt it was not included...) However, TODO check again -> is it the atomic_number? But checking dimensions it did not look like it. Anyway, this should have no effect since the charge/atomic_number is always 0 in our case\n",
    "                positions = chain[:, :, :3]\n",
    "                chain_node_mask = torch.cat([node_mask[i].unsqueeze(0) for _ in range(keep_frames)], dim=0)\n",
    "                names = [f'{name}_{j}' for j in range(keep_frames)]\n",
    "\n",
    "                save_xyz_file(chain_output, one_hot, positions, chain_node_mask, names=names, is_geom=model.is_geom)\n",
    "                invert_colormap = False\n",
    "                if average_frobenius_norm_random_samples > frobenius_norm_original_linker:\n",
    "                    invert_colormap = True\n",
    "\n",
    "                visualize_chain_xai(\n",
    "                    chain_output,\n",
    "                    spheres_3d=False,\n",
    "                    alpha=0.7,\n",
    "                    bg='white',\n",
    "                    is_geom=model.is_geom,\n",
    "                    fragment_mask=data['fragment_mask'][i].squeeze(),\n",
    "                    phi_values=phi_values_for_viz,\n",
    "                    invert_colormap=invert_colormap\n",
    "                )\n",
    "\n",
    "                # Saving final prediction and ground truth separately\n",
    "                true_one_hot = data['one_hot'][i].unsqueeze(0)\n",
    "                true_positions = data['positions'][i].unsqueeze(0)\n",
    "                true_node_mask = data['atom_mask'][i].unsqueeze(0)\n",
    "                save_xyz_file(\n",
    "                    final_states_output_dir,\n",
    "                    true_one_hot,\n",
    "                    true_positions,\n",
    "                    true_node_mask,\n",
    "                    names=[f'{name}_true'],\n",
    "                    is_geom=model.is_geom,\n",
    "                )\n",
    "\n",
    "                pred_one_hot = chain[0, :, 3:-1].unsqueeze(0)\n",
    "                pred_positions = chain[0, :, :3].unsqueeze(0)\n",
    "                pred_node_mask = chain_node_mask[0].unsqueeze(0)\n",
    "                save_xyz_file(\n",
    "                    final_states_output_dir,\n",
    "                    pred_one_hot,\n",
    "                    pred_positions,\n",
    "                    pred_node_mask,\n",
    "                    names=[f'{name}_pred'],\n",
    "                    is_geom=model.is_geom\n",
    "                )\n",
    "\n",
    "            start += len(data['positions'])\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diff_explainer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
