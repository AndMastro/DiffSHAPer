{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2024, Andrea Mastropietro. All rights reserved.\n",
    "# This code is licensed under the MIT License.\n",
    "# See the LICENSE file in the project root for more information.\n",
    "\n",
    "import os\n",
    "os.environ[\"http_proxy\"] = \"http://web-proxy.informatik.uni-bonn.de:3128\"\n",
    "os.environ[\"https_proxy\"] = \"http://web-proxy.informatik.uni-bonn.de:3128\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "from numpy.random import default_rng\n",
    "import yaml\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "from pysmiles import read_smiles\n",
    "\n",
    "from src.utils import compute_hausdorff_distance_batch, visualize_mapping_graph, visualize_mapping_structure\n",
    "from src.difflinker.datasets import get_dataloader\n",
    "from src.difflinker.lightning import DDPM\n",
    "from src.utils import save_xyz_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed:  42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mastropietro/anaconda3/envs/diff_explainer/lib/python3.10/site-packages/lightning_fabric/utilities/cloud_io.py:57: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.6.3 to v2.3.3. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint models/zinc_difflinker.ckpt`\n",
      "/home/mastropietro/Repositories/DiffSHAPer/src/difflinker/datasets.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.data = torch.load(dataset_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "with open('config.yml', 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "checkpoint = config['CHECKPOINT']\n",
    "SAVE_FOLDER = config['SAVE_FOLDER']\n",
    "DATA_FOLDER = config['DATA_FOLDER']\n",
    "DATASET_NAME = config['DATASET_NAME']\n",
    "keep_frames = int(config['KEEP_FRAMES'])\n",
    "P = config['P']\n",
    "device = config['DEVICE'] if torch.cuda.is_available() else 'cpu'\n",
    "SEED = int(config['SEED'])\n",
    "ROTATE = config['ROTATE']\n",
    "TRANSLATE = config['TRANSLATE']\n",
    "REFLECT = config['REFLECT']\n",
    "TRANSFORMATION_SEED = int(config['TRANSFORMATION_SEED'])\n",
    "SAVE_VISUALIZATION = config['SAVE_VISUALIZATION']\n",
    "M = int(config['M'])\n",
    "NUM_SAMPLES = int(config['NUM_SAMPLES'])\n",
    "PARALLEL_STEPS = int(config['PARALLEL_STEPS'])\n",
    "LOAD_INITIAL_DISTRIBUTION = config['LOAD_INITIAL_DISTRIBUTION']\n",
    "\n",
    "print(\"Random seed: \", SEED)\n",
    "\n",
    "transformations = []\n",
    "if ROTATE:\n",
    "    transformations.append(\"rotate\")\n",
    "if TRANSLATE:\n",
    "    transformations.append(\"translate\")\n",
    "if REFLECT:\n",
    "    transformations.append(\"reflect\")\n",
    "\n",
    "transformations_str = \"_\".join(transformations) if transformations else \"\"\n",
    "\n",
    "if transformations:\n",
    "    mapping_output_dir = os.path.join(SAVE_FOLDER, DATASET_NAME, f'explanations_seed_{SEED}_{transformations_str}_transformation_seed_{TRANSFORMATION_SEED}', \"mapping\")\n",
    "\n",
    "    shapley_values_save_path = os.path.join(SAVE_FOLDER, DATASET_NAME, f'explanations_seed_{SEED}_{transformations_str}_transformation_seed_{TRANSFORMATION_SEED}', \"shapley_values\")\n",
    "else:\n",
    "    mapping_output_dir = os.path.join(SAVE_FOLDER, DATASET_NAME, f'explanations_seed_{SEED}', \"mapping\")\n",
    "    shapley_values_save_path = os.path.join(SAVE_FOLDER, DATASET_NAME, f'explanations_seed_{SEED}', \"shapley_values\")\n",
    "os.makedirs(mapping_output_dir, exist_ok=True)\n",
    "os.makedirs(shapley_values_save_path, exist_ok=True)\n",
    "# final_states_output_dir = os.path.join(SAVE_FOLDER, DATASET_NAME, \"mapping\", f'final_states_hausdorff_distance_{P}_seed_{SEED}_{transformations_str}_transformation_seed_{TRANSFORMATION_SEED}')\n",
    "# os.makedirs(final_states_output_dir, exist_ok=True)\n",
    "\n",
    "if transformations:\n",
    "    print(\"Applied trasformations: \", transformations_str)\n",
    "    print(\"Seed used for random transformations: \", TRANSFORMATION_SEED)\n",
    "\n",
    "\n",
    "model = DDPM.load_from_checkpoint(checkpoint, map_location=device)\n",
    "\n",
    "model.val_data_prefix = DATASET_NAME\n",
    "\n",
    "print(f\"Running device: {device}\")\n",
    "\n",
    "model.data_path = DATA_FOLDER\n",
    "\n",
    "model = model.eval().to(device)\n",
    "model.setup(stage='val')\n",
    "dataloader = get_dataloader(\n",
    "    model.val_dataset,\n",
    "    batch_size=1, #set to 1 for explanation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set random seeds\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explainabiliy phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Multiple sampling steps at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading initial distribution of noisy features and positions.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8eb32bfa47fb461ab1f86978883ed139",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87d135dc72a143639100306413debc55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cc1ccc(CCNC(=O)NCCc2csc(N3CCCC3)n2)c(C)c1']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47e340b1e7664affaec4592501c98c6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cc1ccc(CCNC(=O)NCCc2csc(N3CCCC3)n2)c(C)c1']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10:54:00] WARNING: could not find number of expected rings. Switching to an approximate ring finding algorithm.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "522b2bae31444ccdba6f7c994de75b9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cc1ccc(CCNC(=O)NCCc2csc(N3CCCC3)n2)c(C)c1']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68cfca457f0649a293c02024fa4c7512",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cc1ccc(CCNC(=O)NCCc2csc(N3CCCC3)n2)c(C)c1']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11:45:54] WARNING: could not find number of expected rings. Switching to an approximate ring finding algorithm.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "452a281971e24dd7a9cfdc311b82822f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cc1ccc(CCNC(=O)NCCc2csc(N3CCCC3)n2)c(C)c1']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[12:10:43] WARNING: could not find number of expected rings. Switching to an approximate ring finding algorithm.\n",
      "[12:10:43] WARNING: could not find number of expected rings. Switching to an approximate ring finding algorithm.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e403beedcfdc4bedb1e231ed4edd1799",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cc1ccc(CCNC(=O)NCCc2csc(N3CCCC3)n2)c(C)c1']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[12:34:35] WARNING: could not find number of expected rings. Switching to an approximate ring finding algorithm.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa7004f0b24c4932a6a855be72e06fea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cc1ccc(CCNC(=O)NCCc2csc(N3CCCC3)n2)c(C)c1']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7946639d36764fcfafbedd9a0f14cf3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cc1ccc(CCNC(=O)NCCc2csc(N3CCCC3)n2)c(C)c1']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49014511e31f42aba79c4818c44f0a29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O=C1CN(S(=O)(=O)c2cccc(NC(=O)c3cc(F)cc(F)c3)c2)CCN1']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11ad2771a7a042d3b76150c6d767cf04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O=C1CN(S(=O)(=O)c2cccc(NC(=O)c3cc(F)cc(F)c3)c2)CCN1']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Atom \"[C@H]\" contains stereochemical information that will be discarded.\n",
      "Atom \"[C@@H]\" contains stereochemical information that will be discarded.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97bce0566fb042db9777782dd4a5f9d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CCc1ccccc1NC(=O)C(=O)NC[C@H](C)C[C@@H](C)O']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E/Z stereochemical information, which is specified by \"/\", will be discarded\n",
      "E/Z stereochemical information, which is specified by \"/\", will be discarded\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "111bce4c70ca43cab0a5f63d318498d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cc1ccc(C(C)C)c(OCC(=O)N/N=C/c2ccco2)c1']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[15:00:55] WARNING: could not find number of expected rings. Switching to an approximate ring finding algorithm.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c911c26121c4428b2dc0a69e94ac453",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cc1ccc2c(c1)C(=O)N(CCC(=O)NCC(C1CC1)C1CC1)C2=O']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efea6473dc3643a6ac94773b3f63ef6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cc1ccc2c(c1)C(=O)N(CCC(=O)NCC(C1CC1)C1CC1)C2=O']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5cf55fc91af448b82d5cac53c0345cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cc1ccc2c(c1)C(=O)N(CCC(=O)NCC(C1CC1)C1CC1)C2=O']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b0758b8eca5480db31fe51a35741a0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cc1ccc2c(c1)C(=O)N(CCC(=O)NCC(C1CC1)C1CC1)C2=O']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "214249f6e81342b8ba6832f4f89a7de8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cc1ccc2c(c1)C(=O)N(CCC(=O)NCC(C1CC1)C1CC1)C2=O']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f600ce73a03c4f5980ca9fdd5c22ff0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cc1ccc2c(c1)C(=O)N(CCC(=O)NCC(C1CC1)C1CC1)C2=O']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Atom \"[C@H]\" contains stereochemical information that will be discarded.\n",
      "Atom \"[C@H]\" contains stereochemical information that will be discarded.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfe8ca08d4b844898a825b8de9314131",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CC[C@H](NC(=O)[C@H]1CN(C)CCO1)c1ccc(Br)cc1']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Atom \"[C@H]\" contains stereochemical information that will be discarded.\n",
      "Atom \"[C@H]\" contains stereochemical information that will be discarded.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26ed7b138f3b4a929c8f9f346d1db143",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CC[C@H](NC(=O)[C@H]1CN(C)CCO1)c1ccc(Br)cc1']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c22b7ce0f0447a0b8a53c7a1dd4efa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CN(C)c1ccc(NC(=O)Cn2c(=O)[nH]c(=O)c3ccccc32)cn1']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17e8bb2cc34840f08c65ec01b2160a30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CN(C)c1ccc(NC(=O)Cn2c(=O)[nH]c(=O)c3ccccc32)cn1']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Atom \"[C@H]\" contains stereochemical information that will be discarded.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4d1612077cd46478f5bcfcb2345fa60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cc1ccc2nccc(C(=O)N3CCC[C@H](C(=O)c4cccc5ccccc45)C3)c2c1']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:33:06] WARNING: could not find number of expected rings. Switching to an approximate ring finding algorithm.\n",
      "[19:33:06] WARNING: could not find number of expected rings. Switching to an approximate ring finding algorithm.\n",
      "Atom \"[C@H]\" contains stereochemical information that will be discarded.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e5687ce938d41b5bbe3af4bc6c36c6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cc1ccc2nccc(C(=O)N3CCC[C@H](C(=O)c4cccc5ccccc45)C3)c2c1']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20:08:53] WARNING: could not find number of expected rings. Switching to an approximate ring finding algorithm.\n",
      "[20:08:53] WARNING: could not find number of expected rings. Switching to an approximate ring finding algorithm.\n",
      "[20:08:54] WARNING: could not find number of expected rings. Switching to an approximate ring finding algorithm.\n",
      "Atom \"[C@H]\" contains stereochemical information that will be discarded.\n",
      "Atom \"[C@@H]\" contains stereochemical information that will be discarded.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c719dbad50e84533852ea42eb32f163a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C[C@H]1C[C@@H](C)CN(S(=O)(=O)c2ccc(NC(=O)c3cccs3)cc2)C1']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Atom \"[C@H]\" contains stereochemical information that will be discarded.\n",
      "Atom \"[C@@H]\" contains stereochemical information that will be discarded.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5e3711158dd435d9b7dfa2c2927d735",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C[C@H]1C[C@@H](C)CN(S(=O)(=O)c2ccc(NC(=O)c3cccs3)cc2)C1']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b85e245f3a84fcb87f0c388a1d4480f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cc1nc(C(F)(F)F)ccc1C(=O)N1CCC(C(=O)Nc2ccc(S(N)(=O)=O)cc2)CC1']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b3e5a5149784605979fc76131808738",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cc1nc(C(F)(F)F)ccc1C(=O)N1CCC(C(=O)Nc2ccc(S(N)(=O)=O)cc2)CC1']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[22:32:33] WARNING: could not find number of expected rings. Switching to an approximate ring finding algorithm.\n",
      "[22:32:34] WARNING: could not find number of expected rings. Switching to an approximate ring finding algorithm.\n",
      "[22:32:34] WARNING: could not find number of expected rings. Switching to an approximate ring finding algorithm.\n",
      "Atom \"[C@H]\" contains stereochemical information that will be discarded.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c904c1a2360648988b8c4f5026bb1743",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C[C@H](NC(=O)NCc1cccnc1)c1cccc(OCc2ccccn2)c1']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[22:59:04] WARNING: could not find number of expected rings. Switching to an approximate ring finding algorithm.\n",
      "[22:59:04] WARNING: could not find number of expected rings. Switching to an approximate ring finding algorithm.\n",
      "Atom \"[C@H]\" contains stereochemical information that will be discarded.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "221fe5556cf1442fab8185798c608332",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C[C@H](NC(=O)NCc1cccnc1)c1cccc(OCc2ccccn2)c1']\n"
     ]
    }
   ],
   "source": [
    "sampled = 0\n",
    "start = 0\n",
    "\n",
    "chain_with_full_fragments = None\n",
    "\n",
    "data_list = []\n",
    "for data in dataloader:\n",
    "\n",
    "    if sampled < NUM_SAMPLES:\n",
    "        data_list.append(data)\n",
    "        sampled += 1\n",
    "\n",
    "#determine max numebr of atoms of the molecules in the dataset. This is used to determine the size of the random noise, which we want to be equal for all molecules -> atoms not present in the molecule will be discarded using masks \n",
    "max_num_atoms = max(data[\"positions\"].shape[1] for data in data_list)\n",
    "\n",
    "\n",
    "pos_size = (data_list[0][\"positions\"].shape[0], max_num_atoms, data_list[0][\"positions\"].shape[2])\n",
    "feature_size = (data_list[0][\"one_hot\"].shape[0], max_num_atoms, data_list[0][\"one_hot\"].shape[2])\n",
    "\n",
    "INTIAL_DISTIBUTION_PATH = \"datasets/initial_distributions/seed_\" + str(SEED)\n",
    "noisy_features = None\n",
    "noisy_positions = None\n",
    "\n",
    "#check if the initial distribution of the noisy features and positions already exists, if not create it\n",
    "if LOAD_INITIAL_DISTRIBUTION:\n",
    "\n",
    "    # load initial distrubution of noisy features and positions\n",
    "    print(\"Loading initial distribution of noisy features and positions.\")\n",
    "    noisy_features = torch.load(INTIAL_DISTIBUTION_PATH + \"/noisy_features_seed_\" + str(SEED) + \".pt\", map_location=device, weights_only=True)\n",
    "    noisy_positions = torch.load(INTIAL_DISTIBUTION_PATH + \"/noisy_positions_seed_\" + str(SEED) + \".pt\", map_location=device, weights_only=True)\n",
    "\n",
    "else:\n",
    "    os.makedirs(INTIAL_DISTIBUTION_PATH, exist_ok=True)\n",
    "    print(\"Creating initial distribution of noisy features and positions.\")\n",
    "    noisy_positions = torch.randn(pos_size, device=device)\n",
    "    noisy_features = torch.randn(feature_size, device=device)\n",
    "\n",
    "\n",
    "    #save the noisy positions and features on file .txt\n",
    "    print(\"Saving noisy features and positions to .txt and .pt files.\")\n",
    "    noisy_positions_file = os.path.join(INTIAL_DISTIBUTION_PATH, \"noisy_positions_seed_\" + str(SEED) + \".txt\")\n",
    "    noisy_features_file = os.path.join(INTIAL_DISTIBUTION_PATH, \"noisy_features_seed_\" + str(SEED) + \".txt\")\n",
    "\n",
    "    with open(noisy_positions_file, \"w\") as f:\n",
    "        f.write(str(noisy_positions))\n",
    "\n",
    "    with open(noisy_features_file, \"w\") as f:\n",
    "        f.write(str(noisy_features))\n",
    "\n",
    "    #save the noisy positions and features on file .pt\n",
    "    torch.save(noisy_positions, os.path.join(INTIAL_DISTIBUTION_PATH, \"noisy_positions_seed_\" + str(SEED) + \".pt\"))\n",
    "    torch.save(noisy_features, os.path.join(INTIAL_DISTIBUTION_PATH, \"noisy_features_seed_\" + str(SEED) + \".pt\"))\n",
    "\n",
    "for data_index, data in enumerate(tqdm(data_list)):\n",
    "\n",
    "        # start_time = time.time()\n",
    "        \n",
    "        smile = data[\"name\"][0]\n",
    "        \n",
    "        mol = read_smiles(smile)\n",
    "        num_nodes = mol.number_of_nodes()\n",
    "        \n",
    "        num_edges = mol.number_of_edges()\n",
    "        num_edges_directed = num_edges*2\n",
    "        \n",
    "        \n",
    "        graph_density = num_edges_directed/(num_nodes*(num_nodes-1))\n",
    "        max_number_of_nodes = num_edges + 1\n",
    "\n",
    "        node_density = num_nodes/max_number_of_nodes\n",
    "\n",
    "        node_edge_ratio = num_nodes/num_edges\n",
    "        \n",
    "        edge_node_ratio = num_edges/num_nodes\n",
    "        \n",
    "        if P == \"graph_density\":\n",
    "            P = graph_density #probability of atom to exist in random graph (not sure if correct approach, this was correct for edges)\n",
    "        elif P == \"node_density\":\n",
    "            P = node_density\n",
    "        elif P == \"node_edge_ratio\" or P == \"edge_node_ratio\":\n",
    "            if node_edge_ratio < edge_node_ratio:\n",
    "                P = node_edge_ratio\n",
    "                print(\"Using node-edge ratio\", node_edge_ratio)\n",
    "            else:\n",
    "                P = edge_node_ratio\n",
    "                print(\"Using edge-node ratio\", edge_node_ratio)            \n",
    "        else:\n",
    "            try:\n",
    "                P = float(P)\n",
    "            except ValueError:\n",
    "                raise ValueError(\"P must be either 'graph_density', 'node_density', 'node_edge_ratio', 'edge_node_ratio' or a float value.\")\n",
    "        \n",
    "\n",
    "        chain_with_full_fragments = None\n",
    "       \n",
    "        rng = default_rng(seed = SEED)\n",
    "        rng_torch = torch.Generator(device=\"cpu\")\n",
    "        rng_torch.manual_seed(SEED)\n",
    "\n",
    "        #apply E(3) trasformations to the molecule. Linker atoms will be tranformed, too, but their transformations will be discarded in liue of the noisy positions\n",
    "        # print(\"Positions before transformations:\", data[\"positions\"])\n",
    "        transform_rng = None\n",
    "        if transformations:\n",
    "            transform_rng = default_rng(seed = TRANSFORMATION_SEED)\n",
    "            \n",
    "        if ROTATE:\n",
    "            #rotate molecule\n",
    "            # Generate a random 3x3 matrix\n",
    "            random_matrix = torch.tensor(transform_rng.uniform(-1, 1, (3, 3)), device=device, dtype=torch.float32)\n",
    "            \n",
    "            # Perform QR decomposition to obtain an orthogonal matrix\n",
    "            q, r = torch.linalg.qr(random_matrix)\n",
    "            \n",
    "            # Ensure the determinant is 1 (if not, adjust it)\n",
    "            if torch.det(q) < 0:\n",
    "                q[:, 0] = -q[:, 0]\n",
    "            \n",
    "            #ensure q has float values\n",
    "            # q = q.float()\n",
    "            # Apply the rotation matrix to the molecule positions\n",
    "            data[\"positions\"] = torch.matmul(data[\"positions\"], q)\n",
    "        if TRANSLATE:\n",
    "            #translate molecule\n",
    "            translation_vector = torch.tensor(transform_rng.uniform(-1, 1, (1, 3)), device=device, dtype=torch.float32)\n",
    "            data[\"positions\"] = data[\"positions\"] + translation_vector\n",
    "        if REFLECT:\n",
    "            #reflect molecule acrpss the xy plane\n",
    "            reflection_matrix = torch.tensor([[1.0, 0.0, 0.0],\n",
    "                                      [0.0, 1.0, 0.0],\n",
    "                                      [0.0, 0.0, -1.0]], device=device)\n",
    "            data[\"positions\"] = torch.matmul(data[\"positions\"], reflection_matrix)\n",
    "        \n",
    "        #filter the noisy positions and features to have the same size as the data, removing the atoms not actually present in the molecule\n",
    "        #we use the same max sized noise for all molecules to guaranteethat the same moleclues are inzialized with the same noise for the linker atoms in common -> noise for the fragme atoms will be discarded\n",
    "        noisy_positions_present_atoms = noisy_positions.clone()\n",
    "        noisy_features_present_atoms = noisy_features.clone()\n",
    "\n",
    "        noisy_positions_present_atoms = noisy_positions_present_atoms[:, :data[\"positions\"].shape[1], :]\n",
    "        noisy_features_present_atoms = noisy_features_present_atoms[:, :data[\"one_hot\"].shape[1], :]\n",
    "\n",
    "\n",
    "        chain_batch, node_mask = model.sample_chain(data, keep_frames=keep_frames, noisy_positions=noisy_positions_present_atoms, noisy_features=noisy_features_present_atoms)\n",
    "        \n",
    "        #get the generated molecule and store it in a variable\n",
    "        chain_with_full_fragments = chain_batch[0, :, :, :] \n",
    "        \n",
    "        \n",
    "        original_linker_mask_batch = data[\"linker_mask\"][0].squeeze().repeat(PARALLEL_STEPS, 1) \n",
    "        \n",
    "        original_positions = data[\"positions\"][0]\n",
    "        chain_positions = chain_with_full_fragments[0, :, :3]\n",
    "       \n",
    "\n",
    "        position_differences = original_positions - chain_positions\n",
    "        position_differences = position_differences[data[\"fragment_mask\"].squeeze().bool()][0]\n",
    "        \n",
    "        chain_with_full_fragments[:, :, :3] = chain_with_full_fragments[:, :, :3] + position_differences\n",
    "        #adding offset to the rest of the frames\n",
    "        for i in range(1, keep_frames):\n",
    "            chain_batch[i, :, :, :3] = chain_batch[i, :, :, :3] + position_differences\n",
    "        \n",
    "        \n",
    "        \n",
    "        num_fragment_atoms = torch.sum(data[\"fragment_mask\"] == 1)\n",
    "\n",
    "        phi_atoms = {}\n",
    "        \n",
    "        num_atoms = data[\"positions\"].shape[1]\n",
    "        num_linker_atoms = torch.sum(data[\"linker_mask\"] == 1)\n",
    "        \n",
    "        distances_random_samples = []\n",
    "        hausdorff_distances_random_samples = []\n",
    "\n",
    "        #DiffSHAPer application\n",
    "        for j in tqdm(range(num_fragment_atoms)): \n",
    "            \n",
    "            marginal_contrib_hausdorff = 0\n",
    "\n",
    "            for step in range(int(M/PARALLEL_STEPS)):\n",
    "\n",
    "                fragment_indices = torch.where(data[\"fragment_mask\"] == 1)[1]\n",
    "                num_fragment_atoms = len(fragment_indices)\n",
    "                fragment_indices = fragment_indices.repeat(PARALLEL_STEPS).to(device)\n",
    "\n",
    "                N_z_mask = torch.tensor(np.array([rng.binomial(1, P, size = num_fragment_atoms) for _ in range(PARALLEL_STEPS)]), dtype=torch.int32)\n",
    "                # Ensure at least one element is 1, otherwise randomly select one since at least one fragment atom must be present\n",
    "                \n",
    "                for i in range(len(N_z_mask)):\n",
    "\n",
    "                    #set the current explained atom to 0 in N_z_mask\n",
    "                    N_z_mask[i][j] = 0 #so it is always one when taken from the oriignal sample and 0 when taken from the random sample. Check if it is more efficient to directly set it or check if it is already 0\n",
    "\n",
    "                    if not N_z_mask[i].any():\n",
    "                        \n",
    "                        random_index = j #j is the current explained atom, it should always be set to 0\n",
    "                        while random_index == j:\n",
    "                            random_index = rng.integers(0, num_fragment_atoms)\n",
    "                        N_z_mask[i][random_index] = 1\n",
    "                          \n",
    "                N_z_mask=N_z_mask.flatten().to(device)\n",
    "                \n",
    "                N_mask = torch.ones(PARALLEL_STEPS * num_fragment_atoms, dtype=torch.int32, device=device)\n",
    "\n",
    "                pi = torch.cat([torch.randperm(num_fragment_atoms, generator=rng_torch) for _ in range(PARALLEL_STEPS)], dim=0)\n",
    "\n",
    "                N_j_plus_index = torch.ones(PARALLEL_STEPS*num_fragment_atoms, dtype=torch.int, device=device)\n",
    "                N_j_minus_index = torch.ones(PARALLEL_STEPS*num_fragment_atoms, dtype=torch.int, device=device)\n",
    "\n",
    "                selected_node_index = np.where(pi == j)\n",
    "                selected_node_index = torch.tensor(np.array(selected_node_index), device=device).squeeze()\n",
    "                selected_node_index = selected_node_index.repeat_interleave(num_fragment_atoms) \n",
    "                k_values = torch.arange(num_fragment_atoms*PARALLEL_STEPS, device=device)\n",
    "\n",
    "                add_to_pi = torch.arange(start=0, end=PARALLEL_STEPS*num_fragment_atoms, step=num_fragment_atoms).repeat_interleave(num_fragment_atoms) \n",
    "\n",
    "                pi_add = pi + add_to_pi\n",
    "                pi_add = pi_add.to(device=device)\n",
    "                \n",
    "                add_to_node_index = torch.arange(start=0, end=PARALLEL_STEPS*num_atoms, step=num_atoms) \n",
    "                \n",
    "                add_to_node_index = add_to_node_index.repeat_interleave(num_fragment_atoms).to(device)\n",
    "\n",
    "                N_j_plus_index[pi_add] = torch.where(k_values <= selected_node_index, N_mask[pi_add], N_z_mask[pi_add])\n",
    "                N_j_minus_index[pi_add] = torch.where(k_values < selected_node_index, N_mask[pi_add], N_z_mask[pi_add]) \n",
    "\n",
    "                #fragements to keep in molecule j plus\n",
    "                fragment_indices = fragment_indices + add_to_node_index\n",
    "                \n",
    "                N_j_plus = fragment_indices[(N_j_plus_index==1)] #fragment to keep in molecule j plus\n",
    "\n",
    "                #fragement indices to keep in molecule j minus\n",
    "                N_j_minus = fragment_indices[(N_j_minus_index==1)] #it contains fragmens indices to keep in molecule j minus (indices that index the atom nodes)\n",
    "\n",
    "                #fragement indices to keep in random molecule\n",
    "                N_random_sample = fragment_indices[(N_z_mask==1)] \n",
    "                \n",
    "                atom_mask_j_plus = torch.zeros(num_atoms*PARALLEL_STEPS, dtype=torch.bool)\n",
    "                atom_mask_j_minus = torch.zeros(num_atoms*PARALLEL_STEPS, dtype=torch.bool)\n",
    "                atom_mask_random_molecule = torch.zeros(num_atoms*PARALLEL_STEPS, dtype=torch.bool)\n",
    "\n",
    "                atom_mask_j_plus[N_j_plus] = True\n",
    "                \n",
    "                atom_mask_j_minus[N_j_minus] = True\n",
    "\n",
    "                #set to true also linker atoms\n",
    "                parallelized_linker_mask = data[\"linker_mask\"][0].squeeze().to(torch.int).repeat(PARALLEL_STEPS)\n",
    "                atom_mask_j_plus[(parallelized_linker_mask == 1)] = True \n",
    "\n",
    "                atom_mask_j_minus[(parallelized_linker_mask == 1)] = True \n",
    "                atom_mask_random_molecule[N_random_sample] = True\n",
    "                atom_mask_random_molecule[(parallelized_linker_mask == 1)] = True\n",
    "                \n",
    "                atom_mask_j_plus = atom_mask_j_plus.view(PARALLEL_STEPS, num_atoms)\n",
    "                atom_mask_j_minus = atom_mask_j_minus.view(PARALLEL_STEPS, num_atoms)\n",
    "                atom_mask_random_molecule = atom_mask_random_molecule.view(PARALLEL_STEPS, num_atoms)\n",
    "                \n",
    "                data_j_plus_dict = {}\n",
    "                data_j_minus_dict = {}\n",
    "                data_random_dict = {}\n",
    "\n",
    "                noisy_features_j_plus_dict = {}\n",
    "                noisy_positions_j_plus_dict = {}\n",
    "                noisy_features_j_minus_dict = {}\n",
    "                noisy_positions_j_minus_dict = {}\n",
    "                noisy_features_random_dict = {}\n",
    "                noisy_positions_random_dict = {}\n",
    "                \n",
    "                for i in range(PARALLEL_STEPS):\n",
    "\n",
    "                    # Remove fragment atoms that are not present for j plus\n",
    "                    noisy_features_present_atoms_j_plus = noisy_features_present_atoms.clone()\n",
    "                    noisy_features_j_plus_dict[i] = noisy_features_present_atoms_j_plus[:, atom_mask_j_plus[i], :]\n",
    "                    \n",
    "                    noisy_positions_present_atoms_j_plus = noisy_positions_present_atoms.clone()\n",
    "                    noisy_positions_j_plus_dict[i] = noisy_positions_present_atoms_j_plus[:, atom_mask_j_plus[i], :]\n",
    "\n",
    "                    # Remove fragment atoms that are not present for j minus\n",
    "                    noisy_features_present_atoms_j_minus = noisy_features_present_atoms.clone()\n",
    "                    noisy_features_j_minus_dict[i] = noisy_features_present_atoms_j_minus[:, atom_mask_j_minus[i], :]\n",
    "\n",
    "                    noisy_positions_present_atoms_j_minus = noisy_positions_present_atoms.clone()\n",
    "                    noisy_positions_j_minus_dict[i] = noisy_positions_present_atoms_j_minus[:, atom_mask_j_minus[i], :]\n",
    "\n",
    "                    # Remove fragment atoms that are not present for random molecule\n",
    "                    noisy_features_present_atoms_random = noisy_features_present_atoms.clone()\n",
    "                    noisy_features_random_dict[i] = noisy_features_present_atoms_random[:, atom_mask_random_molecule[i], :]\n",
    "\n",
    "                    noisy_positions_present_atoms_random = noisy_positions_present_atoms.clone()\n",
    "                    noisy_positions_random_dict[i] = noisy_positions_present_atoms_random[:, atom_mask_random_molecule[i], :]\n",
    "\n",
    "                    #to be more conservative and avoid unwanted side effects, we deepcopy the data\n",
    "                    data_j_plus_dict[i] = copy.deepcopy(data)\n",
    "                    data_j_minus_dict[i] = copy.deepcopy(data)\n",
    "                    data_random_dict[i] = copy.deepcopy(data)\n",
    "\n",
    "                    #data j plus\n",
    "                    data_j_plus_dict[i][\"positions\"] = data_j_plus_dict[i][\"positions\"][:, atom_mask_j_plus[i]]\n",
    "                    data_j_plus_dict[i][\"num_atoms\"] = data_j_plus_dict[i][\"positions\"].shape[1]\n",
    "                    # remove one_hot of atoms in random_indices\n",
    "                    data_j_plus_dict[i][\"one_hot\"] = data_j_plus_dict[i][\"one_hot\"][:, atom_mask_j_plus[i]]\n",
    "                    # remove atom_mask of atoms in random_indices\n",
    "                    data_j_plus_dict[i][\"atom_mask\"] = data_j_plus_dict[i][\"atom_mask\"][:, atom_mask_j_plus[i]]\n",
    "                    # remove fragment_mask of atoms in random_indices\n",
    "                    data_j_plus_dict[i][\"fragment_mask\"] = data_j_plus_dict[i][\"fragment_mask\"][:, atom_mask_j_plus[i]]\n",
    "                    # remove linker_mask of atoms in random_indices\n",
    "                    data_j_plus_dict[i][\"linker_mask\"] = data_j_plus_dict[i][\"linker_mask\"][:, atom_mask_j_plus[i]]\n",
    "                    data_j_plus_dict[i][\"charges\"] = data_j_plus_dict[i][\"charges\"][:, atom_mask_j_plus[i]]\n",
    "                    data_j_plus_dict[i][\"anchors\"] = data_j_plus_dict[i][\"anchors\"][:, atom_mask_j_plus[i]]\n",
    "                    edge_mask_to_keep = (atom_mask_j_plus[i].unsqueeze(1) * atom_mask_j_plus[i]).flatten()\n",
    "                    data_j_plus_dict[i][\"edge_mask\"] = data_j_plus_dict[i][\"edge_mask\"][edge_mask_to_keep]\n",
    "\n",
    "                    #data j minus\n",
    "                    data_j_minus_dict[i][\"positions\"] = data_j_minus_dict[i][\"positions\"][:, atom_mask_j_minus[i]]\n",
    "                    data_j_minus_dict[i][\"num_atoms\"] = data_j_minus_dict[i][\"positions\"].shape[1]\n",
    "                    # remove one_hot of atoms in random_indices\n",
    "                    data_j_minus_dict[i][\"one_hot\"] = data_j_minus_dict[i][\"one_hot\"][:, atom_mask_j_minus[i]]\n",
    "                    # remove atom_mask of atoms in random_indices\n",
    "                    data_j_minus_dict[i][\"atom_mask\"] = data_j_minus_dict[i][\"atom_mask\"][:, atom_mask_j_minus[i]]\n",
    "                    # remove fragment_mask of atoms in random_indices\n",
    "                    data_j_minus_dict[i][\"fragment_mask\"] = data_j_minus_dict[i][\"fragment_mask\"][:, atom_mask_j_minus[i]]\n",
    "                    # remove linker_mask of atoms in random_indices\n",
    "                    data_j_minus_dict[i][\"linker_mask\"] = data_j_minus_dict[i][\"linker_mask\"][:, atom_mask_j_minus[i]]\n",
    "                    data_j_minus_dict[i][\"charges\"] = data_j_minus_dict[i][\"charges\"][:, atom_mask_j_minus[i]]\n",
    "                    data_j_minus_dict[i][\"anchors\"] = data_j_minus_dict[i][\"anchors\"][:, atom_mask_j_minus[i]]\n",
    "                    # remove edge_mask of atoms in random_indices\n",
    "                    edge_mask_to_keep = (atom_mask_j_minus[i].unsqueeze(1) * atom_mask_j_minus[i]).flatten() \n",
    "                    data_j_minus_dict[i][\"edge_mask\"] = data_j_minus_dict[i][\"edge_mask\"][edge_mask_to_keep]\n",
    "\n",
    "                    #data random\n",
    "                    data_random_dict[i][\"positions\"] = data_random_dict[i][\"positions\"][:, atom_mask_random_molecule[i]]\n",
    "                    data_random_dict[i][\"num_atoms\"] = data_random_dict[i][\"positions\"].shape[1]\n",
    "                    # remove one_hot of atoms in random_indices\n",
    "                    data_random_dict[i][\"one_hot\"] = data_random_dict[i][\"one_hot\"][:, atom_mask_random_molecule[i]]\n",
    "                    # remove atom_mask of atoms in random_indices\n",
    "                    data_random_dict[i][\"atom_mask\"] = data_random_dict[i][\"atom_mask\"][:, atom_mask_random_molecule[i]]\n",
    "                    # remove fragment_mask of atoms in random_indices\n",
    "                    data_random_dict[i][\"fragment_mask\"] = data_random_dict[i][\"fragment_mask\"][:, atom_mask_random_molecule[i]]\n",
    "                    # remove linker_mask of atoms in random_indices\n",
    "                    data_random_dict[i][\"linker_mask\"] = data_random_dict[i][\"linker_mask\"][:, atom_mask_random_molecule[i]]\n",
    "                    data_random_dict[i][\"charges\"] = data_random_dict[i][\"charges\"][:, atom_mask_random_molecule[i]]\n",
    "                    data_random_dict[i][\"anchors\"] = data_random_dict[i][\"anchors\"][:, atom_mask_random_molecule[i]]\n",
    "                    # remove edge_mask of atoms in random_indices\n",
    "                    edge_mask_to_keep = (atom_mask_random_molecule[i].unsqueeze(1) * atom_mask_random_molecule[i]).flatten() \n",
    "\n",
    "                    data_random_dict[i][\"edge_mask\"] = data_random_dict[i][\"edge_mask\"][edge_mask_to_keep]\n",
    "                \n",
    "\n",
    "\n",
    "                max_atoms_j_plus = max(data_j_plus_dict[i][\"num_atoms\"] for i in range(PARALLEL_STEPS))\n",
    "\n",
    "                max_edges_j_plus = max(data_j_plus_dict[i][\"edge_mask\"].shape[0] for i in range(PARALLEL_STEPS))\n",
    "                \n",
    "                \n",
    "                max_atoms_j_minus = max(data_j_minus_dict[i][\"num_atoms\"] for i in range(PARALLEL_STEPS))\n",
    "\n",
    "                max_edges_j_minus = max(data_j_minus_dict[i][\"edge_mask\"].shape[0] for i in range(PARALLEL_STEPS))\n",
    "\n",
    "                max_atoms_random = max(data_random_dict[i][\"num_atoms\"] for i in range(PARALLEL_STEPS))\n",
    "\n",
    "                max_edges_random = max(data_random_dict[i][\"edge_mask\"].shape[0] for i in range(PARALLEL_STEPS))\n",
    "                \n",
    "                for i in range(PARALLEL_STEPS):\n",
    "                    #for j plus positions\n",
    "                    num_atoms_to_stack = max_atoms_j_plus - data_j_plus_dict[i][\"positions\"].shape[1]\n",
    "                    padding = torch.zeros(data_j_plus_dict[i][\"positions\"].shape[0], num_atoms_to_stack, data_j_plus_dict[i][\"positions\"].shape[2]).to(device)\n",
    "                    stacked_positions = torch.cat((data_j_plus_dict[i][\"positions\"], padding), dim=1)\n",
    "                    data_j_plus_dict[i][\"positions\"] = stacked_positions\n",
    "                    #for j plus one_hot\n",
    "                    padding = torch.zeros(data_j_plus_dict[i][\"one_hot\"].shape[0], num_atoms_to_stack, data_j_plus_dict[i][\"one_hot\"].shape[2]).to(device)\n",
    "                    stacked_one_hot = torch.cat((data_j_plus_dict[i][\"one_hot\"], padding), dim=1)\n",
    "                    data_j_plus_dict[i][\"one_hot\"] = stacked_one_hot\n",
    "                    padding = torch.zeros(data_j_plus_dict[i][\"fragment_mask\"].shape[0], num_atoms_to_stack, data_j_plus_dict[i][\"fragment_mask\"].shape[2]).to(device)\n",
    "                    stacked_fragment_mask = torch.cat((data_j_plus_dict[i][\"fragment_mask\"], padding), dim=1)\n",
    "                    data_j_plus_dict[i][\"fragment_mask\"] = stacked_fragment_mask\n",
    "                    padding = torch.zeros(data_j_plus_dict[i][\"charges\"].shape[0], num_atoms_to_stack, data_j_plus_dict[i][\"charges\"].shape[2]).to(device)\n",
    "                    stacked_charges = torch.cat((data_j_plus_dict[i][\"charges\"], padding), dim=1)\n",
    "                    data_j_plus_dict[i][\"charges\"] = stacked_charges\n",
    "                    padding = torch.zeros(data_j_plus_dict[i][\"anchors\"].shape[0], num_atoms_to_stack, data_j_plus_dict[i][\"anchors\"].shape[2]).to(device)\n",
    "                    stacked_anchors = torch.cat((data_j_plus_dict[i][\"anchors\"], padding), dim=1)\n",
    "                    data_j_plus_dict[i][\"anchors\"] = stacked_anchors\n",
    "                    padding = torch.zeros(data_j_plus_dict[i][\"linker_mask\"].shape[0], num_atoms_to_stack, data_j_plus_dict[i][\"linker_mask\"].shape[2]).to(device)\n",
    "                    stacked_linker_mask = torch.cat((data_j_plus_dict[i][\"linker_mask\"], padding), dim=1)\n",
    "                    data_j_plus_dict[i][\"linker_mask\"] = stacked_linker_mask\n",
    "                    padding = torch.zeros(data_j_plus_dict[i][\"atom_mask\"].shape[0], num_atoms_to_stack, data_j_plus_dict[i][\"atom_mask\"].shape[2]).to(device)\n",
    "                    stacked_atom_mask = torch.cat((data_j_plus_dict[i][\"atom_mask\"], padding), dim=1)\n",
    "                    data_j_plus_dict[i][\"atom_mask\"] = stacked_atom_mask\n",
    "                    num_edges_to_stack = max_edges_j_plus - data_j_plus_dict[i][\"edge_mask\"].shape[0]\n",
    "                    data_j_plus_dict[i][\"edge_mask\"] = data_j_plus_dict[i][\"edge_mask\"].unsqueeze(0)\n",
    "                    padding = torch.zeros(data_j_plus_dict[i][\"edge_mask\"].shape[0], num_edges_to_stack, data_j_plus_dict[i][\"edge_mask\"].shape[2]).to(device)\n",
    "                    stacked_edge_mask = torch.cat((data_j_plus_dict[i][\"edge_mask\"], padding), dim=1)\n",
    "                    data_j_plus_dict[i][\"edge_mask\"] = stacked_edge_mask\n",
    "                    \n",
    "                    #for noisy positions and features for j plus\n",
    "                    noisy_positions_j_plus_dict[i] = noisy_positions_j_plus_dict[i] #check this\n",
    "                    padding = torch.zeros(noisy_positions_j_plus_dict[i].shape[0], num_atoms_to_stack, noisy_positions_j_plus_dict[i].shape[2]).to(device)\n",
    "                    stacked_positions = torch.cat((noisy_positions_j_plus_dict[i], padding), dim=1)\n",
    "                    noisy_positions_j_plus_dict[i] = stacked_positions\n",
    "\n",
    "                    noisy_features_j_plus_dict[i] = noisy_features_j_plus_dict[i]\n",
    "                    padding = torch.zeros(noisy_features_j_plus_dict[i].shape[0], num_atoms_to_stack, noisy_features_j_plus_dict[i].shape[2]).to(device)\n",
    "                    stacked_features = torch.cat((noisy_features_j_plus_dict[i], padding), dim=1)\n",
    "                    noisy_features_j_plus_dict[i] = stacked_features\n",
    "\n",
    "                    #for j minus\n",
    "                    num_atoms_to_stack = max_atoms_j_minus - data_j_minus_dict[i][\"positions\"].shape[1]\n",
    "                    padding = torch.zeros(data_j_minus_dict[i][\"positions\"].shape[0], num_atoms_to_stack, data_j_minus_dict[i][\"positions\"].shape[2]).to(device) \n",
    "                    stacked_positions = torch.cat((data_j_minus_dict[i][\"positions\"], padding), dim=1)\n",
    "                    data_j_minus_dict[i][\"positions\"] = stacked_positions\n",
    "                    \n",
    "                    padding = torch.zeros(data_j_minus_dict[i][\"one_hot\"].shape[0], num_atoms_to_stack, data_j_minus_dict[i][\"one_hot\"].shape[2]).to(device)\n",
    "                    stacked_one_hot = torch.cat((data_j_minus_dict[i][\"one_hot\"], padding), dim=1)\n",
    "                    data_j_minus_dict[i][\"one_hot\"] = stacked_one_hot\n",
    "                    \n",
    "                    padding = torch.zeros(data_j_minus_dict[i][\"fragment_mask\"].shape[0], num_atoms_to_stack, data_j_minus_dict[i][\"fragment_mask\"].shape[2]).to(device)\n",
    "                    stacked_fragment_mask = torch.cat((data_j_minus_dict[i][\"fragment_mask\"], padding), dim=1)\n",
    "                    data_j_minus_dict[i][\"fragment_mask\"] = stacked_fragment_mask\n",
    "\n",
    "                    \n",
    "                    padding = torch.zeros(data_j_minus_dict[i][\"charges\"].shape[0], num_atoms_to_stack, data_j_minus_dict[i][\"charges\"].shape[2]).to(device)\n",
    "                    stacked_charges = torch.cat((data_j_minus_dict[i][\"charges\"], padding), dim=1)\n",
    "                    data_j_minus_dict[i][\"charges\"] = stacked_charges\n",
    "                    \n",
    "                    padding = torch.zeros(data_j_minus_dict[i][\"anchors\"].shape[0], num_atoms_to_stack, data_j_minus_dict[i][\"anchors\"].shape[2]).to(device)\n",
    "                    stacked_anchors = torch.cat((data_j_minus_dict[i][\"anchors\"], padding), dim=1)\n",
    "                    data_j_minus_dict[i][\"anchors\"] = stacked_anchors\n",
    "                    \n",
    "                    padding = torch.zeros(data_j_minus_dict[i][\"linker_mask\"].shape[0], num_atoms_to_stack, data_j_minus_dict[i][\"linker_mask\"].shape[2]).to(device)\n",
    "                    stacked_linker_mask = torch.cat((data_j_minus_dict[i][\"linker_mask\"], padding), dim=1)\n",
    "                    data_j_minus_dict[i][\"linker_mask\"] = stacked_linker_mask\n",
    "                    \n",
    "                    padding = torch.zeros(data_j_minus_dict[i][\"atom_mask\"].shape[0], num_atoms_to_stack, data_j_minus_dict[i][\"atom_mask\"].shape[2]).to(device)\n",
    "                    stacked_atom_mask = torch.cat((data_j_minus_dict[i][\"atom_mask\"], padding), dim=1)\n",
    "                    data_j_minus_dict[i][\"atom_mask\"] = stacked_atom_mask\n",
    "                    \n",
    "                    num_edges_to_stack = max_edges_j_minus - data_j_minus_dict[i][\"edge_mask\"].shape[0]\n",
    "                    data_j_minus_dict[i][\"edge_mask\"] = data_j_minus_dict[i][\"edge_mask\"].unsqueeze(0)\n",
    "                    padding = torch.zeros(data_j_minus_dict[i][\"edge_mask\"].shape[0], num_edges_to_stack, data_j_minus_dict[i][\"edge_mask\"].shape[2]).to(device)\n",
    "                    stacked_edge_mask = torch.cat((data_j_minus_dict[i][\"edge_mask\"], padding), dim=1)\n",
    "                    data_j_minus_dict[i][\"edge_mask\"] = stacked_edge_mask\n",
    "                \n",
    "                    #for noisy positions and features for j plus\n",
    "                    noisy_positions_j_minus_dict[i] = noisy_positions_j_minus_dict[i] #check this\n",
    "                    padding = torch.zeros(noisy_positions_j_minus_dict[i].shape[0], num_atoms_to_stack, noisy_positions_j_minus_dict[i].shape[2]).to(device)\n",
    "                    stacked_positions = torch.cat((noisy_positions_j_minus_dict[i], padding), dim=1)\n",
    "                    noisy_positions_j_minus_dict[i] = stacked_positions\n",
    "\n",
    "                    noisy_features_j_minus_dict[i] = noisy_features_j_minus_dict[i]\n",
    "                    padding = torch.zeros(noisy_features_j_minus_dict[i].shape[0], num_atoms_to_stack, noisy_features_j_minus_dict[i].shape[2]).to(device)\n",
    "                    stacked_features = torch.cat((noisy_features_j_minus_dict[i], padding), dim=1)\n",
    "                    noisy_features_j_minus_dict[i] = stacked_features\n",
    "\n",
    "                    #for random\n",
    "                    num_atoms_to_stack = max_atoms_random - data_random_dict[i][\"positions\"].shape[1]\n",
    "                    padding = torch.zeros(data_random_dict[i][\"positions\"].shape[0], num_atoms_to_stack, data_random_dict[i][\"positions\"].shape[2]).to(device)\n",
    "                    stacked_positions = torch.cat((data_random_dict[i][\"positions\"], padding), dim=1)\n",
    "                    data_random_dict[i][\"positions\"] = stacked_positions\n",
    "                    \n",
    "                    padding = torch.zeros(data_random_dict[i][\"one_hot\"].shape[0], num_atoms_to_stack, data_random_dict[i][\"one_hot\"].shape[2]).to(device)\n",
    "                    stacked_one_hot = torch.cat((data_random_dict[i][\"one_hot\"], padding), dim=1)\n",
    "                    data_random_dict[i][\"one_hot\"] = stacked_one_hot\n",
    "                    \n",
    "                    padding = torch.zeros(data_random_dict[i][\"fragment_mask\"].shape[0], num_atoms_to_stack, data_random_dict[i][\"fragment_mask\"].shape[2]).to(device)\n",
    "                    stacked_fragment_mask = torch.cat((data_random_dict[i][\"fragment_mask\"], padding), dim=1)\n",
    "                    data_random_dict[i][\"fragment_mask\"] = stacked_fragment_mask\n",
    "                    \n",
    "                    padding = torch.zeros(data_random_dict[i][\"linker_mask\"].shape[0], num_atoms_to_stack, data_random_dict[i][\"linker_mask\"].shape[2]).to(device)\n",
    "                    stacked_linker_mask = torch.cat((data_random_dict[i][\"linker_mask\"], padding), dim=1)\n",
    "                    data_random_dict[i][\"linker_mask\"] = stacked_linker_mask\n",
    "\n",
    "                    \n",
    "                    padding = torch.zeros(data_random_dict[i][\"charges\"].shape[0], num_atoms_to_stack, data_random_dict[i][\"charges\"].shape[2]).to(device)\n",
    "                    stacked_charges = torch.cat((data_random_dict[i][\"charges\"], padding), dim=1)\n",
    "                    data_random_dict[i][\"charges\"] = stacked_charges\n",
    "\n",
    "                \n",
    "                    padding = torch.zeros(data_random_dict[i][\"anchors\"].shape[0], num_atoms_to_stack, data_random_dict[i][\"anchors\"].shape[2]).to(device)\n",
    "                    stacked_anchors = torch.cat((data_random_dict[i][\"anchors\"], padding), dim=1)\n",
    "                    data_random_dict[i][\"anchors\"] = stacked_anchors\n",
    "                    \n",
    "                    padding = torch.zeros(data_random_dict[i][\"atom_mask\"].shape[0], num_atoms_to_stack, data_random_dict[i][\"atom_mask\"].shape[2]).to(device)\n",
    "                    stacked_atom_mask = torch.cat((data_random_dict[i][\"atom_mask\"], padding), dim=1)\n",
    "                    data_random_dict[i][\"atom_mask\"] = stacked_atom_mask\n",
    "                    \n",
    "                    num_edges_to_stack = max_edges_random - data_random_dict[i][\"edge_mask\"].shape[0]\n",
    "                    data_random_dict[i][\"edge_mask\"] = data_random_dict[i][\"edge_mask\"].unsqueeze(0)\n",
    "                    padding = torch.zeros(data_random_dict[i][\"edge_mask\"].shape[0], num_edges_to_stack, data_random_dict[i][\"edge_mask\"].shape[2]).to(device)\n",
    "                    stacked_edge_mask = torch.cat((data_random_dict[i][\"edge_mask\"], padding), dim=1)\n",
    "                    data_random_dict[i][\"edge_mask\"] = stacked_edge_mask\n",
    "\n",
    "                    #for noisy positions and features for j plus\n",
    "                    noisy_positions_random_dict[i] = noisy_positions_random_dict[i] #check this\n",
    "                    padding = torch.zeros(noisy_positions_random_dict[i].shape[0], num_atoms_to_stack, noisy_positions_random_dict[i].shape[2]).to(device)\n",
    "                    stacked_positions = torch.cat((noisy_positions_random_dict[i], padding), dim=1)\n",
    "                    noisy_positions_random_dict[i] = stacked_positions\n",
    "\n",
    "                    noisy_features_random_dict[i] = noisy_features_random_dict[i]\n",
    "                    padding = torch.zeros(noisy_features_random_dict[i].shape[0], num_atoms_to_stack, noisy_features_random_dict[i].shape[2]).to(device)\n",
    "                    stacked_features = torch.cat((noisy_features_random_dict[i], padding), dim=1)\n",
    "                    noisy_features_random_dict[i] = stacked_features\n",
    "                        \n",
    "                        \n",
    "                #create batch for j plus\n",
    "                data_j_plus_batch = {}\n",
    "                data_j_plus_batch[\"positions\"] = torch.stack([data_j_plus_dict[i][\"positions\"] for i in range(PARALLEL_STEPS)], dim=0).squeeze()\n",
    "                data_j_plus_batch[\"one_hot\"] = torch.stack([data_j_plus_dict[i][\"one_hot\"] for i in range(PARALLEL_STEPS)], dim=0).squeeze(1)\n",
    "                data_j_plus_batch[\"atom_mask\"] = torch.stack([data_j_plus_dict[i][\"atom_mask\"] for i in range(PARALLEL_STEPS)], dim=0).squeeze(1)\n",
    "                data_j_plus_batch[\"fragment_mask\"] = torch.stack([data_j_plus_dict[i][\"fragment_mask\"] for i in range(PARALLEL_STEPS)], dim=0).squeeze(1)\n",
    "                data_j_plus_batch[\"linker_mask\"] = torch.stack([data_j_plus_dict[i][\"linker_mask\"] for i in range(PARALLEL_STEPS)], dim=0).squeeze(1)\n",
    "                data_j_plus_batch[\"charges\"] = torch.stack([data_j_plus_dict[i][\"charges\"] for i in range(PARALLEL_STEPS)], dim=0).squeeze(1)\n",
    "                data_j_plus_batch[\"anchors\"] = torch.stack([data_j_plus_dict[i][\"anchors\"] for i in range(PARALLEL_STEPS)], dim=0).squeeze(1)\n",
    "                \n",
    "                \n",
    "                data_j_plus_batch[\"uuid\"] = [i for i in range(PARALLEL_STEPS)]\n",
    "                data_j_plus_batch[\"num_atoms\"] = [data_j_plus_dict[i][\"num_atoms\"] for i in range(PARALLEL_STEPS)]\n",
    "                data_j_plus_batch[\"name\"] = [data[\"name\"] for _ in range(PARALLEL_STEPS)]\n",
    "                data_j_plus_batch[\"edge_mask\"] = torch.cat([data_j_plus_dict[i][\"edge_mask\"] for i in range(PARALLEL_STEPS)], dim=0).squeeze().view(-1).unsqueeze(1)\n",
    "\n",
    "\n",
    "                #create batch for j minus\n",
    "                data_j_minus_batch = {}\n",
    "                data_j_minus_batch[\"positions\"] = torch.stack([data_j_minus_dict[i][\"positions\"] for i in range(PARALLEL_STEPS)], dim=0).squeeze()\n",
    "                data_j_minus_batch[\"one_hot\"] = torch.stack([data_j_minus_dict[i][\"one_hot\"] for i in range(PARALLEL_STEPS)], dim=0).squeeze(1)\n",
    "                data_j_minus_batch[\"atom_mask\"] = torch.stack([data_j_minus_dict[i][\"atom_mask\"] for i in range(PARALLEL_STEPS)], dim=0).squeeze(1)\n",
    "                data_j_minus_batch[\"fragment_mask\"] = torch.stack([data_j_minus_dict[i][\"fragment_mask\"] for i in range(PARALLEL_STEPS)], dim=0).squeeze(1)\n",
    "                data_j_minus_batch[\"linker_mask\"] = torch.stack([data_j_minus_dict[i][\"linker_mask\"] for i in range(PARALLEL_STEPS)], dim=0).squeeze(1)\n",
    "                data_j_minus_batch[\"charges\"] = torch.stack([data_j_minus_dict[i][\"charges\"] for i in range(PARALLEL_STEPS)], dim=0).squeeze(1)\n",
    "                data_j_minus_batch[\"anchors\"] = torch.stack([data_j_minus_dict[i][\"anchors\"] for i in range(PARALLEL_STEPS)], dim=0).squeeze(1)\n",
    "                \n",
    "                data_j_minus_batch[\"uuid\"] = [i for i in range(PARALLEL_STEPS)]\n",
    "                data_j_minus_batch[\"num_atoms\"] = [data_j_minus_dict[i][\"num_atoms\"] for i in range(PARALLEL_STEPS)]\n",
    "                data_j_minus_batch[\"name\"] = [data[\"name\"] for _ in range(PARALLEL_STEPS)]\n",
    "                data_j_minus_batch[\"edge_mask\"] = torch.cat([data_j_minus_dict[i][\"edge_mask\"] for i in range(PARALLEL_STEPS)], dim=0).squeeze().view(-1).unsqueeze(1)\n",
    "\n",
    "                #create batch for random\n",
    "                data_random_batch = {}\n",
    "                data_random_batch[\"positions\"] = torch.stack([data_random_dict[i][\"positions\"] for i in range(PARALLEL_STEPS)], dim=0).squeeze()\n",
    "                data_random_batch[\"one_hot\"] = torch.stack([data_random_dict[i][\"one_hot\"] for i in range(PARALLEL_STEPS)], dim=0).squeeze(1)\n",
    "                data_random_batch[\"atom_mask\"] = torch.stack([data_random_dict[i][\"atom_mask\"] for i in range(PARALLEL_STEPS)], dim=0).squeeze(1)\n",
    "                data_random_batch[\"fragment_mask\"] = torch.stack([data_random_dict[i][\"fragment_mask\"] for i in range(PARALLEL_STEPS)], dim=0).squeeze(1)\n",
    "                data_random_batch[\"linker_mask\"] = torch.stack([data_random_dict[i][\"linker_mask\"] for i in range(PARALLEL_STEPS)], dim=0).squeeze(1)\n",
    "                data_random_batch[\"charges\"] = torch.stack([data_random_dict[i][\"charges\"] for i in range(PARALLEL_STEPS)], dim=0).squeeze(1)\n",
    "                data_random_batch[\"anchors\"] = torch.stack([data_random_dict[i][\"anchors\"] for i in range(PARALLEL_STEPS)], dim=0).squeeze(1)\n",
    "                \n",
    "                data_random_batch[\"uuid\"] = [i for i in range(PARALLEL_STEPS)]\n",
    "                data_random_batch[\"num_atoms\"] = [data_random_dict[i][\"num_atoms\"] for i in range(PARALLEL_STEPS)]\n",
    "                data_random_batch[\"name\"] = [data[\"name\"] for _ in range(PARALLEL_STEPS)]\n",
    "                data_random_batch[\"edge_mask\"] = torch.cat([data_random_dict[i][\"edge_mask\"] for i in range(PARALLEL_STEPS)], dim=0).squeeze().view(-1).unsqueeze(1)\n",
    "\n",
    "                #create batches for noisy positions and features\n",
    "                noisy_positions_batch_j_plus = torch.stack([noisy_positions_j_plus_dict[i] for i in range(PARALLEL_STEPS)], dim=0).squeeze()\n",
    "                noisy_features_batch_j_plus = torch.stack([noisy_features_j_plus_dict[i] for i in range(PARALLEL_STEPS)], dim=0).squeeze()\n",
    "\n",
    "                noisy_positions_batch_j_minus = torch.stack([noisy_positions_j_minus_dict[i] for i in range(PARALLEL_STEPS)], dim=0).squeeze()\n",
    "                noisy_features_batch_j_minus = torch.stack([noisy_features_j_minus_dict[i] for i in range(PARALLEL_STEPS)], dim=0).squeeze()\n",
    "\n",
    "                noisy_positions_batch_random = torch.stack([noisy_positions_random_dict[i] for i in range(PARALLEL_STEPS)], dim=0).squeeze()\n",
    "                noisy_features_batch_random = torch.stack([noisy_features_random_dict[i] for i in range(PARALLEL_STEPS)], dim=0).squeeze()\n",
    "                \n",
    "\n",
    "                chain_j_plus_batch, node_mask_j_plus_batch = model.sample_chain(data_j_plus_batch, keep_frames=keep_frames, noisy_positions=noisy_positions_batch_j_plus, noisy_features=noisy_features_batch_j_plus)\n",
    "\n",
    "                chain_j_plus = chain_j_plus_batch[0, :, :, :] \n",
    "                j_plus_original_positions = data_j_plus_batch[\"positions\"].clone()\n",
    "\n",
    "                chain_j_plus_positions = chain_j_plus[:, :, :3]\n",
    "                position_differences_j_plus = j_plus_original_positions - chain_j_plus_positions\n",
    "                \n",
    "                fragment_and_linker_mask = data_j_plus_batch[\"fragment_mask\"].squeeze().bool() | data_j_plus_batch[\"linker_mask\"].squeeze().bool()\n",
    "                \n",
    "                position_differences_j_plus_to_use = torch.zeros((PARALLEL_STEPS, 3), device=device)\n",
    "                for step in range(PARALLEL_STEPS):\n",
    "                    position_differences_j_plus_to_use[step, :] = position_differences_j_plus[step][data_j_plus_batch[\"fragment_mask\"].squeeze().bool()[step]][0, :]\n",
    "                \n",
    "                \n",
    "                for step in range(PARALLEL_STEPS):\n",
    "                    chain_j_plus[step, fragment_and_linker_mask[step], :3] = chain_j_plus[step, fragment_and_linker_mask[step], :3] + position_differences_j_plus_to_use[step]\n",
    "                \n",
    "                chain_j_minus_batch, node_mask_j_minus_batch = model.sample_chain(data_j_minus_batch, keep_frames=keep_frames, noisy_positions=noisy_positions_batch_j_minus, noisy_features=noisy_features_batch_j_minus)\n",
    "\n",
    "                chain_j_minus = chain_j_minus_batch[0, :, :, :]\n",
    "\n",
    "                j_minus_original_positions = data_j_minus_batch[\"positions\"].clone()\n",
    "\n",
    "                chain_j_minus_positions = chain_j_minus[:, :, :3]\n",
    "                position_differences_j_minus = j_minus_original_positions - chain_j_minus_positions\n",
    "                \n",
    "                fragment_and_linker_mask = data_j_minus_batch[\"fragment_mask\"].squeeze().bool() | data_j_minus_batch[\"linker_mask\"].squeeze().bool()\n",
    "                \n",
    "                position_differences_j_minus_to_use = torch.zeros((PARALLEL_STEPS, 3), device=device)\n",
    "                for step in range(PARALLEL_STEPS):\n",
    "                    position_differences_j_minus_to_use[step, :] = position_differences_j_minus[step][data_j_minus_batch[\"fragment_mask\"].squeeze().bool()[step]][0, :]\n",
    "                \n",
    "                for step in range(PARALLEL_STEPS):\n",
    "                    chain_j_minus[step, fragment_and_linker_mask[step], :3] = chain_j_minus[step, fragment_and_linker_mask[step], :3] + position_differences_j_minus_to_use[step]\n",
    "                \n",
    "                chain_random_batch, node_mask_random_batch = model.sample_chain(data_random_batch, keep_frames=keep_frames, noisy_positions=noisy_positions_batch_random, noisy_features=noisy_features_batch_random)\n",
    "\n",
    "                chain_random = chain_random_batch[0, :, :, :]\n",
    "\n",
    "                random_original_positions = data_random_batch[\"positions\"].clone()\n",
    "                \n",
    "                chain_random_positions = chain_random[:, :, :3]\n",
    "                position_differences_random = random_original_positions - chain_random_positions\n",
    "                \n",
    "                fragment_and_linker_mask = data_random_batch[\"fragment_mask\"].squeeze().bool() | data_random_batch[\"linker_mask\"].squeeze().bool()\n",
    "                \n",
    "                position_differences_random_to_use = torch.zeros((PARALLEL_STEPS, 3), device=device)\n",
    "                for step in range(PARALLEL_STEPS):\n",
    "                    position_differences_random_to_use[step, :] = position_differences_random[step][data_random_batch[\"fragment_mask\"].squeeze().bool()[step]][0, :]\n",
    "                \n",
    "                \n",
    "                for step in range(PARALLEL_STEPS):\n",
    "                    chain_random[step, fragment_and_linker_mask[step], :3] = chain_random[step, fragment_and_linker_mask[step], :3] + position_differences_random_to_use[step]\n",
    "                \n",
    "                chain_with_full_fragments_batch = chain_with_full_fragments.repeat(PARALLEL_STEPS, 1, 1)\n",
    "\n",
    "                \n",
    "                V_j_plus_hausdorff_batch = compute_hausdorff_distance_batch(chain_with_full_fragments_batch.cpu(), chain_j_plus.cpu(), mask1=original_linker_mask_batch.cpu(), mask2=data_j_plus_batch[\"linker_mask\"].squeeze().cpu())\n",
    "                \n",
    "                V_j_plus_hausdorff = sum(V_j_plus_hausdorff_batch)\n",
    "\n",
    "                V_j_minus_hausdorff_batch = compute_hausdorff_distance_batch(chain_with_full_fragments_batch.cpu(), chain_j_minus.cpu(), mask1=original_linker_mask_batch.cpu(), mask2=data_j_minus_batch[\"linker_mask\"].squeeze().cpu())\n",
    "\n",
    "                V_j_minus_hausdorff = sum(V_j_minus_hausdorff_batch)\n",
    "\n",
    "                V_random_hausdorff_batch = compute_hausdorff_distance_batch(chain_with_full_fragments_batch.cpu(), chain_random.cpu(), mask1=original_linker_mask_batch.cpu(), mask2=data_random_batch[\"linker_mask\"].squeeze().cpu())\n",
    "\n",
    "                for r_haus in V_random_hausdorff_batch:\n",
    "                    hausdorff_distances_random_samples.append(r_haus)\n",
    "\n",
    "                marginal_contrib_hausdorff += (V_j_plus_hausdorff - V_j_minus_hausdorff)\n",
    "                \n",
    "            phi_atoms[fragment_indices[j].item()] = [0]    \n",
    "            phi_atoms[fragment_indices[j].item()][0] = marginal_contrib_hausdorff/M\n",
    "\n",
    "        print(data[\"name\"])\n",
    "\n",
    "        phi_atoms_hausdorff = {}\n",
    "        for atom_index, phi_values in phi_atoms.items():\n",
    "            phi_atoms_hausdorff[atom_index] = phi_values[0]\n",
    "\n",
    "        \n",
    "        # Save phi_atoms to a text file\n",
    "        with open(f'{shapley_values_save_path}/shapley_values_atoms_{data_index}.txt', 'w') as write_file:\n",
    "            write_file.write(\"Sample SMILES: \" + str(data[\"name\"]) + \"\\n\")\n",
    "            write_file.write(\"atom_index,shapley_value\\n\")\n",
    "            for atom_index, phi_values in phi_atoms.items():\n",
    "                write_file.write(f\"{atom_index},{phi_values[0]}\\n\")\n",
    "\n",
    "            # write_file.write(\"\\n\")\n",
    "            \n",
    "            # write_file.write(\"Sum of phi values for hausdorff\\n\")\n",
    "            # write_file.write(str(sum([p_values[0] for p_values in phi_atoms.values()])) + \"\\n\")     \n",
    "            \n",
    "            # write_file.write(\"Average hausdorff distance random samples:\\n\")\n",
    "            # write_file.write(str(sum(hausdorff_distances_random_samples)/len(hausdorff_distances_random_samples)) + \"\\n\")      \n",
    "            \n",
    "            # write_file.write(\"Hausdorff distances random samples\\n\")\n",
    "            # write_file.write(str(hausdorff_distances_random_samples) + \"\\n\")\n",
    "\n",
    "        if SAVE_VISUALIZATION:\n",
    "            phi_values_for_viz = phi_atoms_hausdorff\n",
    "\n",
    "            # Saving chains and final states\n",
    "            for i in range(len(data['positions'])):\n",
    "                chain = chain_batch[:, i, :, :]\n",
    "                assert chain.shape[0] == keep_frames\n",
    "                assert chain.shape[1] == data['positions'].shape[1]\n",
    "                assert chain.shape[2] == data['positions'].shape[2] + data['one_hot'].shape[2] + model.include_charges\n",
    "\n",
    "                # Saving chains\n",
    "                name = str(i + start)\n",
    "                mapping_output = os.path.join(mapping_output_dir, \"graphs\", name)\n",
    "                os.makedirs(mapping_output, exist_ok=True)\n",
    "                \n",
    "                #save initial random distrubution with noise\n",
    "                positions_combined = torch.zeros_like(data['positions'])\n",
    "                one_hot_combined = torch.zeros_like(data['one_hot'])\n",
    "\n",
    "                # Iterate over each atom and decide whether to use original or noisy data\n",
    "                for atom_idx in range(data['positions'].shape[1]):\n",
    "                    if data['fragment_mask'][0, atom_idx] == 1:\n",
    "                        # Use original positions and features for fragment atoms\n",
    "                        positions_combined[:, atom_idx, :] = data['positions'][:, atom_idx, :]\n",
    "                        one_hot_combined[:, atom_idx, :] = data['one_hot'][:, atom_idx, :]\n",
    "                    else:\n",
    "                        # Use noisy positions and features for linker atoms\n",
    "                        positions_combined[:, atom_idx, :] = noisy_positions_present_atoms[:, atom_idx, :]\n",
    "                        one_hot_combined[:, atom_idx, :] = noisy_features_present_atoms[:, atom_idx, :]\n",
    "\n",
    "                #save initial distribution \n",
    "                save_xyz_file(\n",
    "                    mapping_output,\n",
    "                    one_hot_combined,\n",
    "                    positions_combined,\n",
    "                    node_mask[i].unsqueeze(0),\n",
    "                    names=[f'{name}_' + str(keep_frames)],\n",
    "                    is_geom=model.is_geom\n",
    "                )\n",
    "\n",
    "                \n",
    "                one_hot = chain[:, :, 3:]\n",
    "                positions = chain[:, :, :3]\n",
    "                chain_node_mask = torch.cat([node_mask[i].unsqueeze(0) for _ in range(keep_frames)], dim=0)\n",
    "                names = [f'{name}_{j}' for j in range(keep_frames + 1)]\n",
    "\n",
    "                save_xyz_file(mapping_output, one_hot, positions, chain_node_mask, names=names, is_geom=model.is_geom)\n",
    "                \n",
    "            \n",
    "                visualize_mapping_graph(\n",
    "                    mapping_output,\n",
    "                    spheres_3d=False,\n",
    "                    alpha=1.0,\n",
    "                    bg='white',\n",
    "                    is_geom=model.is_geom,\n",
    "                    fragment_mask=data['fragment_mask'][i].squeeze(),\n",
    "                    phi_values=list(phi_values_for_viz.values()) #this keeps the order as per implementarion but should be edited to be safe and guarantee the order\n",
    "                )\n",
    "\n",
    "                mapping_output_structure = os.path.join(mapping_output_dir, \"structures\", name)\n",
    "                \n",
    "                visualize_mapping_structure(\n",
    "                    file_names=names,\n",
    "                    generation_folder = mapping_output,\n",
    "                    shapley_values = list(phi_values_for_viz.values()),\n",
    "                    fragment_mask = data['fragment_mask'][0].cpu().numpy(),\n",
    "                    linker_mask = data['linker_mask'][0].cpu().numpy(),\n",
    "                    save_folder = mapping_output_structure\n",
    "                )\n",
    "\n",
    "            start += len(data['positions'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diff_explainer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
