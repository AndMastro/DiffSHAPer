{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"http_proxy\"] = \"http://web-proxy.informatik.uni-bonn.de:3128\"\n",
    "# os.environ[\"https_proxy\"] = \"http://web-proxy.informatik.uni-bonn.de:3128\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mastro\\anaconda3\\envs\\diff_explainer\\lib\\site-packages\\lightning_fabric\\utilities\\cloud_io.py:57: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.6.3 to v2.3.3. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint c:\\Repositories\\DiffLinker\\models\\zinc_difflinker.ckpt`\n",
      "c:\\Repositories\\DiffLinker\\src\\datasets.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.data = torch.load(dataset_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import torch\n",
    "\n",
    "from src.datasets import get_dataloader\n",
    "from src.lightning import DDPM\n",
    "from src.molecule_builder import get_bond_order\n",
    "from src.visualizer import save_xyz_file, visualize_chain\n",
    "from tqdm.auto import tqdm\n",
    "from pdb import set_trace\n",
    "import sys #@mastro\n",
    "from src import const #@mastro\n",
    "import numpy as np #@mastro\n",
    "from numpy.random import default_rng\n",
    "from sklearn.metrics import jaccard_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.spatial.distance import directed_hausdorff\n",
    "import random\n",
    "from sklearn.decomposition import PCA\n",
    "from src.visualizer import load_molecule_xyz, load_xyz_files\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "from src import const\n",
    "import networkx as nx\n",
    "import time \n",
    "\n",
    "from pysmiles import read_smiles\n",
    "#get running device from const file\n",
    "running_device = const.RUNNING_DEVICE\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "# os.environ[\"https_proxy\"] = \"http://web-proxy.informatik.uni-bonn.de:3128\"\n",
    "# os.environ[\"http_proxy\"] = \"http://web-proxy.informatik.uni-bonn.de:3128\"\n",
    "# Simulate command-line arguments\n",
    "sys.argv = [\n",
    "    'ipykernel_launcher.py',\n",
    "    '--checkpoint', 'models/zinc_difflinker.ckpt',\n",
    "    '--chains', 'trajectories',\n",
    "    '--data', 'datasets',\n",
    "    '--prefix', 'zinc_final_test',\n",
    "    '--keep_frames', '10',\n",
    "    '--device', 'cuda:0', #not used, it is set in the code\n",
    "    '--P', \"graph_density\"\n",
    "]\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--checkpoint', action='store', type=str, required=True)\n",
    "parser.add_argument('--chains', action='store', type=str, required=True)\n",
    "parser.add_argument('--prefix', action='store', type=str, required=True)\n",
    "parser.add_argument('--data', action='store', type=str, required=False, default=None)\n",
    "parser.add_argument('--keep_frames', action='store', type=int, required=True)\n",
    "parser.add_argument('--device', action='store', type=str, required=True) #not used, it is set in the code\n",
    "parser.add_argument('--P', action='store', type=str, required=True)\n",
    "args = parser.parse_args()\n",
    "\n",
    "args.device = running_device #@mastro\n",
    "SEED = 42\n",
    "experiment_name = args.checkpoint.split('/')[-1].replace('.ckpt', '')\n",
    "chains_output_dir = os.path.join(args.chains, experiment_name, args.prefix, 'chains_' + args.P)\n",
    "final_states_output_dir = os.path.join(args.chains, experiment_name, args.prefix, 'final_states_' + args.P)\n",
    "os.makedirs(chains_output_dir, exist_ok=True)\n",
    "os.makedirs(final_states_output_dir, exist_ok=True)\n",
    "\n",
    "# Loading model form checkpoint (all hparams will be automatically set)\n",
    "model = DDPM.load_from_checkpoint(args.checkpoint, map_location=args.device)\n",
    "\n",
    "# Possibility to evaluate on different datasets (e.g., on CASF instead of ZINC)\n",
    "model.val_data_prefix = args.prefix\n",
    "\n",
    "print(f\"Running device: {args.device}\")\n",
    "# In case <Anonymous> will run my model or vice versa\n",
    "if args.data is not None:\n",
    "    model.data_path = args.data\n",
    "\n",
    "model = model.eval().to(args.device)\n",
    "model.setup(stage='val')\n",
    "dataloader = get_dataloader(\n",
    "    model.val_dataset,\n",
    "    batch_size=1, #@mastro, it was 32\n",
    "    # batch_size=len(model.val_dataset)\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similarity functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_molecular_similarity(mol1, mol2, mask1 = None, mask2 = None):\n",
    "    \"\"\"\n",
    "    Compute the similarity between two molecules based on distances and atom type.\n",
    "    \n",
    "    Args:\n",
    "        mol1 (torch.Tensor): The first molecule.\n",
    "        mol2 (torch.Tensor): The second molecule.\n",
    "        mask (torch.Tensor, optional): A mask indicating which atoms to consider. If not provided, all atoms will be considered.\n",
    "        \n",
    "    Returns:\n",
    "        float: The similarity between the two molecules.\n",
    "    \"\"\"\n",
    "    # If fragmen_mask is provided, only consider the atoms in the mask\n",
    "    if mask1 is not None:\n",
    "        mask1 = mask1.bool()\n",
    "        mol1 = mol1[mask1,:]\n",
    "\n",
    "    if mask2 is not None:\n",
    "        mask2 = mask2.bool()\n",
    "        mol2 = mol2[mask2,:]\n",
    "\n",
    "    return 1 - torch.norm(mol1 - mol2)\n",
    "\n",
    "def compute_molecular_distance(mol1, mol2, mask1 = None, mask2 = None):\n",
    "    \"\"\"\n",
    "    Compute the similarity between two molecules based on distances and atom type.\n",
    "    \n",
    "    Args:\n",
    "        mol1 (torch.Tensor): The first molecule.\n",
    "        mol2 (torch.Tensor): The second molecule.\n",
    "        mask (torch.Tensor, optional): A mask indicating which atoms to consider. If not provided, all atoms will be considered.\n",
    "        \n",
    "    Returns:\n",
    "        float: The similarity between the two molecules.\n",
    "    \"\"\"\n",
    "    # If fragmen_mask is provided, only consider the atoms in the mask\n",
    "    if mask1 is not None:\n",
    "        mask1 = mask1.bool()\n",
    "        mol1 = mol1[mask1,:]\n",
    "\n",
    "    if mask2 is not None:\n",
    "        mask2 = mask2.bool()\n",
    "        mol2 = mol2[mask2,:]\n",
    "\n",
    "    return torch.norm(mol1 - mol2).item()\n",
    "\n",
    "def compute_molecular_distance_batch(mol1, mol2, mask1 = None, mask2 = None):\n",
    "    \"\"\"\n",
    "    Compute the similarity between two molecules based on distances and atom type.\n",
    "    \n",
    "    Args:\n",
    "        mol1 (torch.Tensor): The first molecule.\n",
    "        mol2 (torch.Tensor): The second molecule.\n",
    "        mask (torch.Tensor, optional): A mask indicating which atoms to consider. If not provided, all atoms will be considered.\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: The similarity between the two molecules for each element in the batch.\n",
    "    \"\"\"\n",
    "    # If fragment_mask is provided, only consider the atoms in the mask\n",
    "    if mask1 is not None:\n",
    "        mask1 = mask1.bool()\n",
    "        batch_size = mol1.shape[0]\n",
    "        masked_mol1 = []\n",
    "        for i in range(batch_size):\n",
    "            masked_mol1.append(mol1[i, mask1[i], :])\n",
    "\n",
    "        if batch_size == 1:\n",
    "            mol1 = masked_mol1[0].unsqueeze(0)\n",
    "        else:    \n",
    "            mol1 = torch.stack(masked_mol1)\n",
    "           \n",
    "    if mask2 is not None:\n",
    "        mask2 = mask2.bool()\n",
    "        batch_size = mol2.shape[0]\n",
    "        masked_mol2 = []\n",
    "        for i in range(batch_size):\n",
    "            masked_mol2.append(mol2[i, mask2[i], :])\n",
    "        \n",
    "        if batch_size == 1:\n",
    "            mol2 = masked_mol2[0].unsqueeze(0)\n",
    "        else:    \n",
    "            mol2 = torch.stack(masked_mol2)\n",
    "\n",
    "    return torch.norm(mol1 - mol2, dim=(1,2))\n",
    "\n",
    "def compute_cosine_similarity(mol1, mol2, mask1 = None, mask2 = None):\n",
    "    \"\"\"\n",
    "    Compute the similarity between two molecules based on distances and atom type.\n",
    "    \n",
    "    Args:\n",
    "        mol1 (torch.Tensor): The first molecule.\n",
    "        mol2 (torch.Tensor): The second molecule.\n",
    "        mask (torch.Tensor, optional): A mask indicating which atoms to consider. If not provided, all atoms will be considered.\n",
    "        \n",
    "    Returns:\n",
    "        float: The similarity between the two molecules.\n",
    "    \"\"\"\n",
    "    # If fragmen_mask is provided, only consider the atoms in the mask\n",
    "    if mask1 is not None:\n",
    "        mask1 = mask1.bool()\n",
    "        mol1 = mol1[mask1,:]\n",
    "\n",
    "    if mask2 is not None:\n",
    "        mask2 = mask2.bool()\n",
    "        mol2 = mol2[mask2,:]\n",
    "\n",
    "    return cosine_similarity(mol1.flatten().reshape(1, -1), mol2.flatten().reshape(1, -1)).item()\n",
    "\n",
    "\n",
    "def compute_cosine_similarity_batch(mol1, mol2, mask1 = None, mask2 = None):\n",
    "    \"\"\"\n",
    "    Compute the similarity between two molecules based on distances and atom type.\n",
    "    \n",
    "    Args:\n",
    "        mol1 (torch.Tensor): The first molecule.\n",
    "        mol2 (torch.Tensor): The second molecule.\n",
    "        mask (torch.Tensor, optional): A mask indicating which atoms to consider. If not provided, all atoms will be considered.\n",
    "        \n",
    "    Returns:\n",
    "        float: The similarity between the two molecules.\n",
    "    \"\"\"\n",
    "    # If fragmen_mask is provided, only consider the atoms in the mask\n",
    "    if mask1 is not None:\n",
    "        mask1 = mask1.bool()\n",
    "        batch_size = mol1.shape[0]\n",
    "        masked_mol1 = []\n",
    "        for i in range(batch_size):\n",
    "            masked_mol1.append(mol1[i, mask1[i], :])\n",
    "        \n",
    "        if batch_size == 1:\n",
    "            mol1 = masked_mol1[0].unsqueeze(0)\n",
    "        else:    \n",
    "            mol1 = torch.stack(masked_mol1)\n",
    "        \n",
    "\n",
    "    if mask2 is not None:\n",
    "        mask2 = mask2.bool()\n",
    "        mask2 = mask2.bool()\n",
    "        batch_size = mol2.shape[0]\n",
    "        masked_mol2 = []\n",
    "        for i in range(batch_size):\n",
    "            masked_mol2.append(mol2[i, mask2[i], :])\n",
    "        \n",
    "        if batch_size == 1:\n",
    "            mol2 = masked_mol2[0].unsqueeze(0)\n",
    "        else:    \n",
    "            mol2 = torch.stack(masked_mol2)\n",
    "\n",
    "    cos_sims = []\n",
    "    for i in range(mol1.shape[0]):\n",
    "        cos_sims.append(cosine_similarity(mol1[i].flatten().reshape(1, -1), mol2[i].flatten().reshape(1, -1)).item())\n",
    "\n",
    "    return cos_sims\n",
    "\n",
    "def compute_molecular_similarity_positions(mol1, mol2, mask1 = None, mask2 = None):\n",
    "    \"\"\"\n",
    "    Compute the similarity between two molecules based on positions.\n",
    "    \n",
    "    Args:\n",
    "        mol1 (torch.Tensor): The first molecule.\n",
    "        mol2 (torch.Tensor): The second molecule.\n",
    "        mask (torch.Tensor, optional): A mask indicating which atoms to consider. If not provided, all atoms will be considered.\n",
    "        \n",
    "    Returns:\n",
    "        float: The similarity between the two molecules.\n",
    "    \"\"\"\n",
    "    # If fragmen_mask is provided, only consider the atoms in the mask\n",
    "    positions1 = mol1[:, :3].squeeze()\n",
    "    positions2 = mol2[:, :3].squeeze()\n",
    "\n",
    "    if mask1 is not None:\n",
    "        mask1 = mask1.bool()\n",
    "        positions1 = positions1[mask1,:]\n",
    "\n",
    "    if mask2 is not None:\n",
    "        mask2 = mask2.bool()\n",
    "        positions2 = positions2[mask2,:]\n",
    "\n",
    "\n",
    "    return 1 - torch.norm(positions1 - positions2) #choose if distance or similarity, need to check what it the better choice\n",
    "\n",
    "def compute_one_hot_similarity(mol1, mol2, mask1 = None, mask2 = None):\n",
    "    \"\"\"\n",
    "    Computes the similarity between two one-hot encoded molecules. The one-hot encoding indicates the atom type\n",
    "    \n",
    "    Args:\n",
    "        mol1 (torch.Tensor): The first one-hot encoded molecule.\n",
    "        mol2 (torch.Tensor): The second one-hot encoded molecule.\n",
    "        mask (torch.Tensor, optional): A mask to apply on the atoms. Defaults to None.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: The similarity between the two molecules.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Apply mask if provided\n",
    "    if mask1 is not None:\n",
    "        mask1 = mask1.bool()\n",
    "        mol1 = mol1[mask1,:]\n",
    "\n",
    "    if mask2 is not None:\n",
    "        mask2 = mask2.bool()\n",
    "        mol2 = mol2[mask2,:]\n",
    "    \n",
    "    # Compute similarity by comparing the one-hot encoded features\n",
    "    similarity = torch.sum(mol1[:,3:-1] == mol2[:,3:-1]) / mol1[:, 3:-1].numel()\n",
    "    \n",
    "    return similarity\n",
    "\n",
    "def create_edge_index(mol, weighted=False):\n",
    "    \"\"\"\n",
    "    Create edge index for a molecule.\n",
    "    \"\"\"\n",
    "    adj = nx.to_scipy_sparse_array(mol).todense()\n",
    "    row = torch.from_numpy(adj.row.astype(np.int64)).to(torch.long)\n",
    "    col = torch.from_numpy(adj.col.astype(np.int64)).to(torch.long)\n",
    "    edge_index = torch.stack([row, col], dim=0)\n",
    "\n",
    "    if weighted:\n",
    "        weights = torch.from_numpy(adj.data.astype(np.float32))\n",
    "        edge_weight = torch.FloatTensor(weights)\n",
    "        return edge_index, edge_weight\n",
    "\n",
    "    return edge_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explainability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility function for visualization purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_sphere_xai(ax, x, y, z, size, color, alpha):\n",
    "    u = np.linspace(0, 2 * np.pi, 100)\n",
    "    v = np.linspace(0, np.pi, 100)\n",
    "\n",
    "    xs = size * np.outer(np.cos(u), np.sin(v))\n",
    "    ys = size * np.outer(np.sin(u), np.sin(v)) #* 0.8\n",
    "    zs = size * np.outer(np.ones(np.size(u)), np.cos(v))\n",
    "    ax.plot_surface(x + xs, y + ys, z + zs, rstride=2, cstride=2, color=color, alpha=alpha)\n",
    "\n",
    "def plot_molecule_xai(ax, positions, atom_type, alpha, spheres_3d, hex_bg_color, is_geom, fragment_mask=None, phi_values=None):\n",
    "    x = positions[:, 0]\n",
    "    y = positions[:, 1]\n",
    "    z = positions[:, 2]\n",
    "    # Hydrogen, Carbon, Nitrogen, Oxygen, Flourine\n",
    "\n",
    "    idx2atom = const.GEOM_IDX2ATOM if is_geom else const.IDX2ATOM\n",
    "\n",
    "    colors_dic = np.array(const.COLORS)\n",
    "    radius_dic = np.array(const.RADII)\n",
    "    area_dic = 1500 * radius_dic ** 2\n",
    "\n",
    "    areas = area_dic[atom_type]\n",
    "    radii = radius_dic[atom_type]\n",
    "    colors = colors_dic[atom_type]\n",
    "\n",
    "    if fragment_mask is None:\n",
    "        fragment_mask = torch.ones(len(x))\n",
    "\n",
    "    for i in range(len(x)):\n",
    "        for j in range(i + 1, len(x)):\n",
    "            p1 = np.array([x[i], y[i], z[i]])\n",
    "            p2 = np.array([x[j], y[j], z[j]])\n",
    "            dist = np.sqrt(np.sum((p1 - p2) ** 2))\n",
    "            atom1, atom2 = idx2atom[atom_type[i]], idx2atom[atom_type[j]]\n",
    "            draw_edge_int = get_bond_order(atom1, atom2, dist)\n",
    "            line_width = (3 - 2) * 2 * 2\n",
    "            draw_edge = draw_edge_int > 0\n",
    "            if draw_edge:\n",
    "                if draw_edge_int == 4:\n",
    "                    linewidth_factor = 1.5\n",
    "                else:\n",
    "                    linewidth_factor = 1\n",
    "                linewidth_factor *= 0.5\n",
    "                ax.plot(\n",
    "                    [x[i], x[j]], [y[i], y[j]], [z[i], z[j]],\n",
    "                    linewidth=line_width * linewidth_factor * 2,\n",
    "                    c=hex_bg_color,\n",
    "                    alpha=alpha\n",
    "                )\n",
    "\n",
    "    # from pdb import set_trace\n",
    "    # set_trace()\n",
    "\n",
    "    if spheres_3d:\n",
    "        # idx = torch.where(fragment_mask[:len(x)] == 0)[0]\n",
    "        # ax.scatter(\n",
    "        #     x[idx],\n",
    "        #     y[idx],\n",
    "        #     z[idx],\n",
    "        #     alpha=0.9 * alpha,\n",
    "        #     edgecolors='#FCBA03',\n",
    "        #     facecolors='none',\n",
    "        #     linewidths=2,\n",
    "        #     s=900\n",
    "        # )\n",
    "        for i, j, k, s, c, f, phi in zip(x, y, z, radii, colors, fragment_mask, phi_values):\n",
    "            if f == 1:\n",
    "                alpha = 1.0\n",
    "                if phi > 0:\n",
    "                    c = 'red'\n",
    "\n",
    "            draw_sphere_xai(ax, i.item(), j.item(), k.item(), 0.5 * s, c, alpha)\n",
    "\n",
    "    else:\n",
    "        phi_values_array = np.array(list(phi_values.values()))\n",
    "\n",
    "        # #draw fragments\n",
    "        # fragment_mask_on_cpu = fragment_mask.cpu().numpy()\n",
    "        # colors_fragment = colors[fragment_mask_on_cpu == 1]\n",
    "        # x_fragment = x[fragment_mask_on_cpu == 1]\n",
    "        # y_fragment = y[fragment_mask_on_cpu == 1]\n",
    "        # z_fragment = z[fragment_mask_on_cpu == 1]\n",
    "        # areas_fragment = areas[fragment_mask_on_cpu == 1]\n",
    "        # ax.scatter(x_fragment, y_fragment, z_fragment, s=areas_fragment, alpha=0.9 * alpha, c=np.where(phi_values_array > 0, 'red', colors_fragment))\n",
    "\n",
    "        # #draw non-fragment atoms\n",
    "        # colors = colors[fragment_mask_on_cpu == 0]\n",
    "        # x = x[fragment_mask_on_cpu == 0]\n",
    "        # y = y[fragment_mask_on_cpu == 0]\n",
    "        # z = z[fragment_mask_on_cpu == 0]\n",
    "        # areas = areas[fragment_mask_on_cpu == 0]\n",
    "        # ax.scatter(x, y, z, s=areas, alpha=0.9 * alpha, c=colors)\n",
    "\n",
    "        #draw fragments\n",
    "        fragment_mask_on_cpu = fragment_mask.cpu().numpy()\n",
    "        colors_fragment = colors[fragment_mask_on_cpu == 1]\n",
    "        x_fragment = x[fragment_mask_on_cpu == 1]\n",
    "        y_fragment = y[fragment_mask_on_cpu == 1]\n",
    "        z_fragment = z[fragment_mask_on_cpu == 1]\n",
    "        areas_fragment = areas[fragment_mask_on_cpu == 1]\n",
    "        \n",
    "        # Calculate the gradient colors based on phi values\n",
    "        cmap = plt.cm.get_cmap('coolwarm')\n",
    "        norm = plt.Normalize(vmin=min(phi_values_array), vmax=max(phi_values_array))\n",
    "        colors_fragment_shadow = cmap(norm(phi_values_array))\n",
    "        \n",
    "        # ax.scatter(x_fragment, y_fragment, z_fragment, s=areas_fragment, alpha=0.9 * alpha, c=colors_fragment)\n",
    "\n",
    "        ax.scatter(x_fragment, y_fragment, z_fragment, s=areas_fragment, alpha=0.9 * alpha, c=colors_fragment, edgecolors=colors_fragment_shadow, linewidths=5, rasterized=False)\n",
    "\n",
    "        #draw non-fragment atoms\n",
    "        colors = colors[fragment_mask_on_cpu == 0]\n",
    "        x = x[fragment_mask_on_cpu == 0]\n",
    "        y = y[fragment_mask_on_cpu == 0]\n",
    "        z = z[fragment_mask_on_cpu == 0]\n",
    "        areas = areas[fragment_mask_on_cpu == 0]\n",
    "        ax.scatter(x, y, z, s=areas, alpha=0.9 * alpha, c=colors, rasterized=False)\n",
    "\n",
    "\n",
    "def plot_data3d_xai(positions, atom_type, is_geom, camera_elev=0, camera_azim=0, save_path=None, spheres_3d=False,\n",
    "                bg='black', alpha=1., fragment_mask=None, phi_values=None):\n",
    "    black = (0, 0, 0)\n",
    "    white = (1, 1, 1)\n",
    "    hex_bg_color = '#FFFFFF' if bg == 'black' else '#000000' #'#666666'\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = fig.add_subplot(projection='3d')\n",
    "    ax.set_aspect('auto')\n",
    "    ax.view_init(elev=camera_elev, azim=camera_azim)\n",
    "    if bg == 'black':\n",
    "        ax.set_facecolor(black)\n",
    "    else:\n",
    "        ax.set_facecolor(white)\n",
    "    ax.xaxis.pane.set_alpha(0)\n",
    "    ax.yaxis.pane.set_alpha(0)\n",
    "    ax.zaxis.pane.set_alpha(0)\n",
    "    ax._axis3don = False\n",
    "\n",
    "    if bg == 'black':\n",
    "        ax.w_xaxis.line.set_color(\"black\")\n",
    "    else:\n",
    "        ax.w_xaxis.line.set_color(\"white\")\n",
    "\n",
    "    plot_molecule_xai(\n",
    "        ax, positions, atom_type, alpha, spheres_3d, hex_bg_color, is_geom=is_geom, fragment_mask=fragment_mask, phi_values=phi_values\n",
    "    )\n",
    "\n",
    "    max_value = positions.abs().max().item()\n",
    "    axis_lim = min(40, max(max_value / 1.5 + 0.3, 3.2))\n",
    "    ax.set_xlim(-axis_lim, axis_lim)\n",
    "    ax.set_ylim(-axis_lim, axis_lim)\n",
    "    ax.set_zlim(-axis_lim, axis_lim)\n",
    "    dpi = 300 if spheres_3d else 300 #it was 120 and 50\n",
    "\n",
    "    if save_path is not None:\n",
    "        plt.savefig(save_path, bbox_inches='tight', pad_inches=0.0, dpi=dpi)\n",
    "        # plt.savefig(save_path, bbox_inches='tight', pad_inches=0.0, dpi=dpi, transparent=True)\n",
    "\n",
    "        if spheres_3d:\n",
    "            img = imageio.imread(save_path)\n",
    "            img_brighter = np.clip(img * 1.4, 0, 255).astype('uint8')\n",
    "            imageio.imsave(save_path, img_brighter)\n",
    "    else:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def visualize_chain_xai(\n",
    "        path, spheres_3d=False, bg=\"black\", alpha=1.0, wandb=None, mode=\"chain\", is_geom=False, fragment_mask=None, phi_values=None\n",
    "):\n",
    "    files = load_xyz_files(path)\n",
    "    save_paths = []\n",
    "\n",
    "    # Fit PCA to the final molecule – to obtain the best orientation for visualization\n",
    "    positions, one_hot, charges = load_molecule_xyz(files[-1], is_geom=is_geom)\n",
    "    pca = PCA(n_components=3)\n",
    "    pca.fit(positions)\n",
    "\n",
    "    for i in range(len(files)):\n",
    "        file = files[i]\n",
    "\n",
    "        positions, one_hot, charges = load_molecule_xyz(file, is_geom=is_geom)\n",
    "        atom_type = torch.argmax(one_hot, dim=1).numpy()\n",
    "\n",
    "        # Transform positions of each frame according to the best orientation of the last frame\n",
    "        positions = pca.transform(positions)\n",
    "        positions = torch.tensor(positions)\n",
    "\n",
    "        fn = file[:-4] + '.png'\n",
    "        plot_data3d_xai(\n",
    "            positions, atom_type,\n",
    "            save_path=fn,\n",
    "            spheres_3d=spheres_3d,\n",
    "            alpha=alpha,\n",
    "            bg=bg,\n",
    "            camera_elev=90,\n",
    "            camera_azim=90,\n",
    "            is_geom=is_geom,\n",
    "            fragment_mask=fragment_mask,\n",
    "            phi_values=phi_values\n",
    "        )\n",
    "        save_paths.append(fn)\n",
    "\n",
    "    imgs = [imageio.imread(fn) for fn in save_paths]\n",
    "    dirname = os.path.dirname(save_paths[0])\n",
    "    gif_path = dirname + '/output.gif'\n",
    "    imageio.mimsave(gif_path, imgs, subrectangles=True)\n",
    "\n",
    "    if wandb is not None:\n",
    "        wandb.log({mode: [wandb.Video(gif_path, caption=gif_path)]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explainabiliy phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### One sampling step at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #@mastro\n",
    "# num_samples = 5\n",
    "# sampled = 0\n",
    "# #end @mastro\n",
    "# start = 0\n",
    "# bond_order_dict = {0:0, 1:0, 2:0, 3:0}\n",
    "# ATOM_SAMPLER = False\n",
    "# SAVE_VISUALIZATION = True\n",
    "# chain_with_full_fragments = None\n",
    "# M = 100 #number of Monte Carlo Sampling steps\n",
    "# P = 0.2 #probability of atom to exist in random graph (also edge in the future)\n",
    "\n",
    "# # Create the folder if it does not exist\n",
    "# folder_save_path = \"results/explanations\"\n",
    "# if not os.path.exists(folder_save_path):\n",
    "#     os.makedirs(folder_save_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# for data in dataloader:\n",
    "    \n",
    "#     if sampled < num_samples:\n",
    "#         chain_with_full_fragments = None\n",
    "#         sampled += 1\n",
    "#         rng = default_rng(seed = SEED)\n",
    "#         # generate chain with original and full fragments\n",
    "#         print(data[\"positions\"].shape)\n",
    "#         chain_batch, node_mask = model.sample_chain(data, keep_frames=args.keep_frames)\n",
    "\n",
    "#         # import gc\n",
    "\n",
    "#         # # Collect all objects\n",
    "#         # all_objects = gc.get_objects()\n",
    "\n",
    "#         # # Filter out tensors and print their devices\n",
    "#         # for obj in all_objects:\n",
    "#         #     if torch.is_tensor(obj):\n",
    "#         #         if obj.device == torch.device('cuda:0'):\n",
    "#         #             print(f\"Tensor: {obj}, Device: {obj.device}\")\n",
    "\n",
    "        \n",
    "\n",
    "#         # print(torch.cuda.memory_summary(device=0, abbreviated=False)) #@mastro\n",
    "#         # sys.exit() #@mastro\n",
    "        \n",
    "#         #get the generated molecule and store it in a variable\n",
    "#         chain_with_full_fragments = chain_batch[0, 0, :, :] #need to get only the final frame, is 0 ok in the first dimension?\n",
    "        \n",
    "#         # Compute distance of two chains\n",
    "#         mol_similarity = compute_molecular_similarity(chain_with_full_fragments.squeeze(), chain_with_full_fragments.squeeze(), mask1=data[\"linker_mask\"][0].squeeze(), mask2=data[\"linker_mask\"][0].squeeze())\n",
    "#         print(\"Similarity between the two chains:\", mol_similarity.item())\n",
    "#         # compute similarity of one-hot vectors\n",
    "#         positional_similarity = compute_molecular_similarity_positions(chain_with_full_fragments.squeeze(), chain_with_full_fragments.squeeze(), mask1=data[\"linker_mask\"][0].squeeze(), mask2=data[\"linker_mask\"][0].squeeze())\n",
    "#         print(\"Similarity between the two chains based on positions:\", positional_similarity.item())\n",
    "#         one_hot_similarity = compute_one_hot_similarity(chain_with_full_fragments.squeeze(), chain_with_full_fragments.squeeze(), mask1=data[\"linker_mask\"][0].squeeze(), mask2=data[\"linker_mask\"][0].squeeze())\n",
    "#         print(\"Similarity between the two one-hot vectors:\", one_hot_similarity.item())\n",
    "#         # compute cosine similarity\n",
    "#         cos_simil = compute_cosine_similarity(chain_with_full_fragments.squeeze().cpu(), chain_with_full_fragments.squeeze().cpu(), mask1=data[\"linker_mask\"][0].squeeze().cpu(), mask2=data[\"linker_mask\"][0].squeeze().cpu())\n",
    "#         print(\"Cosine similarity between the two chains:\", cos_simil)\n",
    "    \n",
    "        \n",
    "#         # display(data[\"fragment_mask\"])\n",
    "#         # display(data[\"fragment_mask\"].shape)\n",
    "\n",
    "#         # display(data[\"linker_mask\"])\n",
    "#         # display(data[\"linker_mask\"].shape)\n",
    "        \n",
    "#         # display(data[\"edge_mask\"])\n",
    "#         # display(data[\"edge_mask\"].shape)\n",
    "\n",
    "#         #mask out all edges that are not bonds\n",
    "#         # idx2atom = const.GEOM_IDX2ATOM if model.is_geom else const.IDX2ATOM\n",
    "      \n",
    "#         # positions = data[\"positions\"][0].detach().cpu().numpy()\n",
    "#         # x  = positions[:,0]\n",
    "#         # y  = positions[:,1]\n",
    "#         # z  = positions[:,2]\n",
    "#         # # print(x)\n",
    "       \n",
    "#         # atom_type = torch.argmax(data[\"one_hot\"][0], dim=1)\n",
    "#         # print(\"Number of edges\", len(x) * len(x))\n",
    "#         # sys.exit()\n",
    "#         #uncomment to work on edge_mask (not huge effect, tho)\n",
    "#         # for i in range(len(x)):\n",
    "#         #     for j in range(i+1, len(x)):\n",
    "#         #         p1 = np.array([x[i], y[i], z[i]])\n",
    "#         #         p2 = np.array([x[j], y[j], z[j]])\n",
    "#         #         dist =  np.sqrt(np.sum((p1 - p2) ** 2)) #np.linalg.norm(p1-p2)\n",
    "                \n",
    "#         #         atom1, atom2 = idx2atom[atom_type[i].item()], idx2atom[atom_type[j].item()]\n",
    "#         #         bond_order = get_bond_order(atom1, atom2, dist)\n",
    "                \n",
    "#         #         bond_order_dict[bond_order] += 1\n",
    "#         #         # if bond_order <= 0: #TODO debug. Why not all set to 0?\n",
    "#         #         if True:\n",
    "#         #             data[\"edge_mask\"][i * len(x) + j] = 0\n",
    "#         #             data[\"edge_mask\"][j * len(x) + i] = 0\n",
    "#         #         #set all edge_mask indices to 0\n",
    "#         #         data[\"edge_mask\"] = torch.zeros_like(data[\"edge_mask\"])\n",
    "\n",
    "#         #randomly mask out 50% of atoms\n",
    "#         # mask = torch.rand(data[\"atom_mask\"].shape) > 0.5\n",
    "#         # data[\"atom_mask\"] = data[\"atom_mask\"] * mask.to(model.device)\n",
    "#         #mask out all atoms\n",
    "#         # data[\"atom_mask\"] = torch.zeros_like(data[\"atom_mask\"])\n",
    "        \n",
    "#         #variables that will become function/class arguments/variables\n",
    "\n",
    "        \n",
    "#         num_fragment_atoms = torch.sum(data[\"fragment_mask\"] == 1)\n",
    "\n",
    "        \n",
    "#         phi_atoms = {}\n",
    "#         fragment_indices = torch.where(data[\"fragment_mask\"] == 1)[1]\n",
    "#         num_fragment_atoms = len(fragment_indices)\n",
    "#         num_atoms = data[\"positions\"].shape[1]\n",
    "\n",
    "#         distances_random_samples = []\n",
    "#         cosine_similarities_random_samples = []\n",
    "\n",
    "#         for j in tqdm(range(num_fragment_atoms)):\n",
    "            \n",
    "#             marginal_contrib_distance = 0\n",
    "#             marginal_contrib_cosine_similarity = 0\n",
    "#             marginal_contrib_hausdorff = 0\n",
    "\n",
    "#             for step in tqdm(range(M)):\n",
    "#                 data_j_plus = data.copy()\n",
    "#                 data_j_minus = data.copy()\n",
    "#                 data_random = data.copy()\n",
    "\n",
    "#                 N_z_mask = rng.binomial(1, P, size = num_fragment_atoms)\n",
    "\n",
    "#                 # Ensure at least one element is 1, otherwise randomly select one since at least one fragment atom must be present\n",
    "#                 if not np.any(N_z_mask):\n",
    "#                     print(\"Zero elements in N_z_mask, randomly selecting one.\")\n",
    "#                     random_index = rng.integers(0, num_fragment_atoms)\n",
    "#                     N_z_mask[random_index] = 1\n",
    "\n",
    "#                 # print(\"N_z_mask for sample\", sampled, step, N_z_mask)\n",
    "\n",
    "#                 N_mask = torch.ones(num_fragment_atoms, dtype=torch.int)\n",
    "\n",
    "#                 pi = torch.randperm(num_fragment_atoms)\n",
    "\n",
    "#                 N_j_plus_index = torch.ones(num_fragment_atoms, dtype=torch.int)\n",
    "#                 N_j_minus_index = torch.ones(num_fragment_atoms, dtype=torch.int)\n",
    "#                 selected_node_index = np.where(pi == j)[0].item()\n",
    "                \n",
    "#                 # print(\"Selected node index\", selected_node_index)\n",
    "#                 for k in range(num_fragment_atoms):\n",
    "#                     if k <= selected_node_index:\n",
    "#                         N_j_plus_index[pi[k]] = N_mask[pi[k]]\n",
    "#                     else:\n",
    "#                         N_j_plus_index[pi[k]] = N_z_mask[pi[k]]\n",
    "\n",
    "#                 for k in range(num_fragment_atoms):\n",
    "#                     if k < selected_node_index:\n",
    "#                         N_j_minus_index[pi[k]] = N_mask[pi[k]]\n",
    "#                     else:\n",
    "#                         N_j_minus_index[pi[k]] = N_z_mask[pi[k]]\n",
    "\n",
    "\n",
    "#                 # print(\"N_j_plus_index\", N_j_plus_index)\n",
    "#                 # print(\"N_j_minus_index\", N_j_minus_index)\n",
    "#                 # print(N_j_plus_index == N_j_minus_index)\n",
    "                \n",
    "#                 N_j_plus = fragment_indices[N_j_plus_index.bool()] #fragement indices to keep in molecule j plus\n",
    "#                 N_j_minus = fragment_indices[N_j_minus_index.bool()] #fragement indices to keep in molecule j minus\n",
    "\n",
    "#                 N_random_sample = fragment_indices[torch.IntTensor(N_z_mask).bool()] #fragement indices to keep in random molecule\n",
    "#                 # print(\"N_j_plus\", N_j_plus)\n",
    "#                 # print(\"N_j_minus\", N_j_minus)\n",
    "#                 # print(N_j_plus == N_j_minus)\n",
    "#                 atom_mask_j_plus = torch.zeros(num_atoms, dtype=torch.bool)\n",
    "#                 atom_mask_j_minus = torch.zeros(num_atoms, dtype=torch.bool)\n",
    "\n",
    "#                 atom_mask_random_molecule = torch.zeros(num_atoms, dtype=torch.bool)\n",
    "\n",
    "#                 atom_mask_j_plus[N_j_plus] = True\n",
    "#                 #set to true also linker atoms\n",
    "#                 atom_mask_j_plus[data[\"linker_mask\"][0].squeeze().to(torch.int) == 1] = True\n",
    "#                 atom_mask_j_minus[N_j_minus] = True\n",
    "#                 #set to true also linker atoms\n",
    "#                 atom_mask_j_minus[data[\"linker_mask\"][0].squeeze().to(torch.int) == 1] = True\n",
    "\n",
    "#                 atom_mask_random_molecule[N_random_sample] = True\n",
    "#                 #set to true also linker atoms\n",
    "#                 atom_mask_random_molecule[data[\"linker_mask\"][0].squeeze().to(torch.int) == 1] = True\n",
    "\n",
    "#                 # print(\"Atom mask j plus\", atom_mask_j_plus)\n",
    "#                 # print(\"Atom mask j minus\", atom_mask_j_minus)\n",
    "#                 # print(atom_mask_j_minus==atom_mask_j_plus)\n",
    "\n",
    "#                 #for sample containing j\n",
    "#                 #remove positions of atoms in random_indices\n",
    "#                 data_j_plus[\"positions\"] = data_j_plus[\"positions\"][:, atom_mask_j_plus]\n",
    "#                 #remove one_hot of atoms in random_indices\n",
    "#                 data_j_plus[\"one_hot\"] = data_j_plus[\"one_hot\"][:, atom_mask_j_plus]\n",
    "#                 #remove atom_mask of atoms in random_indices\n",
    "#                 data_j_plus[\"atom_mask\"] = data_j_plus[\"atom_mask\"][:, atom_mask_j_plus]\n",
    "#                 #remove fragment_mask of atoms in random_indices\n",
    "#                 data_j_plus[\"fragment_mask\"] =  data_j_plus[\"fragment_mask\"][:, atom_mask_j_plus]\n",
    "#                 #remove linker_mask of atoms in random_indices\n",
    "#                 data_j_plus[\"linker_mask\"] = data_j_plus[\"linker_mask\"][:, atom_mask_j_plus]\n",
    "#                 #remove edge_mask of atoms in random_indices\n",
    "#                 for index in N_j_plus:\n",
    "#                     for i in range(num_atoms):\n",
    "#                         data_j_plus[\"edge_mask\"][index * num_atoms + i] = 0\n",
    "#                         data_j_plus[\"edge_mask\"][i * num_atoms + index] = 0\n",
    "\n",
    "#                 #remove all values in edge_mask that are 0\n",
    "#                 data_j_plus[\"edge_mask\"] = data_j_plus[\"edge_mask\"][data_j_plus[\"edge_mask\"] != 0]  #to be checked, but working on atoms has as effect. For the moment we stick to atoms, then we move to edges (need to edit internal function for this, or redefine everything...)\n",
    "\n",
    "#                 # print(\"After removal j plus:\", data_j_plus[\"positions\"])\n",
    "#                 # print(data_j_plus[\"positions\"].shape)\n",
    "                \n",
    "#                 #for sample not containing j\n",
    "#                 #remove positions of atoms in random_indices\n",
    "#                 data_j_minus[\"positions\"] = data_j_minus[\"positions\"][:, atom_mask_j_minus]\n",
    "#                 #remove one_hot of atoms in random_indices\n",
    "#                 data_j_minus[\"one_hot\"] = data_j_minus[\"one_hot\"][:, atom_mask_j_minus]\n",
    "#                 #remove atom_mask of atoms in random_indices\n",
    "#                 data_j_minus[\"atom_mask\"] = data_j_minus[\"atom_mask\"][:, atom_mask_j_minus]\n",
    "#                 #remove fragment_mask of atoms in random_indices\n",
    "#                 data_j_minus[\"fragment_mask\"] =  data_j_minus[\"fragment_mask\"][:, atom_mask_j_minus]\n",
    "#                 #remove linker_mask of atoms in random_indices\n",
    "#                 data_j_minus[\"linker_mask\"] = data_j_minus[\"linker_mask\"][:, atom_mask_j_minus]\n",
    "#                 #remove edge_mask of atoms in random_indices\n",
    "#                 for index in N_j_minus:\n",
    "#                     for i in range(num_atoms):\n",
    "#                         data_j_minus[\"edge_mask\"][index * num_atoms + i] = 0\n",
    "#                         data_j_minus[\"edge_mask\"][i * num_atoms + index] = 0\n",
    "\n",
    "#                 #remove all values in edge_mask that are 0\n",
    "#                 data_j_minus[\"edge_mask\"] = data_j_minus[\"edge_mask\"][data_j_minus[\"edge_mask\"] != 0]  #to be checked, but working on atoms has as effect. For the moment we stick to atoms, then we move to edges (need to edit internal function for this, or redefine everything...)\n",
    "\n",
    "#                 # print(\"After removal j minus:\", data_j_minus[\"positions\"])\n",
    "#                 # print(data_j_minus[\"positions\"].shape)\n",
    "\n",
    "#                 #for random sample\n",
    "#                 data_random[\"positions\"] = data_random[\"positions\"][:, atom_mask_random_molecule]\n",
    "#                 #remove one_hot of atoms in random_indices\n",
    "#                 data_random[\"one_hot\"] = data_random[\"one_hot\"][:, atom_mask_random_molecule]\n",
    "#                 #remove atom_mask of atoms in random_indices\n",
    "#                 data_random[\"atom_mask\"] = data_random[\"atom_mask\"][:, atom_mask_random_molecule]\n",
    "#                 #remove fragment_mask of atoms in random_indices\n",
    "#                 data_random[\"fragment_mask\"] =  data_random[\"fragment_mask\"][:, atom_mask_random_molecule]\n",
    "#                 #remove linker_mask of atoms in random_indices\n",
    "#                 data_random[\"linker_mask\"] = data_random[\"linker_mask\"][:, atom_mask_random_molecule]\n",
    "#                 #remove edge_mask of atoms in random_indices\n",
    "#                 for index in N_z_mask:\n",
    "#                     for i in range(num_atoms):\n",
    "#                         data_random[\"edge_mask\"][index * num_atoms + i] = 0\n",
    "#                         data_random[\"edge_mask\"][i * num_atoms + index] = 0\n",
    "\n",
    "#                 #remove all values in edge_mask that are 0\n",
    "#                 data_random[\"edge_mask\"] = data_random[\"edge_mask\"][data_random[\"edge_mask\"] != 0] \n",
    "\n",
    "\n",
    "\n",
    "#                 #with node j\n",
    "#                 chain_j_plus, node_mask_j_plus = model.sample_chain(data_j_plus, keep_frames=args.keep_frames)\n",
    "#                 #take only the ts 0 frame\n",
    "#                 chain_j_plus = chain_j_plus[0, 0, :, :]\n",
    "                \n",
    "            \n",
    "#                 V_j_plus_distance = compute_molecular_distance(chain_with_full_fragments.squeeze(), chain_j_plus.squeeze(), mask1=data[\"linker_mask\"][0].squeeze(), mask2=data_j_plus[\"linker_mask\"][0].squeeze())\n",
    "\n",
    "#                 V_j_plus_cosine_similarity = compute_cosine_similarity(chain_with_full_fragments.squeeze().cpu(), chain_j_plus.squeeze().cpu(), mask1=data[\"linker_mask\"][0].squeeze().cpu(), mask2=data_j_plus[\"linker_mask\"][0].squeeze().cpu())\n",
    "\n",
    "#                 # print(\"V_j_plus\", V_j_plus)\n",
    "\n",
    "#                 #without node j\n",
    "#                 chain_j_minus, node_mask_j_minus = model.sample_chain(data_j_minus, keep_frames=args.keep_frames)\n",
    "\n",
    "#                 #take only the ts 0 frame\n",
    "#                 chain_j_minus = chain_j_minus[0, 0, :, :]\n",
    "\n",
    "#                 V_j_minus_distance = compute_molecular_distance(chain_with_full_fragments.squeeze(), chain_j_minus.squeeze(), mask1=data[\"linker_mask\"][0].squeeze(), mask2=data_j_minus[\"linker_mask\"][0].squeeze())\n",
    "\n",
    "#                 V_j_minus_cosine_similarity = compute_cosine_similarity(chain_with_full_fragments.squeeze().cpu(), chain_j_minus.squeeze().cpu(), mask1=data[\"linker_mask\"][0].squeeze().cpu(), mask2=data_j_minus[\"linker_mask\"][0].squeeze().cpu())\n",
    "\n",
    "#                 #with random sample\n",
    "#                 chain_random, node_mask_random = model.sample_chain(data_random, keep_frames=args.keep_frames)\n",
    "\n",
    "#                 chain_random = chain_random[0, 0, :, :]\n",
    "\n",
    "#                 V_random_distance = compute_molecular_distance(chain_with_full_fragments.squeeze(), chain_random.squeeze(), mask1=data[\"linker_mask\"][0].squeeze(), mask2=data_random[\"linker_mask\"][0].squeeze())\n",
    "\n",
    "#                 V_random_cosine_similarity = compute_cosine_similarity(chain_with_full_fragments.squeeze().cpu(), chain_random.squeeze().cpu(), mask1=data[\"linker_mask\"][0].squeeze().cpu(), mask2=data_random[\"linker_mask\"][0].squeeze().cpu())\n",
    "\n",
    "#                 distances_random_samples.append(V_random_distance)\n",
    "#                 cosine_similarities_random_samples.append(V_random_cosine_similarity)\n",
    "\n",
    "#                 # print(V_random_distance, V_random_cosine_similarity)\n",
    "                \n",
    "#                 marginal_contrib_distance += (V_j_plus_distance - V_j_minus_distance)\n",
    "\n",
    "#                 marginal_contrib_cosine_similarity += (V_j_plus_cosine_similarity - V_j_minus_cosine_similarity)\n",
    "\n",
    "#                 # marginal_contrib_hausdorff += (V_j_plus_hausdorff - V_j_minus_hausdorff)\n",
    "\n",
    "#             phi_atoms[fragment_indices[j].item()] = [0,0] #,0]    \n",
    "#             phi_atoms[fragment_indices[j].item()][0] = marginal_contrib_distance/M #j is the index of the fragment atom in the fragment indices tensor\n",
    "#             phi_atoms[fragment_indices[j].item()][1] = marginal_contrib_cosine_similarity/M\n",
    "#             # phi_atoms[fragment_indices[j]][2] = marginal_contrib_hausdorff/M\n",
    "\n",
    "#             print(data[\"name\"])\n",
    "\n",
    "#         phi_atoms_distances = {}\n",
    "#         phi_atoms_cosine_similarity = {}\n",
    "#         for atom_index, phi_values in phi_atoms.items():\n",
    "#             phi_atoms_distances[atom_index] = phi_values[0]\n",
    "#             phi_atoms_cosine_similarity[atom_index] = phi_values[1]\n",
    "\n",
    "#         if SAVE_VISUALIZATION:\n",
    "#             for i in range(len(data['positions'])):\n",
    "#                 chain = chain_batch[:, i, :, :]\n",
    "#                 assert chain.shape[0] == args.keep_frames\n",
    "#                 assert chain.shape[1] == data['positions'].shape[1]\n",
    "#                 assert chain.shape[2] == data['positions'].shape[2] + data['one_hot'].shape[2] + model.include_charges\n",
    "\n",
    "#                 # Saving chains\n",
    "#                 name = str(i + start)\n",
    "#                 chain_output = os.path.join(chains_output_dir, name)\n",
    "#                 os.makedirs(chain_output, exist_ok=True)\n",
    "\n",
    "#                 one_hot = chain[:, :, 3:-1]\n",
    "#                 positions = chain[:, :, :3]\n",
    "#                 chain_node_mask = torch.cat([node_mask[i].unsqueeze(0) for _ in range(args.keep_frames)], dim=0)\n",
    "#                 names = [f'{name}_{j}' for j in range(args.keep_frames)]\n",
    "\n",
    "#                 save_xyz_file(chain_output, one_hot, positions, chain_node_mask, names=names, is_geom=model.is_geom)\n",
    "#                 visualize_chain_xai(\n",
    "#                     chain_output,\n",
    "#                     spheres_3d=False,\n",
    "#                     alpha=0.7,\n",
    "#                     bg='white',\n",
    "#                     is_geom=model.is_geom,\n",
    "#                     fragment_mask=data['fragment_mask'][i].squeeze(),\n",
    "#                     phi_values=phi_atoms_distances\n",
    "#                 )\n",
    "\n",
    "#                 # Saving final prediction and ground truth separately\n",
    "#                 true_one_hot = data['one_hot'][i].unsqueeze(0)\n",
    "#                 true_positions = data['positions'][i].unsqueeze(0)\n",
    "#                 true_node_mask = data['atom_mask'][i].unsqueeze(0)\n",
    "#                 save_xyz_file(\n",
    "#                     final_states_output_dir,\n",
    "#                     true_one_hot,\n",
    "#                     true_positions,\n",
    "#                     true_node_mask,\n",
    "#                     names=[f'{name}_true'],\n",
    "#                     is_geom=model.is_geom,\n",
    "#                 )\n",
    "\n",
    "#                 pred_one_hot = chain[0, :, 3:-1].unsqueeze(0)\n",
    "#                 pred_positions = chain[0, :, :3].unsqueeze(0)\n",
    "#                 pred_node_mask = chain_node_mask[0].unsqueeze(0)\n",
    "#                 save_xyz_file(\n",
    "#                     final_states_output_dir,\n",
    "#                     pred_one_hot,\n",
    "#                     pred_positions,\n",
    "#                     pred_node_mask,\n",
    "#                     names=[f'{name}_pred'],\n",
    "#                     is_geom=model.is_geom\n",
    "#                 )\n",
    "\n",
    "#             start += len(data['positions'])\n",
    "\n",
    "#         # Save phi_atoms to a text file\n",
    "#         with open(f'{folder_save_path}/phi_atoms_{sampled}.txt', 'w') as write_file:\n",
    "#             write_file.write(\"sample name: \" + str(data[\"name\"]) + \"\\n\")\n",
    "#             write_file.write(\"atom_index,distance,cosine_similarity\\n\")\n",
    "#             for atom_index, phi_values in phi_atoms.items():\n",
    "#                 write_file.write(f\"{atom_index},{phi_values[0]},{phi_values[1]}\\n\")\n",
    "\n",
    "#             write_file.write(\"\\n\")\n",
    "#             #save sum of phi values for disance and cosine similarity\n",
    "#             write_file.write(\"Sum of phi values for distance\\n\")\n",
    "#             write_file.write(str(sum([p_values[0] for p_values in phi_atoms.values()])) + \"\\n\")\n",
    "#             write_file.write(\"Sum of phi values for cosine similarity\\n\")\n",
    "#             write_file.write(str(sum([p_values[1] for p_values in phi_atoms.values()])) + \"\\n\")     \n",
    "#             write_file.write(\"Distance random samples\\n\")\n",
    "#             write_file.write(str(distances_random_samples) + \"\\n\")\n",
    "#             write_file.write(\"Cosine similarity random samples\\n\")\n",
    "#             write_file.write(str(cosine_similarities_random_samples) + \"\\n\")\n",
    "      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Multiple sampling steps at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70cfd4147e404eed8413893bb288a71d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph density: 0.08615384615384615\n",
      "Node density: 0.896551724137931\n",
      "Node-edge ratio: 0.9285714285714286\n",
      "Edge-node ratio: 1.0769230769230769\n",
      "Using P: graph_density 0.08615384615384615\n",
      "Similarity between the two chains: 1.0\n",
      "Molecular distance using batches:  tensor([0.], device='cuda:0')\n",
      "Similarity between the two chains based on positions: 1.0\n",
      "Similarity between the two one-hot vectors: 1.0\n",
      "Cosine similarity between the two chains: 1.0000001192092896\n",
      "Cosine similarity between the two chains using batches: [1.0000001192092896]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4f565896aaf490e92137dc1d6d52551",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82828e7626a64c85bf5cea4ea38193d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random index 6\n",
      "j 0\n",
      "Random index 16\n",
      "j 0\n",
      "Random index 8\n",
      "j 0\n",
      "Random index 16\n",
      "j 0\n",
      "Random index 19\n",
      "j 0\n",
      "Random index 5\n",
      "j 0\n",
      "Random index 13\n",
      "j 0\n",
      "Random index 3\n",
      "j 0\n",
      "Random index 2\n",
      "j 0\n",
      "Random index 7\n",
      "j 0\n",
      "Random index 17\n",
      "j 0\n",
      "Random index 13\n",
      "j 0\n",
      "Random index 9\n",
      "j 0\n",
      "Random index 5\n",
      "j 0\n",
      "Random index 2\n",
      "j 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 476\u001b[0m\n\u001b[0;32m    469\u001b[0m data_random_batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medge_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([data_random_dict[i][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medge_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(PARALLEL_STEPS)], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    471\u001b[0m \u001b[38;5;66;03m# end_time = time.time()\u001b[39;00m\n\u001b[0;32m    472\u001b[0m \u001b[38;5;66;03m# print(\"Time to create batches for j plus, j minus and random molecule in seconds:\", end_time - start_time)\u001b[39;00m\n\u001b[0;32m    473\u001b[0m \n\u001b[0;32m    474\u001b[0m \u001b[38;5;66;03m# start_time = time.time()\u001b[39;00m\n\u001b[1;32m--> 476\u001b[0m chain_j_plus_batch, node_mask_j_plus_batch \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_j_plus_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_frames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeep_frames\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    478\u001b[0m chain_j_plus \u001b[38;5;241m=\u001b[39m chain_j_plus_batch[\u001b[38;5;241m0\u001b[39m, :, :, :] \u001b[38;5;66;03m#it should take the first frame and all batch elements -> check it is really the first frame (I need the one at t0, the final generated molecule)\u001b[39;00m\n\u001b[0;32m    480\u001b[0m chain_j_minus_batch, node_mask_j_minus_batch \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39msample_chain(data_j_minus_batch, keep_frames\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mkeep_frames)\n",
      "File \u001b[1;32mc:\\Repositories\\DiffLinker\\src\\lightning.py:456\u001b[0m, in \u001b[0;36mDDPM.sample_chain\u001b[1;34m(self, data, sample_fn, keep_frames)\u001b[0m\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcenter_of_mass)\n\u001b[0;32m    454\u001b[0m x \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mremove_partial_mean_with_mask(x, node_mask, center_of_mass_mask)\n\u001b[1;32m--> 456\u001b[0m chain \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_chain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[43m    \u001b[49m\u001b[43mh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnode_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnode_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m    \u001b[49m\u001b[43medge_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfragment_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfragment_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    462\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlinker_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlinker_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    463\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    464\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_frames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    465\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m chain, node_mask\n",
      "File \u001b[1;32mc:\\Users\\Mastro\\anaconda3\\envs\\diff_explainer\\lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Repositories\\DiffLinker\\src\\edm.py:152\u001b[0m, in \u001b[0;36mEDM.sample_chain\u001b[1;34m(self, x, h, node_mask, fragment_mask, linker_mask, edge_mask, context, keep_frames)\u001b[0m\n\u001b[0;32m    149\u001b[0m s_array \u001b[38;5;241m=\u001b[39m s_array \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m    150\u001b[0m t_array \u001b[38;5;241m=\u001b[39m t_array \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m--> 152\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_p_zs_given_zt_only_linker\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[43m    \u001b[49m\u001b[43ms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ms_array\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt_array\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mz_t\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnode_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnode_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfragment_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfragment_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlinker_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlinker_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[43medge_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    162\u001b[0m write_index \u001b[38;5;241m=\u001b[39m (s \u001b[38;5;241m*\u001b[39m keep_frames) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m    163\u001b[0m chain[write_index] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnormalize_z(z)\n",
      "File \u001b[1;32mc:\\Repositories\\DiffLinker\\src\\edm.py:188\u001b[0m, in \u001b[0;36mEDM.sample_p_zs_given_zt_only_linker\u001b[1;34m(self, s, t, z_t, node_mask, fragment_mask, linker_mask, edge_mask, context)\u001b[0m\n\u001b[0;32m    185\u001b[0m sigma_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigma(gamma_t, target_tensor\u001b[38;5;241m=\u001b[39mz_t)\n\u001b[0;32m    187\u001b[0m \u001b[38;5;66;03m# Neural net prediction.\u001b[39;00m\n\u001b[1;32m--> 188\u001b[0m eps_hat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdynamics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mz_t\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnode_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnode_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlinker_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlinker_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[43medge_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    196\u001b[0m eps_hat \u001b[38;5;241m=\u001b[39m eps_hat \u001b[38;5;241m*\u001b[39m linker_mask\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# Compute mu for p(z_s | z_t)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Repositories\\DiffLinker\\src\\egnn.py:449\u001b[0m, in \u001b[0;36mDynamics.forward\u001b[1;34m(self, t, xh, node_mask, linker_mask, edge_mask, context)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;66;03m# Forward EGNN\u001b[39;00m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;66;03m# Output: h_final (B*N, nf), x_final (B*N, 3), vel (B*N, 3)\u001b[39;00m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124megnn_dynamics\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 449\u001b[0m     h_final, x_final \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdynamics\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    450\u001b[0m \u001b[43m        \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    451\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43medges\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnode_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnode_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlinker_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlinker_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43medge_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_mask\u001b[49m\n\u001b[0;32m    456\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    457\u001b[0m     node_mask \u001b[38;5;241m=\u001b[39m node_mask\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mrunning_device) \u001b[38;5;66;03m#@mastro moved node_mask to device\u001b[39;00m\n\u001b[0;32m    458\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mrunning_device) \u001b[38;5;66;03m#@mastro moved x to device \u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Mastro\\anaconda3\\envs\\diff_explainer\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Mastro\\anaconda3\\envs\\diff_explainer\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Repositories\\DiffLinker\\src\\egnn.py:252\u001b[0m, in \u001b[0;36mEGNN.forward\u001b[1;34m(self, h, x, edge_index, node_mask, linker_mask, edge_mask)\u001b[0m\n\u001b[0;32m    249\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(h) \u001b[38;5;66;03m#@mastro moved h to device of embedding (hopefully the selected GPU)\u001b[39;00m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_layers):\n\u001b[1;32m--> 252\u001b[0m     h, x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_modules\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43me_block_\u001b[39;49m\u001b[38;5;132;43;01m%d\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnode_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnode_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlinker_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlinker_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[43medge_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdistances\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;66;03m# Important, the bias of the last linear might be non-zero\u001b[39;00m\n\u001b[0;32m    261\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_out(h) \u001b[38;5;66;03m#@mastro moved h to device of embedding (hopefully the selected GPU)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Mastro\\anaconda3\\envs\\diff_explainer\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Mastro\\anaconda3\\envs\\diff_explainer\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Repositories\\DiffLinker\\src\\egnn.py:185\u001b[0m, in \u001b[0;36mEquivariantBlock.forward\u001b[1;34m(self, h, x, edge_index, node_mask, linker_mask, edge_mask, edge_attr)\u001b[0m\n\u001b[0;32m    183\u001b[0m edge_attr \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([distances, edge_attr], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_layers):\n\u001b[1;32m--> 185\u001b[0m     h, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_modules\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgcl_\u001b[39;49m\u001b[38;5;132;43;01m%d\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_attr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnode_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    186\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgcl_equiv\u001b[39m\u001b[38;5;124m\"\u001b[39m](\n\u001b[0;32m    187\u001b[0m     h, x,\n\u001b[0;32m    188\u001b[0m     edge_index\u001b[38;5;241m=\u001b[39medge_index,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    193\u001b[0m     edge_mask\u001b[38;5;241m=\u001b[39medge_mask,\n\u001b[0;32m    194\u001b[0m )\n\u001b[0;32m    196\u001b[0m \u001b[38;5;66;03m# Important, the bias of the last linear might be non-zero\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Mastro\\anaconda3\\envs\\diff_explainer\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Mastro\\anaconda3\\envs\\diff_explainer\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Repositories\\DiffLinker\\src\\egnn.py:87\u001b[0m, in \u001b[0;36mGCL.forward\u001b[1;34m(self, h, edge_index, edge_attr, node_attr, node_mask, edge_mask)\u001b[0m\n\u001b[0;32m     85\u001b[0m col \u001b[38;5;241m=\u001b[39m col\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mrunning_device)\n\u001b[0;32m     86\u001b[0m edge_feat, mij \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39medge_model(h[row], h[col], edge_attr, edge_mask)\n\u001b[1;32m---> 87\u001b[0m h, agg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_feat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_attr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m node_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     89\u001b[0m     node_mask \u001b[38;5;241m=\u001b[39m node_mask\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mrunning_device) \u001b[38;5;66;03m#@mastro moved node_mask to device (hopefully the selected GPU)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Repositories\\DiffLinker\\src\\egnn.py:71\u001b[0m, in \u001b[0;36mGCL.node_model\u001b[1;34m(self, x, edge_index, edge_attr, node_attr)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnode_model\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, edge_index, edge_attr, node_attr):\n\u001b[0;32m     70\u001b[0m     row, col \u001b[38;5;241m=\u001b[39m edge_index\n\u001b[1;32m---> 71\u001b[0m     agg \u001b[38;5;241m=\u001b[39m \u001b[43munsorted_segment_sum\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_attr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_segments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mnormalization_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalization_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m                               \u001b[49m\u001b[43maggregation_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maggregation_method\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m node_attr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     75\u001b[0m         agg \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([x, agg, node_attr], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Repositories\\DiffLinker\\src\\egnn.py:347\u001b[0m, in \u001b[0;36munsorted_segment_sum\u001b[1;34m(data, segment_ids, num_segments, normalization_factor, aggregation_method)\u001b[0m\n\u001b[0;32m    345\u001b[0m segment_ids \u001b[38;5;241m=\u001b[39m segment_ids\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mrunning_device) \u001b[38;5;66;03m#@mastro moved segment_ids to device (hopefully the selected GPU)\u001b[39;00m\n\u001b[0;32m    346\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mrunning_device) \u001b[38;5;66;03m#@mastro moved data to device (hopefully the selected GPU)\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscatter_add_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msegment_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m aggregation_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    349\u001b[0m     result \u001b[38;5;241m=\u001b[39m result \u001b[38;5;241m/\u001b[39m normalization_factor\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#@mastro\n",
    "torch.set_printoptions(threshold=float('inf'))\n",
    "\n",
    "num_samples = 30\n",
    "sampled = 0\n",
    "#end @mastro\n",
    "start = 0\n",
    "\n",
    "SAVE_VISUALIZATION = True\n",
    "chain_with_full_fragments = None\n",
    "M = 100 #100 #number of Monte Carlo Sampling steps\n",
    "P = None #probability of atom to exist in random graph (also edge in the future)\n",
    "PARALLEL_STEPS = 100\n",
    "# Create the folder if it does not exist\n",
    "folder_save_path = \"results/explanations_\" + args.P\n",
    "if not os.path.exists(folder_save_path):\n",
    "    os.makedirs(folder_save_path)\n",
    "\n",
    "data_list = []\n",
    "for data in dataloader:\n",
    "\n",
    "    if sampled < num_samples:\n",
    "        data_list.append(data)\n",
    "        sampled += 1\n",
    "\n",
    "for data_index, data in enumerate(tqdm(data_list)): #7:\n",
    "\n",
    "        # start_time = time.time()\n",
    "\n",
    "        smile = data[\"name\"][0]\n",
    "        \n",
    "        mol = read_smiles(smile)\n",
    "        num_nodes = mol.number_of_nodes()\n",
    "        \n",
    "        num_edges = mol.number_of_edges()\n",
    "        num_edges_directed = num_edges*2\n",
    "        \n",
    "        \n",
    "        graph_density = num_edges_directed/(num_nodes*(num_nodes-1))\n",
    "        max_number_of_nodes = num_edges + 1\n",
    "\n",
    "        node_density = num_nodes/max_number_of_nodes\n",
    "\n",
    "        node_edge_ratio = num_nodes/num_edges\n",
    "        \n",
    "        edge_node_ratio = num_edges/num_nodes\n",
    "        print(\"Graph density:\", graph_density)\n",
    "        print(\"Node density:\", node_density)\n",
    "        print(\"Node-edge ratio:\", node_edge_ratio)\n",
    "        print(\"Edge-node ratio:\", edge_node_ratio)\n",
    "        \n",
    "        if args.P == \"graph_density\":\n",
    "            P = graph_density #probability of atom to exist in random graph (not sure if correct approach, this was correct for edges)\n",
    "        elif args.P == \"node_density\":\n",
    "            P = node_density\n",
    "        elif args.P == \"node_edge_ratio\" or args.P == \"edge_node_ratio\":\n",
    "            if node_edge_ratio < edge_node_ratio:\n",
    "                P = node_edge_ratio\n",
    "                print(\"Using node-edge ratio\", node_edge_ratio)\n",
    "            else:\n",
    "                P = edge_node_ratio\n",
    "                print(\"Using edge-node ratio\", edge_node_ratio)            \n",
    "        else:\n",
    "            P = 0.2\n",
    "\n",
    "        print(\"Using P:\", args.P, P)\n",
    "\n",
    "        chain_with_full_fragments = None\n",
    "       \n",
    "        rng = default_rng(seed = SEED)\n",
    "        rng_torch = torch.Generator(device=\"cpu\")\n",
    "        rng_torch.manual_seed(SEED)\n",
    "        # generate chain with original and full fragments\n",
    "       \n",
    "        chain_batch, node_mask = model.sample_chain(data, keep_frames=args.keep_frames)\n",
    "        \n",
    "        #get the generated molecule and store it in a variable\n",
    "        chain_with_full_fragments = chain_batch[0, :, :, :] #need to get only the final frame, is 0 ok in the first dimension?\n",
    "        \n",
    "        # Compute distance of two chains\n",
    "        mol_similarity = compute_molecular_similarity(chain_with_full_fragments.squeeze(), chain_with_full_fragments.squeeze(), mask1=data[\"linker_mask\"][0].squeeze(), mask2=data[\"linker_mask\"][0].squeeze())\n",
    "        print(\"Similarity between the two chains:\", mol_similarity.item())\n",
    "        #compute molecular distance using batches\n",
    "        original_linker_mask_batch = data[\"linker_mask\"][0].squeeze().repeat(PARALLEL_STEPS, 1) #check why it works\n",
    "        \n",
    "        mol_distance = compute_molecular_distance_batch(chain_with_full_fragments, chain_with_full_fragments, mask1=original_linker_mask_batch, mask2=original_linker_mask_batch)\n",
    "        print(\"Molecular distance using batches: \", mol_distance)\n",
    "        \n",
    "        # compute similarity of one-hot vectors\n",
    "        positional_similarity = compute_molecular_similarity_positions(chain_with_full_fragments.squeeze(), chain_with_full_fragments.squeeze(), mask1=data[\"linker_mask\"][0].squeeze(), mask2=data[\"linker_mask\"][0].squeeze())\n",
    "        print(\"Similarity between the two chains based on positions:\", positional_similarity.item())\n",
    "        one_hot_similarity = compute_one_hot_similarity(chain_with_full_fragments.squeeze(), chain_with_full_fragments.squeeze(), mask1=data[\"linker_mask\"][0].squeeze(), mask2=data[\"linker_mask\"][0].squeeze())\n",
    "        print(\"Similarity between the two one-hot vectors:\", one_hot_similarity.item())\n",
    "        # compute cosine similarity\n",
    "        cos_simil = compute_cosine_similarity(chain_with_full_fragments.squeeze().cpu(), chain_with_full_fragments.squeeze().cpu(), mask1=data[\"linker_mask\"][0].squeeze().cpu(), mask2=data[\"linker_mask\"][0].squeeze().cpu())\n",
    "        print(\"Cosine similarity between the two chains:\", cos_simil)\n",
    "        cos_simil_batch = compute_cosine_similarity_batch(chain_with_full_fragments.cpu(), chain_with_full_fragments.cpu(), mask1=original_linker_mask_batch.cpu(), mask2=original_linker_mask_batch.cpu())\n",
    "        print(\"Cosine similarity between the two chains using batches:\", cos_simil_batch)\n",
    "       \n",
    "        num_fragment_atoms = torch.sum(data[\"fragment_mask\"] == 1)\n",
    "\n",
    "        phi_atoms = {}\n",
    "        \n",
    "        num_atoms = data[\"positions\"].shape[1]\n",
    "        num_linker_atoms = torch.sum(data[\"linker_mask\"] == 1)\n",
    "        \n",
    "        distances_random_samples = []\n",
    "        cosine_similarities_random_samples = []\n",
    "\n",
    "        # end_time = time.time()\n",
    "        # print(\"Time to compute similarities in seconds:\", end_time - start_time)\n",
    "\n",
    "\n",
    "        for j in tqdm(range(num_fragment_atoms)): \n",
    "            \n",
    "            marginal_contrib_distance = 0\n",
    "            marginal_contrib_cosine_similarity = 0\n",
    "            marginal_contrib_hausdorff = 0\n",
    "\n",
    "            for step in tqdm(range(int(M/PARALLEL_STEPS))):\n",
    "\n",
    "                # start_time = time.time()\n",
    "\n",
    "                fragment_indices = torch.where(data[\"fragment_mask\"] == 1)[1]\n",
    "                num_fragment_atoms = len(fragment_indices)\n",
    "                fragment_indices = fragment_indices.repeat(PARALLEL_STEPS).to(args.device)\n",
    "\n",
    "                data_j_plus = data.copy()\n",
    "                data_j_minus = data.copy()\n",
    "                data_random = data.copy()\n",
    "\n",
    "                N_z_mask = torch.tensor(np.array([rng.binomial(1, P, size = num_fragment_atoms) for _ in range(PARALLEL_STEPS)]), dtype=torch.int32)\n",
    "                # Ensure at least one element is 1, otherwise randomly select one since at least one fragment atom must be present\n",
    "                # print(N_z_mask)\n",
    "                \n",
    "                for i in range(len(N_z_mask)):\n",
    "\n",
    "                    #set the current explained atom to 0 in N_z_mask\n",
    "                    N_z_mask[i][j] = 0 #so it is always one when taken from the oriignal sample and 0 when taken from the random sample\n",
    "\n",
    "                    if not N_z_mask[i].any():\n",
    "                        \n",
    "                        # print(\"Zero elements in mask, randomly selecting one.\")\n",
    "                        random_index = j #j is the current explained atom, it should always be set to 0\n",
    "                        while random_index == j:\n",
    "                            random_index = rng.integers(0, num_fragment_atoms)\n",
    "                        N_z_mask[i][random_index] = 1\n",
    "                        print(\"Random index\", random_index)\n",
    "                        print(\"j\", j)\n",
    "                       \n",
    "                    \n",
    "\n",
    "                N_z_mask=N_z_mask.flatten().to(args.device)\n",
    "                \n",
    "                \n",
    "                # print(\"N_z_mask for sample\", sampled, step, N_z_mask)\n",
    "\n",
    "                N_mask = torch.ones(PARALLEL_STEPS * num_fragment_atoms, dtype=torch.int32, device=args.device)\n",
    "\n",
    "                # end_time = time.time()\n",
    "\n",
    "                # print(\"Time to generate N_z_mask and N_mask in seconds:\", end_time - start_time)\n",
    "\n",
    "                # start_time = time.time()\n",
    "\n",
    "                pi = torch.cat([torch.randperm(num_fragment_atoms, generator=rng_torch) for _ in range(PARALLEL_STEPS)], dim=0)\n",
    "\n",
    "                N_j_plus_index = torch.ones(PARALLEL_STEPS*num_fragment_atoms, dtype=torch.int, device=args.device)\n",
    "                N_j_minus_index = torch.ones(PARALLEL_STEPS*num_fragment_atoms, dtype=torch.int, device=args.device)\n",
    "\n",
    "                selected_node_index = np.where(pi == j)\n",
    "                selected_node_index = torch.tensor(np.array(selected_node_index), device=args.device).squeeze()\n",
    "                selected_node_index = selected_node_index.repeat_interleave(num_fragment_atoms) #@mastro TO BE CHECKED IF THIS IS CORRECT\n",
    "                # print(\"Selected node index\", selected_node_index)\n",
    "                k_values = torch.arange(num_fragment_atoms*PARALLEL_STEPS, device=args.device)\n",
    "\n",
    "                add_to_pi = torch.arange(start=0, end=PARALLEL_STEPS*num_fragment_atoms, step=num_fragment_atoms).repeat_interleave(num_fragment_atoms) #check if it is correct ot consider num_fragment_atoms and not num_atoms\n",
    "\n",
    "                pi_add = pi + add_to_pi\n",
    "                pi_add = pi_add.to(device=args.device)\n",
    "                #this must be cafeully checked. this should be adapted for nodes\n",
    "                add_to_node_index = torch.arange(start=0, end=PARALLEL_STEPS*num_atoms, step=num_atoms) #@mastro change step from num_fragment_atoms to num_atoms\n",
    "                \n",
    "                add_to_node_index = add_to_node_index.repeat_interleave(num_fragment_atoms).to(args.device) #changed from num_atoms to num_fragment_atoms\n",
    "\n",
    "                \n",
    "                N_j_plus_index[pi_add] = torch.where(k_values <= selected_node_index, N_mask[pi_add], N_z_mask[pi_add])\n",
    "                N_j_minus_index[pi_add] = torch.where(k_values < selected_node_index, N_mask[pi_add], N_z_mask[pi_add]) \n",
    "\n",
    "                #fragements to keep in molecule j plus\n",
    "                fragment_indices = fragment_indices + add_to_node_index\n",
    "                \n",
    "                \n",
    "                N_j_plus = fragment_indices[(N_j_plus_index==1)] #fragement to keep in molecule j plus\n",
    "                #fragement indices to keep in molecule j minus\n",
    "               \n",
    "                N_j_minus = fragment_indices[(N_j_minus_index==1)] #it is ok. it contains fragmens indices to keep in molecule j minus (indices that index the atom nodes)\n",
    "\n",
    "                #fragement indices to keep in random molecule\n",
    "                N_random_sample = fragment_indices[(N_z_mask==1)] \n",
    "                \n",
    "\n",
    "                atom_mask_j_plus = torch.zeros(num_atoms*PARALLEL_STEPS, dtype=torch.bool)\n",
    "                atom_mask_j_minus = torch.zeros(num_atoms*PARALLEL_STEPS, dtype=torch.bool)\n",
    "                atom_mask_random_molecule = torch.zeros(num_atoms*PARALLEL_STEPS, dtype=torch.bool)\n",
    "\n",
    "                atom_mask_j_plus[N_j_plus] = True\n",
    "                \n",
    "                atom_mask_j_minus[N_j_minus] = True\n",
    "\n",
    "                #set to true also linker atoms\n",
    "                parallelized_linker_mask = data[\"linker_mask\"][0].squeeze().to(torch.int).repeat(PARALLEL_STEPS)\n",
    "                atom_mask_j_plus[(parallelized_linker_mask == 1)] = True \n",
    "\n",
    "                #set to true also linker atoms\n",
    "                atom_mask_j_minus[(parallelized_linker_mask == 1)] = True \n",
    "\n",
    "                atom_mask_random_molecule[N_random_sample] = True\n",
    "                #set to true also linker atoms\n",
    "                atom_mask_random_molecule[(parallelized_linker_mask == 1)] = True\n",
    "                \n",
    "               \n",
    "                atom_mask_j_plus = atom_mask_j_plus.view(PARALLEL_STEPS, num_atoms)\n",
    "                atom_mask_j_minus = atom_mask_j_minus.view(PARALLEL_STEPS, num_atoms)\n",
    "                atom_mask_random_molecule = atom_mask_random_molecule.view(PARALLEL_STEPS, num_atoms)\n",
    "                \n",
    "                # end_time = time.time()\n",
    "                # print(\"Time to generate atom masks in seconds:\", end_time - start_time)\n",
    "\n",
    "                data_j_plus_dict = {}\n",
    "                data_j_minus_dict = {}\n",
    "                data_random_dict = {}\n",
    "\n",
    "                # start_time = time.time()\n",
    "                for i in range(PARALLEL_STEPS):\n",
    "                    data_j_plus_dict[i] = data.copy()\n",
    "                    data_j_minus_dict[i] = data.copy()\n",
    "                    data_random_dict[i] = data.copy()\n",
    "\n",
    "                    #data j plus\n",
    "                    data_j_plus_dict[i][\"positions\"] = data_j_plus_dict[i][\"positions\"][:, atom_mask_j_plus[i]]\n",
    "                    data_j_plus_dict[i][\"num_atoms\"] = data_j_plus_dict[i][\"positions\"].shape[1]\n",
    "                    # remove one_hot of atoms in random_indices\n",
    "                    data_j_plus_dict[i][\"one_hot\"] = data_j_plus_dict[i][\"one_hot\"][:, atom_mask_j_plus[i]]\n",
    "                    # remove atom_mask of atoms in random_indices\n",
    "                    data_j_plus_dict[i][\"atom_mask\"] = data_j_plus_dict[i][\"atom_mask\"][:, atom_mask_j_plus[i]]\n",
    "                    # remove fragment_mask of atoms in random_indices\n",
    "                    data_j_plus_dict[i][\"fragment_mask\"] = data_j_plus_dict[i][\"fragment_mask\"][:, atom_mask_j_plus[i]]\n",
    "                    # remove linker_mask of atoms in random_indices\n",
    "                    data_j_plus_dict[i][\"linker_mask\"] = data_j_plus_dict[i][\"linker_mask\"][:, atom_mask_j_plus[i]]\n",
    "                    data_j_plus_dict[i][\"charges\"] = data_j_plus_dict[i][\"charges\"][:, atom_mask_j_plus[i]]\n",
    "                    data_j_plus_dict[i][\"anchors\"] = data_j_plus_dict[i][\"anchors\"][:, atom_mask_j_plus[i]]\n",
    "                    edge_mask_to_keep = (atom_mask_j_plus[i].unsqueeze(1) * atom_mask_j_plus[i]).flatten()\n",
    "                    data_j_plus_dict[i][\"edge_mask\"] = data_j_plus_dict[i][\"edge_mask\"][edge_mask_to_keep]\n",
    "\n",
    "                    #data j minus\n",
    "                    data_j_minus_dict[i][\"positions\"] = data_j_minus_dict[i][\"positions\"][:, atom_mask_j_minus[i]]\n",
    "                    data_j_minus_dict[i][\"num_atoms\"] = data_j_minus_dict[i][\"positions\"].shape[1]\n",
    "                    # remove one_hot of atoms in random_indices\n",
    "                    data_j_minus_dict[i][\"one_hot\"] = data_j_minus_dict[i][\"one_hot\"][:, atom_mask_j_minus[i]]\n",
    "                    # remove atom_mask of atoms in random_indices\n",
    "                    data_j_minus_dict[i][\"atom_mask\"] = data_j_minus_dict[i][\"atom_mask\"][:, atom_mask_j_minus[i]]\n",
    "                    # remove fragment_mask of atoms in random_indices\n",
    "                    data_j_minus_dict[i][\"fragment_mask\"] = data_j_minus_dict[i][\"fragment_mask\"][:, atom_mask_j_minus[i]]\n",
    "                    # remove linker_mask of atoms in random_indices\n",
    "                    data_j_minus_dict[i][\"linker_mask\"] = data_j_minus_dict[i][\"linker_mask\"][:, atom_mask_j_minus[i]]\n",
    "                    data_j_minus_dict[i][\"charges\"] = data_j_minus_dict[i][\"charges\"][:, atom_mask_j_minus[i]]\n",
    "                    data_j_minus_dict[i][\"anchors\"] = data_j_minus_dict[i][\"anchors\"][:, atom_mask_j_minus[i]]\n",
    "                    # remove edge_mask of atoms in random_indices\n",
    "                    edge_mask_to_keep = (atom_mask_j_minus[i].unsqueeze(1) * atom_mask_j_minus[i]).flatten() \n",
    "                    data_j_minus_dict[i][\"edge_mask\"] = data_j_minus_dict[i][\"edge_mask\"][edge_mask_to_keep]\n",
    "\n",
    "                    #data random\n",
    "                    data_random_dict[i][\"positions\"] = data_random_dict[i][\"positions\"][:, atom_mask_random_molecule[i]]\n",
    "                    data_random_dict[i][\"num_atoms\"] = data_random_dict[i][\"positions\"].shape[1]\n",
    "                    # remove one_hot of atoms in random_indices\n",
    "                    data_random_dict[i][\"one_hot\"] = data_random_dict[i][\"one_hot\"][:, atom_mask_random_molecule[i]]\n",
    "                    # remove atom_mask of atoms in random_indices\n",
    "                    data_random_dict[i][\"atom_mask\"] = data_random_dict[i][\"atom_mask\"][:, atom_mask_random_molecule[i]]\n",
    "                    # remove fragment_mask of atoms in random_indices\n",
    "                    data_random_dict[i][\"fragment_mask\"] = data_random_dict[i][\"fragment_mask\"][:, atom_mask_random_molecule[i]]\n",
    "                    # remove linker_mask of atoms in random_indices\n",
    "                    data_random_dict[i][\"linker_mask\"] = data_random_dict[i][\"linker_mask\"][:, atom_mask_random_molecule[i]]\n",
    "                    data_random_dict[i][\"charges\"] = data_random_dict[i][\"charges\"][:, atom_mask_random_molecule[i]]\n",
    "                    data_random_dict[i][\"anchors\"] = data_random_dict[i][\"anchors\"][:, atom_mask_random_molecule[i]]\n",
    "                    # remove edge_mask of atoms in random_indices\n",
    "                    # remove edge_mask of atoms in random_indices\n",
    "                    edge_mask_to_keep = (atom_mask_random_molecule[i].unsqueeze(1) * atom_mask_random_molecule[i]).flatten() \n",
    "\n",
    "                    data_random_dict[i][\"edge_mask\"] = data_random_dict[i][\"edge_mask\"][edge_mask_to_keep]\n",
    "                \n",
    "                # end_time = time.time()\n",
    "                # print(\"Time to remove atoms from molecules in seconds:\", end_time - start_time)\n",
    "\n",
    "                PADDING = True\n",
    "\n",
    "                # start_time = time.time()\n",
    "                if PADDING:\n",
    "\n",
    "                    max_atoms_j_plus = max(data_j_plus_dict[i][\"num_atoms\"] for i in range(PARALLEL_STEPS))\n",
    "\n",
    "                    max_edges_j_plus = max(data_j_plus_dict[i][\"edge_mask\"].shape[0] for i in range(PARALLEL_STEPS))\n",
    "                    \n",
    "                    \n",
    "                    max_atoms_j_minus = max(data_j_minus_dict[i][\"num_atoms\"] for i in range(PARALLEL_STEPS))\n",
    "\n",
    "                    max_edges_j_minus = max(data_j_minus_dict[i][\"edge_mask\"].shape[0] for i in range(PARALLEL_STEPS))\n",
    "\n",
    "                    max_atoms_random = max(data_random_dict[i][\"num_atoms\"] for i in range(PARALLEL_STEPS))\n",
    "\n",
    "                    max_edges_random = max(data_random_dict[i][\"edge_mask\"].shape[0] for i in range(PARALLEL_STEPS))\n",
    "                    \n",
    "                    for i in range(PARALLEL_STEPS):\n",
    "                        #for j plus positions\n",
    "                        num_atoms_to_stack = max_atoms_j_plus - data_j_plus_dict[i][\"positions\"].shape[1]\n",
    "                        padding = torch.zeros(data_j_plus_dict[i][\"positions\"].shape[0], num_atoms_to_stack, data_j_plus_dict[i][\"positions\"].shape[2]).to(args.device)\n",
    "                        stacked_positions = torch.cat((data_j_plus_dict[i][\"positions\"], padding), dim=1)\n",
    "                        data_j_plus_dict[i][\"positions\"] = stacked_positions\n",
    "                        #for j plus one_hot\n",
    "                        padding = torch.zeros(data_j_plus_dict[i][\"one_hot\"].shape[0], num_atoms_to_stack, data_j_plus_dict[i][\"one_hot\"].shape[2]).to(args.device)\n",
    "                        stacked_one_hot = torch.cat((data_j_plus_dict[i][\"one_hot\"], padding), dim=1)\n",
    "                        data_j_plus_dict[i][\"one_hot\"] = stacked_one_hot\n",
    "                        padding = torch.zeros(data_j_plus_dict[i][\"fragment_mask\"].shape[0], num_atoms_to_stack, data_j_plus_dict[i][\"fragment_mask\"].shape[2]).to(args.device)\n",
    "                        stacked_fragment_mask = torch.cat((data_j_plus_dict[i][\"fragment_mask\"], padding), dim=1)\n",
    "                        data_j_plus_dict[i][\"fragment_mask\"] = stacked_fragment_mask\n",
    "                        padding = torch.zeros(data_j_plus_dict[i][\"charges\"].shape[0], num_atoms_to_stack, data_j_plus_dict[i][\"charges\"].shape[2]).to(args.device)\n",
    "                        stacked_charges = torch.cat((data_j_plus_dict[i][\"charges\"], padding), dim=1)\n",
    "                        data_j_plus_dict[i][\"charges\"] = stacked_charges\n",
    "                        padding = torch.zeros(data_j_plus_dict[i][\"anchors\"].shape[0], num_atoms_to_stack, data_j_plus_dict[i][\"anchors\"].shape[2]).to(args.device)\n",
    "                        stacked_anchors = torch.cat((data_j_plus_dict[i][\"anchors\"], padding), dim=1)\n",
    "                        data_j_plus_dict[i][\"anchors\"] = stacked_anchors\n",
    "                        padding = torch.zeros(data_j_plus_dict[i][\"linker_mask\"].shape[0], num_atoms_to_stack, data_j_plus_dict[i][\"linker_mask\"].shape[2]).to(args.device)\n",
    "                        stacked_linker_mask = torch.cat((data_j_plus_dict[i][\"linker_mask\"], padding), dim=1)\n",
    "                        data_j_plus_dict[i][\"linker_mask\"] = stacked_linker_mask\n",
    "                        padding = torch.zeros(data_j_plus_dict[i][\"atom_mask\"].shape[0], num_atoms_to_stack, data_j_plus_dict[i][\"atom_mask\"].shape[2]).to(args.device)\n",
    "                        stacked_atom_mask = torch.cat((data_j_plus_dict[i][\"atom_mask\"], padding), dim=1)\n",
    "                        data_j_plus_dict[i][\"atom_mask\"] = stacked_atom_mask\n",
    "                        num_edges_to_stack = max_edges_j_plus - data_j_plus_dict[i][\"edge_mask\"].shape[0]\n",
    "                        data_j_plus_dict[i][\"edge_mask\"] = data_j_plus_dict[i][\"edge_mask\"].unsqueeze(0)\n",
    "                        padding = torch.zeros(data_j_plus_dict[i][\"edge_mask\"].shape[0], num_edges_to_stack, data_j_plus_dict[i][\"edge_mask\"].shape[2]).to(args.device)\n",
    "                        stacked_edge_mask = torch.cat((data_j_plus_dict[i][\"edge_mask\"], padding), dim=1)\n",
    "                        data_j_plus_dict[i][\"edge_mask\"] = stacked_edge_mask\n",
    "                        \n",
    "                        #for j minus\n",
    "                        num_atoms_to_stack = max_atoms_j_minus - data_j_minus_dict[i][\"positions\"].shape[1]\n",
    "                        padding = torch.zeros(data_j_minus_dict[i][\"positions\"].shape[0], num_atoms_to_stack, data_j_minus_dict[i][\"positions\"].shape[2]).to(args.device) #why does this work?\n",
    "                        stacked_positions = torch.cat((data_j_minus_dict[i][\"positions\"], padding), dim=1)\n",
    "                        data_j_minus_dict[i][\"positions\"] = stacked_positions\n",
    "                        \n",
    "                        padding = torch.zeros(data_j_minus_dict[i][\"one_hot\"].shape[0], num_atoms_to_stack, data_j_minus_dict[i][\"one_hot\"].shape[2]).to(args.device)\n",
    "                        stacked_one_hot = torch.cat((data_j_minus_dict[i][\"one_hot\"], padding), dim=1)\n",
    "                        data_j_minus_dict[i][\"one_hot\"] = stacked_one_hot\n",
    "                        \n",
    "                        padding = torch.zeros(data_j_minus_dict[i][\"fragment_mask\"].shape[0], num_atoms_to_stack, data_j_minus_dict[i][\"fragment_mask\"].shape[2]).to(args.device)\n",
    "                        stacked_fragment_mask = torch.cat((data_j_minus_dict[i][\"fragment_mask\"], padding), dim=1)\n",
    "                        data_j_minus_dict[i][\"fragment_mask\"] = stacked_fragment_mask\n",
    "\n",
    "                        \n",
    "                        padding = torch.zeros(data_j_minus_dict[i][\"charges\"].shape[0], num_atoms_to_stack, data_j_minus_dict[i][\"charges\"].shape[2]).to(args.device)\n",
    "                        stacked_charges = torch.cat((data_j_minus_dict[i][\"charges\"], padding), dim=1)\n",
    "                        data_j_minus_dict[i][\"charges\"] = stacked_charges\n",
    "                        \n",
    "                        padding = torch.zeros(data_j_minus_dict[i][\"anchors\"].shape[0], num_atoms_to_stack, data_j_minus_dict[i][\"anchors\"].shape[2]).to(args.device)\n",
    "                        stacked_anchors = torch.cat((data_j_minus_dict[i][\"anchors\"], padding), dim=1)\n",
    "                        data_j_minus_dict[i][\"anchors\"] = stacked_anchors\n",
    "                       \n",
    "                        padding = torch.zeros(data_j_minus_dict[i][\"linker_mask\"].shape[0], num_atoms_to_stack, data_j_minus_dict[i][\"linker_mask\"].shape[2]).to(args.device)\n",
    "                        stacked_linker_mask = torch.cat((data_j_minus_dict[i][\"linker_mask\"], padding), dim=1)\n",
    "                        data_j_minus_dict[i][\"linker_mask\"] = stacked_linker_mask\n",
    "                        \n",
    "                        padding = torch.zeros(data_j_minus_dict[i][\"atom_mask\"].shape[0], num_atoms_to_stack, data_j_minus_dict[i][\"atom_mask\"].shape[2]).to(args.device)\n",
    "                        stacked_atom_mask = torch.cat((data_j_minus_dict[i][\"atom_mask\"], padding), dim=1)\n",
    "                        data_j_minus_dict[i][\"atom_mask\"] = stacked_atom_mask\n",
    "                        \n",
    "                        num_edges_to_stack = max_edges_j_minus - data_j_minus_dict[i][\"edge_mask\"].shape[0]\n",
    "                        data_j_minus_dict[i][\"edge_mask\"] = data_j_minus_dict[i][\"edge_mask\"].unsqueeze(0)\n",
    "                        padding = torch.zeros(data_j_minus_dict[i][\"edge_mask\"].shape[0], num_edges_to_stack, data_j_minus_dict[i][\"edge_mask\"].shape[2]).to(args.device)\n",
    "                        stacked_edge_mask = torch.cat((data_j_minus_dict[i][\"edge_mask\"], padding), dim=1)\n",
    "                        data_j_minus_dict[i][\"edge_mask\"] = stacked_edge_mask\n",
    "                    \n",
    "\n",
    "                        #for random\n",
    "                        num_atoms_to_stack = max_atoms_random - data_random_dict[i][\"positions\"].shape[1]\n",
    "                        padding = torch.zeros(data_random_dict[i][\"positions\"].shape[0], num_atoms_to_stack, data_random_dict[i][\"positions\"].shape[2]).to(args.device)\n",
    "                        stacked_positions = torch.cat((data_random_dict[i][\"positions\"], padding), dim=1)\n",
    "                        data_random_dict[i][\"positions\"] = stacked_positions\n",
    "                        \n",
    "                        padding = torch.zeros(data_random_dict[i][\"one_hot\"].shape[0], num_atoms_to_stack, data_random_dict[i][\"one_hot\"].shape[2]).to(args.device)\n",
    "                        stacked_one_hot = torch.cat((data_random_dict[i][\"one_hot\"], padding), dim=1)\n",
    "                        data_random_dict[i][\"one_hot\"] = stacked_one_hot\n",
    "                        \n",
    "                        padding = torch.zeros(data_random_dict[i][\"fragment_mask\"].shape[0], num_atoms_to_stack, data_random_dict[i][\"fragment_mask\"].shape[2]).to(args.device)\n",
    "                        stacked_fragment_mask = torch.cat((data_random_dict[i][\"fragment_mask\"], padding), dim=1)\n",
    "                        data_random_dict[i][\"fragment_mask\"] = stacked_fragment_mask\n",
    "                        \n",
    "                        padding = torch.zeros(data_random_dict[i][\"linker_mask\"].shape[0], num_atoms_to_stack, data_random_dict[i][\"linker_mask\"].shape[2]).to(args.device)\n",
    "                        stacked_linker_mask = torch.cat((data_random_dict[i][\"linker_mask\"], padding), dim=1)\n",
    "                        data_random_dict[i][\"linker_mask\"] = stacked_linker_mask\n",
    "\n",
    "                       \n",
    "                        padding = torch.zeros(data_random_dict[i][\"charges\"].shape[0], num_atoms_to_stack, data_random_dict[i][\"charges\"].shape[2]).to(args.device)\n",
    "                        stacked_charges = torch.cat((data_random_dict[i][\"charges\"], padding), dim=1)\n",
    "                        data_random_dict[i][\"charges\"] = stacked_charges\n",
    "\n",
    "                    \n",
    "                        padding = torch.zeros(data_random_dict[i][\"anchors\"].shape[0], num_atoms_to_stack, data_random_dict[i][\"anchors\"].shape[2]).to(args.device)\n",
    "                        stacked_anchors = torch.cat((data_random_dict[i][\"anchors\"], padding), dim=1)\n",
    "                        data_random_dict[i][\"anchors\"] = stacked_anchors\n",
    "                       \n",
    "                        padding = torch.zeros(data_random_dict[i][\"atom_mask\"].shape[0], num_atoms_to_stack, data_random_dict[i][\"atom_mask\"].shape[2]).to(args.device)\n",
    "                        stacked_atom_mask = torch.cat((data_random_dict[i][\"atom_mask\"], padding), dim=1)\n",
    "                        data_random_dict[i][\"atom_mask\"] = stacked_atom_mask\n",
    "                        \n",
    "                        num_edges_to_stack = max_edges_random - data_random_dict[i][\"edge_mask\"].shape[0]\n",
    "                        data_random_dict[i][\"edge_mask\"] = data_random_dict[i][\"edge_mask\"].unsqueeze(0)\n",
    "                        padding = torch.zeros(data_random_dict[i][\"edge_mask\"].shape[0], num_edges_to_stack, data_random_dict[i][\"edge_mask\"].shape[2]).to(args.device)\n",
    "                        stacked_edge_mask = torch.cat((data_random_dict[i][\"edge_mask\"], padding), dim=1)\n",
    "                        data_random_dict[i][\"edge_mask\"] = stacked_edge_mask\n",
    "\n",
    "                # end_time = time.time()\n",
    "                # print(\"Time to pad molecules in seconds:\", end_time - start_time)\n",
    "\n",
    "                # start_time = time.time()\n",
    "                #create batch for j plus\n",
    "                data_j_plus_batch = {}\n",
    "                data_j_plus_batch[\"positions\"] = torch.stack([data_j_plus_dict[i][\"positions\"] for i in range(PARALLEL_STEPS)], dim=0).squeeze()\n",
    "                data_j_plus_batch[\"one_hot\"] = torch.stack([data_j_plus_dict[i][\"one_hot\"] for i in range(PARALLEL_STEPS)], dim=0).squeeze(1)\n",
    "                data_j_plus_batch[\"atom_mask\"] = torch.stack([data_j_plus_dict[i][\"atom_mask\"] for i in range(PARALLEL_STEPS)], dim=0).squeeze(1)\n",
    "                data_j_plus_batch[\"fragment_mask\"] = torch.stack([data_j_plus_dict[i][\"fragment_mask\"] for i in range(PARALLEL_STEPS)], dim=0).squeeze(1)\n",
    "                data_j_plus_batch[\"linker_mask\"] = torch.stack([data_j_plus_dict[i][\"linker_mask\"] for i in range(PARALLEL_STEPS)], dim=0).squeeze(1)\n",
    "                data_j_plus_batch[\"charges\"] = torch.stack([data_j_plus_dict[i][\"charges\"] for i in range(PARALLEL_STEPS)], dim=0).squeeze(1)\n",
    "                data_j_plus_batch[\"anchors\"] = torch.stack([data_j_plus_dict[i][\"anchors\"] for i in range(PARALLEL_STEPS)], dim=0).squeeze(1)\n",
    "                \n",
    "                \n",
    "                data_j_plus_batch[\"uuid\"] = [i for i in range(PARALLEL_STEPS)]\n",
    "                data_j_plus_batch[\"num_atoms\"] = [data_j_plus_dict[i][\"num_atoms\"] for i in range(PARALLEL_STEPS)]\n",
    "                data_j_plus_batch[\"name\"] = [data[\"name\"] for _ in range(PARALLEL_STEPS)]\n",
    "                data_j_plus_batch[\"edge_mask\"] = torch.cat([data_j_plus_dict[i][\"edge_mask\"] for i in range(PARALLEL_STEPS)], dim=0).squeeze().view(-1).unsqueeze(1)\n",
    "\n",
    "\n",
    "                #create batch for j minus\n",
    "                data_j_minus_batch = {}\n",
    "                data_j_minus_batch[\"positions\"] = torch.stack([data_j_minus_dict[i][\"positions\"] for i in range(PARALLEL_STEPS)], dim=0).squeeze()\n",
    "                data_j_minus_batch[\"one_hot\"] = torch.stack([data_j_minus_dict[i][\"one_hot\"] for i in range(PARALLEL_STEPS)], dim=0).squeeze(1)\n",
    "                data_j_minus_batch[\"atom_mask\"] = torch.stack([data_j_minus_dict[i][\"atom_mask\"] for i in range(PARALLEL_STEPS)], dim=0).squeeze(1)\n",
    "                data_j_minus_batch[\"fragment_mask\"] = torch.stack([data_j_minus_dict[i][\"fragment_mask\"] for i in range(PARALLEL_STEPS)], dim=0).squeeze(1)\n",
    "                data_j_minus_batch[\"linker_mask\"] = torch.stack([data_j_minus_dict[i][\"linker_mask\"] for i in range(PARALLEL_STEPS)], dim=0).squeeze(1)\n",
    "                data_j_minus_batch[\"charges\"] = torch.stack([data_j_minus_dict[i][\"charges\"] for i in range(PARALLEL_STEPS)], dim=0).squeeze(1)\n",
    "                data_j_minus_batch[\"anchors\"] = torch.stack([data_j_minus_dict[i][\"anchors\"] for i in range(PARALLEL_STEPS)], dim=0).squeeze(1)\n",
    "                \n",
    "                data_j_minus_batch[\"uuid\"] = [i for i in range(PARALLEL_STEPS)]\n",
    "                data_j_minus_batch[\"num_atoms\"] = [data_j_minus_dict[i][\"num_atoms\"] for i in range(PARALLEL_STEPS)]\n",
    "                data_j_minus_batch[\"name\"] = [data[\"name\"] for _ in range(PARALLEL_STEPS)]\n",
    "                data_j_minus_batch[\"edge_mask\"] = torch.cat([data_j_minus_dict[i][\"edge_mask\"] for i in range(PARALLEL_STEPS)], dim=0).squeeze().view(-1).unsqueeze(1)\n",
    "\n",
    "                #create batch for random\n",
    "                data_random_batch = {}\n",
    "                data_random_batch[\"positions\"] = torch.stack([data_random_dict[i][\"positions\"] for i in range(PARALLEL_STEPS)], dim=0).squeeze()\n",
    "                data_random_batch[\"one_hot\"] = torch.stack([data_random_dict[i][\"one_hot\"] for i in range(PARALLEL_STEPS)], dim=0).squeeze(1)\n",
    "                data_random_batch[\"atom_mask\"] = torch.stack([data_random_dict[i][\"atom_mask\"] for i in range(PARALLEL_STEPS)], dim=0).squeeze(1)\n",
    "                data_random_batch[\"fragment_mask\"] = torch.stack([data_random_dict[i][\"fragment_mask\"] for i in range(PARALLEL_STEPS)], dim=0).squeeze(1)\n",
    "                data_random_batch[\"linker_mask\"] = torch.stack([data_random_dict[i][\"linker_mask\"] for i in range(PARALLEL_STEPS)], dim=0).squeeze(1)\n",
    "                data_random_batch[\"charges\"] = torch.stack([data_random_dict[i][\"charges\"] for i in range(PARALLEL_STEPS)], dim=0).squeeze(1)\n",
    "                data_random_batch[\"anchors\"] = torch.stack([data_random_dict[i][\"anchors\"] for i in range(PARALLEL_STEPS)], dim=0).squeeze(1)\n",
    "                \n",
    "                data_random_batch[\"uuid\"] = [i for i in range(PARALLEL_STEPS)]\n",
    "                data_random_batch[\"num_atoms\"] = [data_random_dict[i][\"num_atoms\"] for i in range(PARALLEL_STEPS)]\n",
    "                data_random_batch[\"name\"] = [data[\"name\"] for _ in range(PARALLEL_STEPS)]\n",
    "                data_random_batch[\"edge_mask\"] = torch.cat([data_random_dict[i][\"edge_mask\"] for i in range(PARALLEL_STEPS)], dim=0).squeeze().view(-1).unsqueeze(1)\n",
    "\n",
    "                # end_time = time.time()\n",
    "                # print(\"Time to create batches for j plus, j minus and random molecule in seconds:\", end_time - start_time)\n",
    "\n",
    "                # start_time = time.time()\n",
    "               \n",
    "                chain_j_plus_batch, node_mask_j_plus_batch = model.sample_chain(data_j_plus_batch, keep_frames=args.keep_frames)\n",
    "\n",
    "                chain_j_plus = chain_j_plus_batch[0, :, :, :] #it should take the first frame and all batch elements -> check it is really the first frame (I need the one at t0, the final generated molecule)\n",
    "                \n",
    "                chain_j_minus_batch, node_mask_j_minus_batch = model.sample_chain(data_j_minus_batch, keep_frames=args.keep_frames)\n",
    "\n",
    "                chain_j_minus = chain_j_minus_batch[0, :, :, :] \n",
    "\n",
    "                chain_random_batch, node_mask_random_batch = model.sample_chain(data_random_batch, keep_frames=args.keep_frames)\n",
    "\n",
    "                chain_random = chain_random_batch[0, :, :, :]\n",
    "                \n",
    "                # end_time = time.time()\n",
    "                # print(\"Time to sample chains in seconds:\", end_time - start_time)\n",
    "\n",
    "                # start_time = time.time()\n",
    "\n",
    "                chain_with_full_fragments_batch = chain_with_full_fragments.repeat(PARALLEL_STEPS, 1, 1)\n",
    "\n",
    "                \n",
    "\n",
    "                V_j_plus_distance_batch = compute_molecular_distance_batch(chain_with_full_fragments_batch, chain_j_plus, mask1=original_linker_mask_batch, mask2=data_j_plus_batch[\"linker_mask\"].squeeze())\n",
    "                \n",
    "                \n",
    "                V_j_plus_distance = torch.sum(V_j_plus_distance_batch).item()\n",
    "                \n",
    "\n",
    "                V_j_plus_cosine_similarity_batch = compute_cosine_similarity_batch(chain_with_full_fragments_batch.cpu(), chain_j_plus.cpu(), mask1=original_linker_mask_batch.cpu(), mask2=data_j_plus_batch[\"linker_mask\"].squeeze().cpu())\n",
    "\n",
    "                V_j_plus_cosine_similarity = sum(V_j_plus_cosine_similarity_batch)\n",
    "                \n",
    "\n",
    "                V_j_minus_distance_batch = compute_molecular_distance_batch(chain_with_full_fragments_batch, chain_j_minus, mask1=original_linker_mask_batch, mask2=data_j_minus_batch[\"linker_mask\"].squeeze())\n",
    "\n",
    "                V_j_minus_distance = torch.sum(V_j_minus_distance_batch).item()\n",
    "                \n",
    "                \n",
    "                V_j_minus_cosine_similarity_batch = compute_cosine_similarity_batch(chain_with_full_fragments_batch.cpu(), chain_j_minus.cpu(), mask1=original_linker_mask_batch.cpu(), mask2=data_j_minus_batch[\"linker_mask\"].squeeze().cpu())\n",
    "\n",
    "                V_j_minus_cosine_similarity = sum(V_j_minus_cosine_similarity_batch)\n",
    "\n",
    "                \n",
    "\n",
    "                V_random_distance_batch = compute_molecular_distance_batch(chain_with_full_fragments_batch, chain_random, mask1=original_linker_mask_batch, mask2=data_random_batch[\"linker_mask\"].squeeze())\n",
    "                \n",
    "                \n",
    "                \n",
    "\n",
    "                V_random_cosine_similarity = compute_cosine_similarity_batch(chain_with_full_fragments_batch.cpu(), chain_random.cpu(), mask1=original_linker_mask_batch.cpu(), mask2=data_random_batch[\"linker_mask\"].squeeze().cpu())\n",
    "\n",
    "                for r_dist in V_random_distance_batch:\n",
    "                    distances_random_samples.append(r_dist.item())\n",
    "                \n",
    "                for r_cos in V_random_cosine_similarity:\n",
    "                    cosine_similarities_random_samples.append(r_cos)\n",
    "                \n",
    "\n",
    "                \n",
    "                \n",
    "                marginal_contrib_distance += (V_j_plus_distance - V_j_minus_distance)\n",
    "\n",
    "                marginal_contrib_cosine_similarity += (V_j_plus_cosine_similarity - V_j_minus_cosine_similarity)\n",
    "\n",
    "                # end_time = time.time()\n",
    "                # print(\"Time to compute V_j_plus, V_j_minus, V_random, and the marginal contribution in seconds:\", end_time - start_time)\n",
    "                \n",
    "\n",
    "            phi_atoms[fragment_indices[j].item()] = [0,0] #,0]    \n",
    "            phi_atoms[fragment_indices[j].item()][0] = marginal_contrib_distance/M #j is the index of the fragment atom in the fragment indices tensor\n",
    "            phi_atoms[fragment_indices[j].item()][1] = marginal_contrib_cosine_similarity/M\n",
    "            # phi_atoms[fragment_indices[j]][2] = marginal_contrib_hausdorff/M\n",
    "\n",
    "        print(data[\"name\"])\n",
    "\n",
    "        phi_atoms_distances = {}\n",
    "        phi_atoms_cosine_similarity = {}\n",
    "        for atom_index, phi_values in phi_atoms.items():\n",
    "            phi_atoms_distances[atom_index] = phi_values[0]\n",
    "            phi_atoms_cosine_similarity[atom_index] = phi_values[1]\n",
    "        \n",
    "        # Save phi_atoms to a text file\n",
    "        with open(f'{folder_save_path}/phi_atoms_{data_index}.txt', 'w') as write_file:\n",
    "            write_file.write(\"sample name: \" + str(data[\"name\"]) + \"\\n\")\n",
    "            write_file.write(\"atom_index,distance,cosine_similarity\\n\")\n",
    "            for atom_index, phi_values in phi_atoms.items():\n",
    "                write_file.write(f\"{atom_index},{phi_values[0]},{phi_values[1]}\\n\")\n",
    "\n",
    "            write_file.write(\"\\n\")\n",
    "            # save sum of phi values for disance and cosine similarity\n",
    "            write_file.write(\"Sum of phi values for distance\\n\")\n",
    "            write_file.write(str(sum([p_values[0] for p_values in phi_atoms.values()])) + \"\\n\")\n",
    "            write_file.write(\"Sum of phi values for cosine similarity\\n\")\n",
    "            write_file.write(str(sum([p_values[1] for p_values in phi_atoms.values()])) + \"\\n\")     \n",
    "            write_file.write(\"Average distance random samples:\\n\")\n",
    "            write_file.write(str(sum(distances_random_samples)/len(distances_random_samples)) + \"\\n\")\n",
    "            write_file.write(\"Average cosine similarity random samples:\\n\")\n",
    "            write_file.write(str(sum(cosine_similarities_random_samples)/len(cosine_similarities_random_samples)) + \"\\n\")      \n",
    "            write_file.write(\"Distances random samples\\n\")\n",
    "            write_file.write(str(distances_random_samples) + \"\\n\")\n",
    "            write_file.write(\"Cosines similarity random samples\\n\")\n",
    "            write_file.write(str(cosine_similarities_random_samples) + \"\\n\")\n",
    "\n",
    "        if SAVE_VISUALIZATION:\n",
    "            for i in range(len(data['positions'])):\n",
    "                chain = chain_batch[:, i, :, :]\n",
    "                assert chain.shape[0] == args.keep_frames\n",
    "                assert chain.shape[1] == data['positions'].shape[1]\n",
    "                assert chain.shape[2] == data['positions'].shape[2] + data['one_hot'].shape[2] + model.include_charges\n",
    "\n",
    "                # Saving chains\n",
    "                name = str(i + start)\n",
    "                chain_output = os.path.join(chains_output_dir, name)\n",
    "                os.makedirs(chain_output, exist_ok=True)\n",
    "\n",
    "                one_hot = chain[:, :, 3:-1]\n",
    "                positions = chain[:, :, :3]\n",
    "                chain_node_mask = torch.cat([node_mask[i].unsqueeze(0) for _ in range(args.keep_frames)], dim=0)\n",
    "                names = [f'{name}_{j}' for j in range(args.keep_frames)]\n",
    "\n",
    "                save_xyz_file(chain_output, one_hot, positions, chain_node_mask, names=names, is_geom=model.is_geom)\n",
    "                visualize_chain_xai(\n",
    "                    chain_output,\n",
    "                    spheres_3d=False,\n",
    "                    alpha=0.7,\n",
    "                    bg='white',\n",
    "                    is_geom=model.is_geom,\n",
    "                    fragment_mask=data['fragment_mask'][i].squeeze(),\n",
    "                    phi_values=phi_atoms_distances\n",
    "                )\n",
    "\n",
    "                # Saving final prediction and ground truth separately\n",
    "                true_one_hot = data['one_hot'][i].unsqueeze(0)\n",
    "                true_positions = data['positions'][i].unsqueeze(0)\n",
    "                true_node_mask = data['atom_mask'][i].unsqueeze(0)\n",
    "                save_xyz_file(\n",
    "                    final_states_output_dir,\n",
    "                    true_one_hot,\n",
    "                    true_positions,\n",
    "                    true_node_mask,\n",
    "                    names=[f'{name}_true'],\n",
    "                    is_geom=model.is_geom,\n",
    "                )\n",
    "\n",
    "                pred_one_hot = chain[0, :, 3:-1].unsqueeze(0)\n",
    "                pred_positions = chain[0, :, :3].unsqueeze(0)\n",
    "                pred_node_mask = chain_node_mask[0].unsqueeze(0)\n",
    "                save_xyz_file(\n",
    "                    final_states_output_dir,\n",
    "                    pred_one_hot,\n",
    "                    pred_positions,\n",
    "                    pred_node_mask,\n",
    "                    names=[f'{name}_pred'],\n",
    "                    is_geom=model.is_geom\n",
    "                )\n",
    "\n",
    "            start += len(data['positions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Shapley value Propeties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shapley_values = {0:(-0.18252010345458985,-0.03153111279010773),\n",
    "# 1:(-0.19735857963562012,-0.0004155013896524906),\n",
    "# 2:(0.24352870702743531,0.03291264953091741),\n",
    "# 3:(0.5766246366500855,-0.02155731648206711),\n",
    "# 4:(-0.8824016571044921,-0.02419700352475047),\n",
    "# 5:(0.04895777225494385,-0.02288945931941271),\n",
    "# 6:(-0.11883691549301148,-0.017148097790777684),\n",
    "# 7:(-0.3973711347579956,-0.08772553377784789),\n",
    "# 8:(-1.0809556245803833,0.005452861245721578),\n",
    "# 9:(-0.09876126766204835,-0.04913015581667423),\n",
    "# 10:(-0.9884893560409546,0.11794438790529967),\n",
    "# 11:(-1.126043050289154,0.13286019276827574),\n",
    "# 12:(-1.1925089359283447,-0.07200432924553751),\n",
    "# 13:(-1.183656153678894,0.11058532498776913),\n",
    "# 14:(-1.2386692070960998,0.12144924929365515),\n",
    "# 15:(-0.9519470238685608,0.09354953311383724),\n",
    "# 16:(-1.0259105682373046,0.1189951341226697),\n",
    "# 17:(-0.47114646434783936,-0.006474981680512428),\n",
    "# 18:(-0.516231164932251,0.08100643368437886),\n",
    "# 19:(-1.2924485397338867,0.13028908021748065)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum_shapley_values_distance = 0\n",
    "# sum_shapley_values_cosine_similarity = 0\n",
    "\n",
    "# for key, value in shapley_values.items():\n",
    "#     sum_shapley_values_distance += value[0]\n",
    "#     sum_shapley_values_cosine_similarity += value[1]\n",
    "\n",
    "# print(\"Sum of shapley values for distance\", sum_shapley_values_distance)\n",
    "# print(\"Sum of shapley values for cosine similarity\", sum_shapley_values_cosine_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Convert the list to a numpy array\n",
    "# distances_array = np.array(distances_random_graphs)\n",
    "\n",
    "# # Calculate the z-scores for each element in the array\n",
    "# z_scores = (distances_array - np.mean(distances_array)) / np.std(distances_array)\n",
    "\n",
    "# # Define a threshold for outliers (e.g., z-score > 3)\n",
    "# threshold = 0.5\n",
    "\n",
    "# # Create a mask to identify outliers\n",
    "# outlier_mask = np.abs(z_scores) > threshold\n",
    "\n",
    "# # Remove outliers from the array\n",
    "# filtered_distances = distances_array[~outlier_mask]\n",
    "\n",
    "# # Convert the filtered array back to a list\n",
    "# filtered_distances_list = filtered_distances.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum(filtered_distances_list) / len(filtered_distances_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_distances_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert the list to a numpy array\n",
    "# cos_sim_array = np.array(cosine_similarities_random_graphs)\n",
    "\n",
    "# # Calculate the z-scores for each element in the array\n",
    "# z_scores = (cos_sim_array - np.mean(cos_sim_array)) / np.std(cos_sim_array)\n",
    "\n",
    "# # Define a threshold for outliers (e.g., z-score > 3)\n",
    "# threshold = 0.5\n",
    "\n",
    "# # Create a mask to identify outliers\n",
    "# outlier_mask = np.abs(z_scores) > threshold\n",
    "\n",
    "# # Remove outliers from the array\n",
    "# filtered_cos_sim = cos_sim_array[~outlier_mask]\n",
    "\n",
    "# # Convert the filtered array back to a list\n",
    "# filtered_cos_sim_list = filtered_cos_sim.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum(filtered_cos_sim_list) / len(filtered_cos_sim_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diff_explainer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
